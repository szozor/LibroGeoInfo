\seccion{Entrop\'ia condicional, informaci\'on mutua, entrop\'ias relativa \modif{y cruzada}.}
\label{Sec:SZ:Mutua}

Tratando de un par de variables aleatorias $X$ e $Y$, una cuesti\'on natural que
se  plantea es  la  de cuantificar  la  incerteza  que queda  sobre  una de  las
variables cuando se observa la otra.  Dicho de  otra manera, si se mide $Y = y$,
?`qu\'e informaci\'on  tenemos sobre  $X$? \modif{En la  misma vaina,  uno puede
interesarse  a  la informaci\'on  que  comparten  dos variables  aleatorias.  La
respuestas  a estas  interrogantes se  encuentra en  las nociones  de entrop\'ia
condicional  y de  informaci\'on mutua.  Estos conceptos  son v\'inculados  a la
noci\'on de divergencia  entre distribuciones de probabilidades  y de entrop\'ia
cruzada.}


% ================================= Condicional

\modif{
\subseccion{Entrop\'ia condicional}
\label{Ssec:SZ:Condicional}

La  respuesta  a  la  primera  interrogante} se  encuentra  en  la  noci\'on  de
entrop\'ia condicional.  Si uno mide $Y  = y$, la descripci\'on estad\'istica de
$X$  conociendo  que $Y  =  y$  se resume  a  la  distribuci\'on condicional  de
probabilidad  \modif{$P_{X|Y=y}(\cdot)$. \modif{Notando  $p_{X,Y}$ densidad  con
respecto  a  una  medida $\mu$,  y  usando  la  misma  notaci\'i $\mu$  para  la
restricci\'on  de  la  medida  a  $\X  = X(\Omega)$  o  $\Y  =  Y(\Omega)$  (con
$\X\Y \subseteq \X \times \Y$ dominio de $(X,Y)$)}
%= \frac{p_{X,Y}(\cdot,y)}{p_Y(y)}$.
%Con esta restricci\'on,
se puede evaluar una incerteza sobre  $X$, sabiendo que
$Y=y$ bajo la forma,
%
\[
H(X|Y=y \| \mu) = H\left( p_{X|Y=y} \| \mu\right).
\]
}
%
Entonces, condicionalmente a la variable aleatoria $Y$, la incerteza va a ser el
promedio estad\'istico sobre  todos los estados de $Y$,  es decir \modif{$\displaystyle
H(X|Y \| \mu)  = \int_\Y  H(X|Y=y \| \mu)  \, dP_Y(y)$.}
%\ (con  la medida  $\mu$ adecuada).
%
\modif{
\begin{definicion}[Entrop\'ia condicional]
\label{Def:SZ:entropiacondicional}
%
  Sean \ $X$ \  e \ $Y$ \ dos variables aleatorias con
  %,  respectivamente \  $d$ \  y \  $d'$-dimensionales \  $p_{X,Y}$ \  densidad
  conjunta  con respecto  a  una  medida \  $\mu$,  definida  sobre $\X\Y$.  La
  entrop\'ia condicional de \ $X$ con respecto a \ $Y$ se define como
  %
  \[
  H(X|Y \| \mu)   =   -  \int_{\X\Y}    \log
  p_{X|Y=y}(x) \, dP_{X,Y}(x,y).
  \]
  %
  %con \ $\mu$ \ medida adecuada (ej.  discreta en el caso discreto o de Lebesgue
  %en el caso diferencial).
\end{definicion}
}

Cuando $X$ e $Y$ son independientes, \modif{necesariamente $\X\Y = \X \times \Y$
y  m\'as all\'a  $\forall  \, y  \in  \Y,  \: p_{X|Y=y}  =  p_X$: la  entrop\'ia
condicional $H(X|Y \| \mu)$ se reduce a la de $X$, y reciprocamente}
%
\begin{propiedades}
\item\label{Prop:SZ:independenciacondicional}
  %
  \[
  X \: \mbox{e} \: Y \: \mbox{independientes} \quad \Leftrightarrow \quad \modif{H(X|Y \| \mu)
  = H(X \| \mu)}.
  \]
  \modif{donde se usa la misma escritura $\mu$ para la medida sobre $\X\Y$ y su restricci\'on a $\X$}.
\end{propiedades}
%
Esta  propiedad  se   interpreta  como  el  hecho  que   $Y$  no  lleva  ninguna
informaci\'on sobre  $X$, y entonces ninguna  medici\'on de $Y$ va  a cambiar la
incerteza en $X$.

Siendo  \modif{$H(X|Y=y  \|  \mu)$}  una  entrop\'ia, va  a  heredar  todas  las
propiedades  de   la  entrop\'ia  (o  entrop\'ia   diferencial).   Adem\'as,  de
$p_{X,Y}(\cdot,y) = p_{X|Y=y}(\cdot) \, p_Y(y)$ se deduce la propiedad siguiente:
%
\begin{propiedades}
\item\label{Prop:SZ:cadena}  {\it Regla de la cadena}
  %
  \[
  \modif{H(X,Y \| \mu) =  H(X|Y \| \mu) +  H(Y \| \mu)}.
  \]
  %
  Esta regla se generaliza sencillamente a
  %
  \[
  \modif{H(X_1 , \ldots , X_n \| \mu) = H(X_1 \| \mu) + \sum_{i=2}^n H(X_i|X_{i-1} , \ldots , X_1 \| \mu)}.
  \]
  %  
  De      esta     regla      de      la     cadena      se     recupera      la
  propiedad~\ref{Prop:SZ:independenciacondicional}     a     partir    de     la
  propiedad~\ref{Prop:SZ:aditividad}.
\end{propiedades}
%
Siendo  \modif{$H(X|Y=y \| \mu)$}  una  entrop\'ia,  en  el  caso  discreto  esta  cantidad  es
positiva. Entonces, en el caso discreto,  $H(X|Y)$ es positiva, lo que prueba la
super-aditividad~\ref{Prop:SZ:superaditividad}.


% ================================= Mutua

\modif{
\subseccion{Informaci\'on mutua}
\label{Ssec:SZ:Mutua}

De una  forma, cuando quitamos  la ignorancia de  una variable de  la ignorancia
condicional de esta  condicionalmente a una otra variable aleatoria,  se mide la
informaci\'on  que  comparten.   De  eso  viene  el  concepto  de  informaci\'on
mutua. Dicho de otra  manera, de} la regla de la cadena  \modif{$H(X,Y \| \mu) =
H(X|Y \|  \mu) +  H(Y \| \mu)  = H(Y|X  \| \mu)  + H(X \|  \mu)$} surge  que las
cantidades  \  \modif{$H(X|Y  \| \mu)  -  H(X  \|  \mu)$,  \ $H(Y|X  \|  \mu)  -
H(Y \|  \mu)$ \ y \  $H(X,Y \| \mu) -  H(X \| \mu) -  H(Y \| \mu)$} \  son todas
iguales.   Estas cantidades  definen  \modif{precisamente} lo  que  se llama  la
informaci\'on mutua entre \ $X$ \ e \  $Y$ \modif{y aparece que no depende de la
medida $\mu$ (que vamos a quitar en la definici\'on siguiente)}:
%
\begin{definicion}[Informaci\'on mutua]
\label{Def:SZ:mutua}
%
  Sean $X$ e $Y$ dos variables  aleatorias, la informaci\'on mutua entre \ $X$ \
  e  \ $Y$ \  es la  cantidad sim\'etrica
  %
  \[
  I(X;Y) = H(X|Y)-H(X) = H(Y|X)-H(Y) = H(X,Y) - H(X) - H(Y);
  \]
  %
  Se expresa
  %
  \[
  \modif{I(X;Y)  =  \int_{\X\Y}  \log  \left(
    \frac{p_{X,Y}(x,y)}{p_X(x) p_Y(y)} \right) \, dP_{X,Y}(x,y)}
  \]
  %
  %con \ $\mu$ \  medida adecuada (discreta en el caso discreto  o de Lebesgue en
  %el caso diferencial).
\end{definicion}

Las  diferentes  cantidades  pueden  ser  vistas  a  trav\'es  de  una  visi\'on
ensemblista, como descrita en la figura Fig.~\ref{Fig:SZ:Venn}, diagrama de Venn
o de Euler (ver nota de pie~\ref{Foot:MP:Euler}).
% pagina~\pageref{Foot:MP:Euler}.

\begin{figure}[h!]
%
\begin{center} \input{TIKZ_SZ/Venn} \end{center}
%
\leyenda{Diagrama  de Venn: Ilustraci\'on  de la  definici\'on de  la entrop\'ia
  condicional, de la informaci\'on mutua, y de las relaciones entre cada medida.
  La superficia del  elipse en linea llena (parte grise)  representa $H(X)$ y el
  interior de la  en linea discontinua representa $H(Y)$.   La parte grise clara
  representa $H(X|Y)$  superficia del ``conjunto $H(X)$'' quitando  la parte que
  partenece  a  $H(Y)$.  La  parte  blanca  representa  $H(Y|X)$ superficia  del
  ``conjunto $H(Y)$''  quitando la  parte que partenece  a $H(X)$.  La  parte en
  grise oscuro es entonces lo que $X$ e $Y$ comparten, es decir $I(X;Y)$.}
%
\label{Fig:SZ:Venn}
\end{figure}

Como se lo va a probar, $I$ es positiva; representa realmente una informaci\'on,
la compartida  entre \ $X$  \ e  \ $Y$: Si  de la incerteza  de $X$ se  quita la
incerteza  de  $X$   una  vez  que  $Y$  es  medida,  lo   que  queda  tiene  la
significaci\'on de  la informaci\'on que  estas variables tienen en  com\'un. En
particular, de $I(X;X) = H(X)$ se denoma a veces $H(X)$ {\it auto informaci\'on}
de $X$.


% ================================= Relativa y cruzada

\modif{
\subseccion{Entop\'ia relativa y entrop\'ia cruzada}
\label{Ssec:SZ:KL}
}

Para  probar la  positividad de  $I$, se  introduce de  manera m\'as  general la
noci\'on  de  entrop\'ia  relativa,   conocida  tambi\'en  como  divergencia  de
Kullback-Leibler~\cite{KulLei51,    Kul68,    CovTho06,    Rio07}~\footnote{Esta
  divergencia fue introducida despu\'es de  la secunda guerra, en el contexto de
  la guerra  fr\'ia. Kuulback y Leibler  trabajon para la  NSA en cryptolog\'ia,
  as\'i que esta medida encontr\'o  inicialmente aplicaciones en este macro, y el
  de la compresi\'on de datos.}:
%
\begin{definicion}[Entrop\'ia relativa]
\label{Def:SZ:entropiarelativa}
%
La entrop\'ia  relativa, o divergencia \modif{de Kullback-Leibler} de una  medida de probabilidad  $Q$, con
respecto a una medida de probabilidad de referencia $P$ tal que $Q \ll P$, ambas
definidas sobre $\Rset^d$, es definida como
  %
  \[
  \Dkl[Q]{P}  =  \int_{\Rset^d}  \frac{dQ}{dP}(x) \, \log  \left(  \frac{dQ}{dP}(x)
  \right)  \, dP(x)  = \int_{\Rset^d}  \log \left(  \frac{dQ}{dP}(x)  \right) \,
  dQ(x).
  \]
  %
  Si \  $P$ \  y \ $Q$  \ admiten una  densidad con  respecto a una  medida $\mu$
  (basicamente nos interesamos  a $\mu_L$ y $\mu_\X$), se  escribe a trav\'es de
  las  densidades  como~\footnote{En el  caso  discreto,  esta cantidad  depende
    solamente de $p$ y $q$ y no  de los estados. La condici\'on necesaria es que
    $p$ y $q$ tienen los mismos  n\'umeros de componentes (se completa el vector
    lo m\'as corto)  y si la $i$-esima componente de $q$  vale cero, entonces la
    de $p$ vale  cero tambi\'en.  Adem\'as, con $p$ y $q$  de mismo tama\~no, se
    puede poner en biyecci\'on los alfabetos  asociados a $p$ y $q$, sin perdida
    de generalidad.  En el caso  continuo, esta razonamiento no vale m\'as, esta
    cantidad dependiendo en general de los estados\ldots}
  %
 \[
 \Dkl[q]{p}  \equiv \Dkl[Q]{P}  = \modif{\int_{\Rset^d}  \log  \left( \frac{q(x)}{p(x)}
 \right) \, dQ(x)  = \int_{\Rset^d}  \log  \left( \frac{q(x)}{p(x)}
 \right) \, q(x) \, d\mu(x)}.
 \]
  %
\end{definicion}
%
\modif{Se notar\'a que, tratando de densidades, esta definici\'on no depende de la medida $\mu$ que se usa para definir las densidades.}

Inicialmente, esta  medida \modif{informacional} fue introducida  por Kullback y
Leibler    en     la    misma    linea    que     Shannon,    interpretando    \
$\log\left(\frac{dQ}{dP}(x)\right)$ \ como una informaci\'on de discriminaci\'on
entre  dos hip\'otesis  de distribuciones  \ $Q$  \ y  \ $P$  \ a  partir de  la
observaci\'on   \   $x$,  \   la   divergencia   siendo  la   informaci\'on   de
discriminaci\'on    promedia.     Introdujeron     tambi\'en    una    versi\'on
sim\'etrica~\footnote{De hecho, en~\cite{KulLei51} introdujeron primariamente la
versi\'on s\'imetrica  que se escribÃ©  ahora \ $\Dkl[P]{Q} +  \Dkl[Q]{P}$.}, que
veremos m\'as adelante.   \modif{Se puede inmediatamente ver  de la definici\'on
general Def.~\ref{Def:SZ:ShanonMu},
%pagina~\pageref{Def:SZ:ShanonMu}, 
que $\displaystyle  \Dkl[Q]{P} =  -H( Q \|  P)$: a pesar  del signo  menos, este
justifica el nombre  ``entrop\'ia relativa''.  Por ejemplo, en  el caso discreto
finito,  si $p$  es la  distribuci\'on uniforme  sobre un  alfabeto de  cardenal
$\alpha$, $\Dkl[q]{p} = \log \alpha -  H(q)$, lo que representa una desviaci\'on
de la  entrop\'ia de su valor  m\'aximo.  La misma interpretaci\'on  queda en el
caso continuo con la ley uniforme ($p$ y $q$ definidas sobre el mismo espacio de
volumen  finito) o  con la  gaussiana ($p$  y $q$  teniendo la  misma matriz  de
covarianza).

Se notar\'a  que en la  entrop\'ia relativa aparece  menos la entrop\'ia  de $q$
m\'as un otro t\'ermino parecido a una  entrop\'ia, pero donde el promedio de la
ignorancia elemental $\log(p)$ (``referencia'') con respecto a la distribuci\'on
$q$. Este t\'ermino es llamado a veces {\it entrop\'ia cruzada}:
%
\begin{definicion}[Entrop\'ia cruzada]
\label{Def:SZ:entropiacruzada}
%
Sean  dos  distribuciones  de probabilidad  \  $P$  \  y  \ $Q$  \  tal que \ $P$ \  admite una
densidad  \ $p$   con respecto  a una  medida \
$\mu$  \  sobre $\Rset^d$. La  entrop\'ia  cruzada  de  $Q$  con respecto  a  $P$
(referencia) es definida como
  %
  \[
  \Hcruz[Q]{P;\mu}   = - \int_{\Rset^d}  \log \left(  \frac{dP}{d\mu}(x)  \right) \,
  dQ(x).
  \]
  %
  Cuando $Q$ admite tambi\'en una densidad con respecto a $\mu$, se escribe tambi\'en
  %
  \[
  \Hcruz[Q]{P;\mu} \equiv \Hcruz[q]{p;\mu}  = - \int_{\Rset^d} \log \left(
  p(x) \right) \, q(x) \, d\mu(x)
  \]
  %
\end{definicion}
%
Pasando, cuando $\mu = Q$ (si $P \ll Q$ y $Q \ll P$), se recupera la divergencia
de Kullback-Leibler, es decir $\Hcruz[Q]{P;Q} = \Dkl[Q]{P}$.

De esta definici\'on, se connecta ahora  la entrop\'ia, la entrop\'ia relativa y
la entrop\'ia cruzada naturalmente de la manera siguiente:
%
\begin{lema}[Descomposici\'on de la entrop\'ia cruzada a partir de la entrop\'ia y la entrop\'ia relativa]
\label{Lem:SZ:DescomposicionEntropiaCruzada}
%
Sean $P$ y  $Q$ dos distribuciones de probabilidades que  admiten las densidades
respectiva \  $p$ \ y  \ $q$ \  con respecto a  una medida $\mu$.  La entrop\'ia
cruzada de $Q$ con  respeto a $P$ se descompone como la  entrop\'ia de $Q$ m\'as
la entrop\'ia relativa de $Q$ con respecto a $P$, \ie
%
\[
\Hcruz[Q]{P;\mu} = H(Q \| \mu) + \Dkl[Q]{P}
\]
%
\end{lema}
%
\begin{proof}
La  prueba sale  inmediatamente de  las definiciones  de la  entrop\'ia relativa
escribiendo $\log \frac{q}{p} = \log q - \log p$.
\end{proof}
%
Esta relaci\'on, parecida a  la regla de cadena, muestra de  una otra manera que
la  divergencia  de  Kullback-Leibler  es  una  entrop\'ia  relativamente  a  la
distribuci\'on $p$.}

\

\modif{\bf Nuevamente, en general se tratan de  los contextos \ $\mu = \mu_L$ \ medida
de Lebesgues, o \ $\mu = \mu_\X$ \ discreta: en tales casos, se olvida mencionar
la medida $\mu$ en la escritura formal.}

{\it   Como  para   la   entrop\'ia,  cuando   se   necesitar\'a  un   logaritmo
  especificamente de base $a$, se  notar\'a la divergencia $D_{\mathrm{kl},a}$ y
  la entrop\'ia cruzada $H_{\mathrm{cruz},a}$.}

\

\modif{La entrop\'ia relativa tiene varias propiedades y se v\'incula con la informaci\'on mutua.}

\begin{lema}[Positividad de la entrop\'ia relativa]
\label{Lem:SZ:PositividadEntropiaRelativa}
%
  \[  \Dkl[Q]{P}  \ge 0  \quad  \mbox{con  igualdad ssi}  \quad  P  = Q.  %\quad
  (c.s.)  \]
  %
\end{lema}
%
\begin{proof}
  Existen varias pruebas, pero la m\'as linda puede ser la usando la desigualdad
  de  Jensen~\footnote{En   el  caso  discreto,  se  puede   usar  tambi\'en  la
    desigualdad \ $\sum t_i \log t_i \ge  \sum t_i \log t'_i$ \ una instancia de
    la desigualdad conocida como  desigualdad log-sum, o conocida tambi\'en como
    desigualdad de  Gibbs (debido a J.  W.   Gibbs mismo)~\cite{Gib02, CovTho06,
      Rio07,         Mer10,        Mer18}.},        teorema~\ref{Teo:MP:Jensen}:
  %pagina~\pageref{Teo:MP:Jensen}:  
  para $\phi$ convexa  e \ $Y$ \ variable  aleatoria escalar, $\Esp[\phi(Y)] \ge
  \phi(\Esp[Y])$ \ con igualdad ssi \  $Y$ \ es determinista (casi siempre) si \
  $\phi$ es estrictamente  convexa.  Sea \ $X$ \ de  medida de probabilidad $P$.
  Se escribe la entrop\'ia  relativa \ $\Dkl[Q]{P} = \Esp\left[ \frac{dQ}{dP}(X)
    \log \left( \frac{dQ}{dP}(X) \right) \right]$.  Sea \ $Y = \frac{dQ}{dP}(X)$
  y  $\phi(u)  =  u  \log  u$,  funci\'on  estrictamente  convexa.   Entonces  \
  $\Dkl[Q]{P}  =  \Esp[\phi(Y)] \ge  \phi(\Esp[Y])$.   \  Pero \  $\displaystyle
  \Esp[Y]    =   \Esp\left[    \frac{dQ}{dP}(X)    \right]   =    \int_{\Rset^d}
  \frac{dQ}{dP}(x)       \,       dP(x)        =       1$       seg\'un       el
  lema~\ref{Lem:RelacionIntegracionDerivadasRadon}.
  %pagina~\pageref{Lem:RelacionIntegracionDerivadasRadon}.  \ 
  Se cierra  la prueba  con el hecho  que \  $\phi(1) = 0$  El caso  de igualdad
  apareciendo  si   y  solamente   si  \  $Y$   \  es  determinista,   es  decir
  $\frac{dP}{dQ}(X)$ \ determinista, es equivalente a \ $\frac{dP}{dQ} = 1 \quad
  (P$-c.s.)    \    (constante,   la   constante   siendo   igual    a   1   del
  lema~\ref{Lem:RelacionIntegracionDerivadasRadon} y  \ $P$ \  y \ $Q$  \ siendo
  medidas de probabilidad).
\end{proof}

Esta propiedad tiene consecuencias fijandose que
%
\[
I(X;Y) = \Dkl[P_{X,Y}]{P_X P_Y},
\]
%
\ie  la  informaci\'on  mutua  es  la  divergencia  de  Kullback-Leibler  de  la
distribuci\'on  conjunta  relativa al  producto  de  las marginales  (obviamente
$P_{X,Y} \ll  P_X P_Y$). Con este  enfoque, no se  necesita que \ $(X,Y)$  \ sea
discreta o continua admitiendo una densidad.
%
\begin{propiedades}
\item\label{Prop:SZ:Ipositive}   {\it   $I$   es   positiva,  como   medida   de
    independencia:}
  %
  \[
  I(X;Y) \ge 0 \quad \mbox{con igualdad ssi $X$ e $Y$ son independientes.}
  \]
%
\item\label{Prop:SZ:condicionar} {\it  Condicionar reduce la  entrop\'ia}
  %
  \[
  H(X|Y \| \mu) \le H(X \| \mu) \quad \mbox{con igualdad ssi $X$ e $Y$ son independientes.}
  \]
  %  
  Esta    desigualdad,   con    la    regla   de    la    cadena,   prueba    la
  sub-aditividad~\ref{Prop:SZ:subaditividad}.   Esta  reducci\'on  de  incerteza
  vale en  promedio, pero el conocimiento  de un valor particular  puede ser tal
  que $H(X|Y = y \| \mu) > H(X \| \mu)$, \ie  \ !`un conocimiento particular puede aumentar la
  entrop\'ia!  (ver ejemplos en~\cite[p.~59]{Rio07}).
\end{propiedades}

Fijense que  si $\Dkl{}$ es positiva,  no es sim\'etrica y  tampoco satisface la
desigualdad triangular.  Por eso, no es una  distancia y tiene el nombre de {\it
divergencia}.  La distribuci\'on de referencia $P$ juega un rol fundamental.

Al  final, se mencionar\'a las propiedades adicionales siguientes:
%
\begin{enumerate}
\item $\Dkl[Q]{P}$  queda invariante  bajo una misma  transformaci\'on biyectiva
  sobre  ambos $P$ y  $Q$. Es  trivial en  el caso  discreto y  si no  se prueba
  sencillamente por un cambio de variables en la forma integral.
%
\item $\Dkl{}$  es convexa con respecto al  par $(P,Q)$ en el  sentido que, para
  $\pi_i \ge 0,  \: \sum_i \pi_i = 1$,  y dos conjuntos $\{ P_{(i)}  \}_i, \: \{
  Q_{(i)} \}_i$ de medidas de probabilidades tales que $Q_{(i)} \ll P_{(i)}$,
  %
  \[
  \Dkl[\sum_i   \pi_i  \ Q_{(i)}]{\sum_i   \pi_i \   P_{(i)}}  \ge   \sum_i   \pi_i
  \Dkl[Q_{(i)}]{P_{(i)}}.
  \]
  %
  La  prueba de  esta desigualdad  es dada  subsecci\'on~\ref{Ssec:SZ:Csiszar}
  %, pagina~\pageref{Ssec:SZ:Csiszar} 
  en un contexto m\'as general.
%
\item Para  $Q$ fijo, $\Dkl[Q]{P}$ es convexa  con respecto a $P$  en el sentido
  que, para $\pi_i \ge 0, \: \sum_i  \pi_i = 1$, y un conjunto $\{ P_{(i)} \}_i$
  de medidas de probabilidades tales que $Q \ll P_{(i)}$,
  %
  \[
  \Dkl[Q]{\sum_i   \pi_i \,   P_{(i)}}  \ge   \sum_i   \pi_i
  \, \Dkl[Q]{P_{(i)}}.
  \]
  %
  Eso  es  la  consecuencia  obvia  de  la concavidad  de  $u  \mapsto  \log  u$
  (escribiendo la divergencia  como densidad con respecto a  una medida dada). Es
  sencillo ver que si $\Dkl{}$ siendo convexa con respecto a $P$ ($Q$ dada) y al
  par $(P,Q)$, no puede ser convexa con respecto a $Q$ (con $P$ dada).
\end{enumerate}
