\seccion{Entropias y divergencias generalizadas}
\label{sec:SZ:Generalizadas}

A pesar de que la entropia de Shannon y sus cantidades asociadas demostraron sus
potencias tan de un punto de vista descriptivo que en termino de aplicaciones en
la  transmisi\'on  de  la  informaci\'on  y  la  compresi\'on,  varios  nociones
informacionales, de  tipo entropias o  divergencias, aparecieron luego.  En esta
secci\'on no se  desarollar\'a todos los enfoques ni  todas las aplicaciones tan
la  literatura es  importante. La  meta  es dar  los caminos  conduciendo a  las
generalizaciones de la  entropia de Shannon por un lado, y  de la divergencia de
Kullback-Leibler por el otro lado. No son siempre vinculados, a pesar de que sea
desirable que a cada entropia sean asociados nociones de entropias condicionales
y relativas.

% ================================= Salicru

\subseccion{Entropias y propiedades}
\label{sec:SZ:Salicru}

% ----- Entropias generalizadas particulares
\paragraph{Unas  primeras  generalizaciones  particulares}  Si  la  entropia  de
Shannon fue el punto de salida fundamental  en todo el desarollo de la teoria de
la informaci\'on, un  poco m\'as de una  decada despues de su papel  clave y muy
completo,  R\'enyi propuso  una medida  generalizada~\cite{Ren61}.  Su  punto de
vista fue m\'as matem\'atico que  f\'isico o ingeniero.  Retom\'o los axiomas de
Fadeev~\cite{Fad56, Fad58, Khi57}
% Feinstein, cf ref  de Rényi
%
a  probabilidades  incompletas   $p  =  \begin{bmatrix}  p_1  &   \cdots  &  p_n
\end{bmatrix}^t,  \quad p_i  \ge  0,  \quad w_p  =  \sum_i p_i  \le  1$: (i)  la
invarianza de  $H(p)$ por permutaci\'on de  os $p_i$, (ii) la  continuidad de la
incerteza elemental  $H(p_i)$ ($p_i$ visto como  probabilidad incompleta), (iii)
$H\left( \frac12 \right) = 1 $,  (iv) la aditividad $H(p \otimes q) = H(p)+H(q)$
donde $p \otimes q$ es el producto de Kronecker~\footref{foot:SZ:Kronecker},
%\footnote{$\begin{bmatrix} p_1 &
%\cdots & p_n \end{bmatrix}^t \otimes \begin{bmatrix} q_1 & \cdots & q_m
%\end{bmatrix}^t = \begin{bmatrix} p_1 q_1 & \cdots  & p_1 q_m & \cdots & p_n q_1
%&  \cdots  &  p_n  q_m  \end{bmatrix}^t$.}, 
\ie  probabilidad conjunta  de dos  variables independientes,  y  consider\'o en
lugar de la recursividad un axioma  dicho de valor promedio, axioma muy parecido
a la recursividad. Para $p$ \ y \ $q$ probabilidades incompletas tales que $p \,
\cup  \,  q   =  \begin{bmatrix}  p_1  &   \cdots  &  p_n  &  q_1   &  \cdots  &
  q_m \end{bmatrix}^t$ sea incompleta ($w_p + w_q \le 1$), el axioma (v) es $H(p
\, \cup \,  q) = \frac{w_p \, H(p)  + w_q \, H(q)}{w_p +  w_q}$.  Demostr\'o que
con (v) en lugar  de la recursividad, el conjunto de axiomas  conduce de nuevo a
la  entropia de  Shannon.   La  generalizaci\'on propuesta  por  R\'enyi era  de
generalizar  el axioma  (v)  remplazando  la media  aritm\'etico  por una  media
generalizada (v')  $H^\ren(p \, \cup \,  q) = g^{-1} \left(  \frac{w_p \, g\big(
    H^\ren(p) \big) + w_q \,  g\big( H^\ren(q) \big)}{w_p + w_q}\right)$ con $g$
estrictamente  monotona y  continua,  llamado media  {\it cuasi-aritm\'etica,  o
  quasi-lineal,  o  de  Kolmogorov-Nagumo}.   De  las propiedades  de  la  media
cuasi-aritmetica~\cite{Nag30,  Kol30,  Kol91, HarLit52},  eso  es equivalente  a
buscar una  entropia elemental $H^\ren(p_i)$  y remplazar la  media aritm\'etica
$\sum_i  p_i H^\ren(p_i)$  por una  media de  Kolmogorov-Nagumo,  $g^{-1} \left(
  \sum_i p_i g\big( H^\ren(p_i)  \big) \right)$.  R\'enyi propus\'o la funci\'on
de Kolmogorov-Nagumo $g_\beta(x) = 2^{(\beta-1)  x}, \quad \beta >0, \quad \beta
\ne 1$,  probando que  la entropia que  los axiomas  (i)-(ii)-(iii)-(iv)-(v') se
cumplen y conduce a la entropia de R\'enyi de un vector de probabilidad $p$,
%
\[
H_\beta^\ren(p) = \frac{1}{1-\beta} \log_2 \left( \sum_{i=1}^n p_i^\beta \right)
\]
%
\noindent   Relaxando  el  axioma   (iii),  se   puede  elegir   $g_\beta(x)  =
a^{(\beta-1) x}, \quad  a > 0, \quad a  \ne 1$; el logaritmo ser\'a  de la base
$a$ cualquiera; En  lo que sigue, usaremog $\log$ sin  precisar la elecci\'on de
base.   R\'enyi  nombr\'o  esta  medida  de incerteza  {\it  entropia  de  orden
$\beta$}. Notablemente,
%
\[
H_1^\ren(p)  \equiv   \lim_{\beta  \to   1}  H_\beta^\ren(p)   =   H(p)  \quad
\mbox{entropia de Shannon}
\]
% \noindent En otros terminos, la clase de R\'enyi contiene como caso particular
la  entropia  de  Shannon.  En  su  papel,  R\'enyi  introdujo una  ganancia  de
informaci\'on,  parecida  a  una  entropia  relativa,  probando  que  las  solas
entropias admisibles  son la  de Shannon  y la que  introdujo. Volveremos  en la
secci\'on siguiente sobre esta entropia  relativa, o divergencia de R\'enyi. Por
axiomas,     las     propiedades    \ref{prop:SZ:continuidad}     (continuidad),
\ref{prop:SZ:permutacion}       (invarianza      por       permutaci\'on)      y
\ref{prop:SZ:aditividad} (additividad)  de la  entropia de Shannon  se conservan
entonces  en  el  marco   de  R\'enyi  y  se  pierde  \ref{prop:SZ:recursividad}
(recursividad), todav\'ia por axiomas. Veremos luego la otras que se conservan o
modifican en un marco m\'as general.

Unos  a\~nos despu\'es de  R\'enyi, de  la famosa  escuela matematica  checa, J.
Havrda   \&    F.    Charv\'at   en~\cite{HavCha67}    (ver   tambi\'en~\cite[en
checo]{Vaj68}) volvieron a los axiomas de Khintchin, para extender la entopia de
Shannon,  \ie  considerando  (i)   la  invarianza  por  permutaci\'on,  (ii)  la
continuidad, (iii) la expensividad, (iv) $H^\hc(1) = 0$ y $H^\hc\left( \frac12 ,
  \frac12   \right)  =   1$,  pero   generalisando  la   recursividad   por  (v)
$H^\hc(p_1,\ldots,p_n)    =   H^\hc(p_1,\ldots,p_{n-2},p_{n-1}+p_n)    +   \beta
(p_{n-1}+p_n)^\beta H^\hc\left( \frac{p_{n-1}}{p_{n-1} + p_p},\frac{p_n}{p_{n-1}
    + p_p}  \right), \quad  \beta > 0$~\footnote{En  sus papel, lo  imponen para
  cualquier pars $p_i, p_j$ sin imponer la invarianza por permutaci\'on, pero es
  equivalente a la  exposici\'on de este parafo.}.  Con $\beta  = 1$ se recupera
la  recursividad estandar,  pero  con $\beta  \ne  1$ eso  permite  dar un  peso
diferente a la incerteza del estado interno \ie probabilidades que se juntan (la
describen como clasificaci\'on refinada).  Estos axiomas conducen necesariamente
a la entropia (teorema~1)
%
\[
H_\beta^\hc(p) = \frac{1}{1-2^{1-\beta}} \left( 1 - \sum_i p_i^\beta \right)
\]
%
que nombraron {\it $\beta$-entropia structural}.  De nuevo, relaxando el axioma
(iv),  se puede  remplazar en  el coeficient  $2^{1-\beta}$  por $a^{1-\beta},
\quad a >  0, \quad a \ne 1$. De  nuevo, parae que la entropia  de Shannon es un
caso particular,
%
\[
H_1^\hc(p)  \equiv   \lim_{\beta  \to   1}  H_\beta^\hc(p)   =   H(p)  \quad
\mbox{entropia de Shannon}
\]
% Por   axioma,   se   conservan   las   propiedades   \ref{prop:SZ:continuidad}
(continuidad), \ref{prop:SZ:expansabilidad} (expansabilidad)  de Shannon en este
marco. Se  prob\'o tambi\'en  que se conserva  la propiedad de  concavavidad con
respeto    a   los   $p_i$    \ref{prop:SZ:concavidad},   la    de   maximalidad
\ref{prop:SZ:cotamaxima}    alcanzada   para    una    distribuci\'on   uniforma
(teorema~2). Aun  que no aparece  as\'i en el  papel, satisface la  propiedad de
Schur-concavidad  \ref{prop:SZ:Schurconcavidad}  (teorema~3).  A  pesar  de  que
mencionan que $H_\beta^\hc$ sea  diferente que $H_\beta^\ren$, es sencillo ver
que hay  un mapa  uno-uno entre  las dos entropias.  Se notara  en un  marco m\'as
general otras propiedades.

Independiente  de Havrda  \&  Charv\'at, todav\'ia  en  el este,  en la  escuela
h\'ugara, Z.  Dar\'oczy en~\cite{Dar70} defino la entropia $H^f$ a partir de una
{\it  funci\'on  informaci\'on}  $f$   satifaciendo  (i)  $f(0)  =  f(1)$,  (ii)
$f\left(\frac12\right)  = 1$  \ y  la ecuaci\'on  funcional (ii)  $f(x)  + (1-x)
f\left(  \frac{y}{1-x} \right)  = f(y)  + (1-y)  f\left(  \frac{x}{1-y} \right)$
sobre  $\{  (x,y)  \in [0  ;  1)^2,  \quad  x+y  \le  1 \}$,  siendo  $H^f(p)  =
\sum_{i=2}^n s_i  f\left( \frac{p_i}{s_i} \right), \quad  s_i = \sum_{j=1}^{i-1}
p_j$.   Dar\'oczy mostr\'o  que  si $f$  es medible,  o  continua en  $0$, o  no
negatiba  y  acotada, necesariamente  $f(x)  =  h_2(x) =  -x  \log_2  x -  (1-x)
\log_2(1-x)$,   conduciendo   a  la   entropia   de   Shannon  (teorema~1;   ver
tambi\'en~\cite{Lee64,  Tve58, Ken64}).   En otros  terminos, su  axioma  (v) es
alternativa a  la recursividad.  Para  extender la entropia de  Shannon, propuso
extender este axioma  (v) por la ecuaci\'on funcional  $f_\beta(x) + (1-x)^\beta
f_\beta\left(  \frac{y}{1-x} \right)  = f_\beta(y)  +  (1-y)^\beta f_\beta\left(
  \frac{x}{1-y}  \right)$,   lo  que   condujo  necesariamente  a   la  entropia
(teoremas~2 y~3)
%
\[
H_\beta^\dar(p) = \frac{1}{1-2^{1-\beta}} \left( 1 - \sum_i p_i^\beta \right)
\]
%
\noindent  es  decir  nada  m\'as  que  la  entropia  introducida  por  Havdra  \&
Charv\'at. En lo que sigue, se la denota $H_\beta^\hcd$. Sin embargo, el estudio
de Dar\'oczy fue  m\'as intensivo que el de Havdra  \& Charv\'at.  Primero, not\'o
el mapa  entre su  entropia y la  de R\'enyi. Adicionalmente  a Havdra-Charv\'at
probaron que se conserva  la propiedad \ref{prop:SZ:permutacion} (invarianza por
permutaci\'on, que no era un  axioma en su enfoque), $H_\beta^\hcd\left( \frac12
  ,  \frac12   \right)  =  1$   (lo  llama  normalizaci\'on),   la  expansividad
\ref{prop:SZ:expansabilidad},   una  additividad  extendida,   una  recursividad
extendida  precisamente  del modelo  de  Havrda-Charv\'at (teorema~4).   Prob\'o
tambi\'en   \ref{prop:SZ:positividad},   positividad   alcanzado  en   el   caso
deterministico y  la m\'aximalidad \ref{prop:SZ:cotamaxima} en  el caso uniforme
(teorema~6),  que incidentalmente  $H_\beta^\hcd\left( \frac1\alpha  ,  \ldots ,
  \frac1\alpha \right)$ crece con el  cardinal $|\X| = \alpha$.  Muy interesante
tambi\'en es se puede definir una entropia condicional en el mismo modelo que en
el caso de Shannon $H_\beta^\hcd(X|Y) = \sum_y \left[ p_{X|Y}(x,y) \right]^\beta
H_\beta^\hcd(   p_{X|Y}(\cdot,y)   )$,   que   existe  una   regla   de   cadena
\ref{prop:SZ:cadena}, $H_\beta^\hcd(X,Y)  = H_\beta^\hcd(Y) + H_\beta^\hcd(X|Y)$
y    que    \ref{prop:SZ:condicionar}    condicionar    reduce    la    entropia
$H_\beta^\hcd(X|Y) \le H_\beta^\hcd(X)$  (teorema~8).  Mostr\'o tambi\'en que si
se pierde  la additividad, se  obiene para \  $X$ \ e  \ $Y$ \  independientes \
$H_\beta^\hcd(X,Y) = H_\beta^\hcd(X) +  H_\beta^\hcd(Y) + \left( 2^{1-\beta} - 1
\right) H_\beta^\hcd(X) H_\beta^\hcd(Y)$.  La  propiedades de regla de cadena le
permiti\'o  revisitar  la  caracterisaci\'on  de  un canal  de  transmisi\'on  y
redefinir una capacidad canal extendidas (capacidad tipo $\beta$; basicamente se
usa el  mismo enfoque que Shannon,  pero usando $H_\beta^\hcd$ en  lugar de $H$,
ver secci\'on~6 del papel).

% ----- Entropias generalizadas h,phi
\paragraph{Una  clase  m\'as  general   y  sus  propiedades}  Las  entropias  tipo
Havdra-Charv\'at-Dar\'oczy  fueron  (re)descubiertos   varios  otras  veces  y/o
estudiados  m\'as  detenidamente en  varios  campos  y  varios extensiones  fueron
introducidas~\cite[entre  otros]{Var66, Oni66,  Kap67,  Vaj68, LinNie71,  Ari71,
  Bur72, AczDar75, ShaMit75, ShaMit75,  ShaTan75, Mit75, BoeLub80, Fer80, Tsa88,
  Rat91, Kan01, Bec09}.   Un primer enfoque m\'as general es  debido a S.  Arimoto
en  los  primeros  a\~nos  de  la decada  1970~\cite{Ari71}  y  rediscubierto  y
estudiado con m\'as detaller y una decada despues por Burbea y Rao~\cite{BurRao82}
y  luego estudiado  por  Salicr\'u~\cite{Sal87}.  La  medida propuesta,  llamada
$\phi$-entropia, es definida por
%
\[
\hphi[p]  =   -\sum_i  \phi(p_i)  \qquad   \mbox{con}  \qquad  \phi   \:  \mbox{
  estrictamente convexa}
\]
%
Burbea  y  Rao  asociaron  una  medida  de  divegencia  a  esta  entropia.   Las
$\phi$-entropias contienen Shannon como caso  particular ($\phi(x) = x \log x$),
as\'i  que  la  clase   de  Havdra-Charv\'at-Dar\'oczy  ($\phi(x)  =  \frac{x  -
  x^\beta}{2^{1-\beta}-1}$)  como mencionado, pero  no la  clase de  R\'enyi. De
hecho, las $\phi$-entropias se enmarcan en una clase un poco m\'as amplia, llamada
$(h,\phi)$-entropias~\cite{SalMen93, MenMor97}. Cambiamos ac\`a substancialmente
su escritura por razones homogeneidad con la $\phi$-entropia (y las divergencias
que se introducira luego)~\footnote{En la literatura, no hay el signo $-$, y hay
  que invertir concava y convexa.}
%
\begin{definicion}[$(h,\phi)$-entropia]\label{def:SZ:HPhiEntropia}
La $(h,\phi)$-entropia de una  distribuci\'on de probabilidad $p_X$ definida sobre
$\X$ de cardinal finito $|\X| = \alpha$ es definida por
%
\[
\hhphi[X]  = \hhphi[p_X]  = h\left(  - \sum_{x \in  \X} \phi\left(  p_X(x) \right)
\right)
\]
%
donde o
%
\begin{itemize}
\item $\phi$ \ es estrictamente convexa y \ $h$ \ creciente, o
\item $\phi$ \ es estrictamente concava y \ $h$ \ decreciente
\end{itemize}
%
Frecuentemente, se  supone adicionalmente que $\phi$  y $h$ son  de clase $C^2$,
que $\phi(0) = 0$ (incerteza elemental asociada a un estado de probabilidad nula
vale cero) y, sin perdida de generalidad, que $h(-\phi(1)) = 0$.
\end{definicion}
%
\noindent  (ver  tambi\'en~\cite{Est97}  para  una  generalizaci\'on  a\'un  m\'as
amplia). Cu\'ando  $h(x) = x$ se  recupera la $\phi$-entropia,  incluyendo la de
Shannon y las  de Havdra-Charv\'at-Dar\'oczy. Ademas, la familia  de R\'enyi cae
tamb\'ien  en esta  familia ($\phi(x)  = -  x^\beta$ \  y \  $h(x)  = \frac{\log
  x}{1-\beta}$) as\'i que todas las entropias evocadas en el parafo anterior.

Como en el caso de  Shannon, para $X = (X_1,\ldots,X_d)$, la $(h,\phi)$-entropia
de $X$ es una $(h,\phi)$-entropia conjunta de los $X_i$.

Obviamente,  de  las  propiedades  de  la  entropia  de  Shannon,  se  conservan
\ref{prop:SZ:continuidad}  (continuidad),  \ref{prop:SZ:permutacion}  (invariaza
por  permutaci\'on),  \ref{prop:SZ:biyeccion}  (invarianza por  transformaci\'on
biyectiva  de  $X$),   \ref{prop:SZ:expansabilidad}  (expansabilidad,  debido  a
$\phi(0) = 0$).

Ademas  se conserva  la  Schur-convavidad \ con una reciproca:
%
\begin{propiedadesPhi}\setcounter{enumi}{\value{PropSchurConcavidad}}
%
\item Schur-convavidad:
  %
  \[
  p \prec q \quad \Longleftrightarrow \quad \hhphi[p] \ge \hhphi[q] \quad \forall \:
  (h,\phi)
  \]
  %
  % \centerline{Reciprocamente,  si $\quad \forall \:  (h,\phi), \quad \hhphi[p]
  %   \ge \hhphi[q] \quad \mbox{ entonces } \quad p \prec q$}
  %
  En otros terminos, se obtiene la  relaci\'on de mayorisaci\'on si se cumple la
  relaci\'on  de ordre  entropicas para  cualquier par  de  funciones entropicas
  $(h,\phi)$.   La  Schur-concavidad (y  su  reciproca)  es  consecuencia de  la
  desigualdad  de Schur~\cite{Sch23}  o Hardy-Littlewood-P\'olya~\cite{HarLit29,
    HarLit52} o Karamata~\cite{Kar32}  (ver tambi\'en~\cite[Cap.~3, Prop.~C.1 \&
  Cap.~4,  Prop.~B.1]{MarOlk11} o~\cite[Teorema~II.3.1]{Bha97}):  $p \prec  q \:
  \Rightarrow  \: \sum_i  \phi(p_i) \le  \sum_i \phi(q_i)$  para  toda funci\'on
  $\phi$ convexa.
\end{propiedadesPhi}
% Schur, I.  (1923). Issai Schur Collected  Works (A.  Brauer  and H.  Rohrbach,
% eds.), Vol. II. pp. 416œôòó427. Springer-Verlag, Berlin, 1973]
%
%
% Ver Schur-Ostrowski f  sym, Scur-convexe ssi (xi - xj)  (df/dx_i - df/dxj) \ge
% 0, 1 \le i \ne j \le alpha
%
Como consecuencia, se  conservan la positividad \ref{prop:SZ:positividad} gracia
a $\phi(0)  = 0$ y  $h(\phi(1)) = 0$  (alcanzado en el caso  deterministico), la
maximalidad \ref{prop:SZ:cotamaxima} (caso uniforme),
%
\[
0 \le \hhphi[p_X] \le h\left( - \alpha \, \phi\left( \frac1\alpha \right) \right)
\]
%
as\'i que
%
\[
\hhphi[\begin{bmatrix}  \frac1\alpha &  \cdots  & \frac1\alpha  \end{bmatrix}^t]
\quad \mbox{funci\'on creciente de } \alpha
\]

Con respeto a la concavidad  \ref{prop:SZ:concavidad}, no se conserva en general:
%
\begin{propiedadesPhi}\setcounter{enumi}{\value{PropConcavidad}}
\item\label{prop:SZ:concavidadHPhi} Si $h$  est concava, entonces $\hhphi[p]$ es
  concava con respeto a $p$.  Eso es una consecuencia de la concavidad de $\phi$
  y  decrecencia  de  $h$   (resp.   convexidad/crecencia)  conjuntamente  a  la
  concavidad de $h$.  La reciproca no  es verdad.  Por ejemplo, se puede ver que
  si $\beta < 1$, la entropia de R\'enyi es concava, pero se proba que existe un
  $\beta^*(\alpha) >  1$ tal que  para cualquier $\beta \le  \beta^*(\alpha)$ se
  conserva  la   concavidad,  a   pesar  de  que   $h$  no   sea  necesariamente
  concava~\cite[p.~57]{BenZyc06}.
\end{propiedadesPhi}

Se pierde la propiedad de recursividad \ref{prop:SZ:recursividad}, pero se puede
vincular  la  entropia  total con  la  obtenida  juntando  dos estados  por  una
desigualdad:
%
\begin{propiedadesPhi}\setcounter{enumi}{\value{PropRecursividad}}
\item  Sean \  $X$ \  definido  sobre \  $\X$ \  y  \ $\overline{X}$  \ sobre  \
  $\overline{X}$,
  %
  \[
  \left\{  \begin{array}{l}\overline{\X} =  \{  x_1 ,  \ldots  , x_{\alpha-2}  ,
      \overline{x}_{\alpha-1}\}  \quad   \mbox{con  el  estado   interno}  \quad
      \overline{x}_{\alpha-1}   =  \{   x_{\alpha-1}  ,   x_\alpha  \},\\[2.5mm]
      p_{\overline{X}}(x_i)  = p_X(x_i),  \quad 1  \le i  \le \alpha-1  \quad \mbox{y}
      \quad p_{\overline{X}}(\overline{x}_{\alpha-1}) = p_X(x_{\alpha-1}) +
      p(x_\alpha)  \quad  \mbox{distribuci\'on  sobre  }  \overline{\X}\\[2.5mm]
      \displaystyle   \overline{q}(x_j)    =   \frac{p_X(x_j)}{p_X(x_{\alpha-1})   +
        p_X(x_\alpha)}, \quad j =  \alpha-1, \alpha \quad \mbox{distribuci\'on del
        estado   interno}\end{array}\right.
  \]
  %
  \[
  \hhphi[p_X] \ge \hhphi[p_{\overline{X}}]
  \]
  %
  Esta  desigualdad es  consecuencia de  la desigualdad  de Petrovi\'c~\cite[43,
  Teorema~8.7.1]{Kuc09}, $\phi(a + b) \ge \phi(a) + \phi(b)$ para $\phi$ convexa
  y que se  cancela en 0 (y  la conversa en el caso  concavo), conjuntamente con
  $h$ creciente  (resp.  decreciente).  A  parte en el  caso de Shannon y  el de
  Havdra-Charv\'at-Dar\'oczy, no hay un  vinculo inmediato entre $\hhphi[p_X]$ \
  y \ $\hhphi[p_{\overline{X}}]$.
\end{propiedadesPhi}

Se  conserva  la  superadditividad~\ref{prop:SZ:superaditividad}. De  hecho,  si
$\phi$ es convexa (resp. concava) con \ $\phi(0) = 0$, \ $\forall \: 0 \le a \le
1,  \: \phi(a  u)  = \phi(a  u  + (1-a)  0) \le  a  \phi(u)$ (resp.  desigualdad
reversa).  Entonces,   \  $\phi\left(  p_{X,Y}(x_i,y_j)   \right)  =  \phi\left(
  p_{X|Y}(x_i,y_j)  p_Y(y_j)  \right)  \le  p_{X|Y}(x_i,y_j)  \phi\left(p_Y(y_j)
\right)$, \ \ie \ $\sum_{i,j} \phi\left( p_{X,Y}(x_i,y_j) \right) \le \sum_{i,j}
p_{X|Y}(x_i,y_j) \phi\left(p_Y(y_j) \right) = \sum_i \phi\left(p_Y(y_j) \right)$
\  (resp.  desigualdad  reversa).  Se   cierra  la  prueba  con  la  crecencia
(resp. decrecencia) de $h$.


Sin  embargo, en  general, se  pierden las  propiedades \ref{prop:SZ:aditividad}
(additividad),  y la  propidad~\ref{prop:SZ:subaditividad}  (subadditividad). En
particular, se conserva solamente en el caso Shannon:
%
\begin{teorema}
  Sea $p_{X,Y}$ distribuci\'on conjunta  de variables aleatorias discretas \ $X$
  \ y \ $Y$ \ y \ $p_X$ \ y \ $p_Y$ \ las de \ $X$ \ y de \ $Y$ (marginales).
%
\[
\hhphi[p_{X,Y}] \:  \le \:  \hhphi[p_X \otimes  p_Y]
\quad \forall  \: p_{X,Y}  \qquad \Longleftrightarrow \qquad \phi(x) = x \log x
\]
%
\ie $\hhphi$ es una funci\'on creciente de la entropia de Shannon
\end{teorema}
%
\begin{proof}
  La     reciproca    de    este     teorema    es     nada    m\'as     que    la
  propidad~\ref{prop:SZ:subaditividad} con  el hecho de que $h$  es creciente en
  este caso.

  A continuaci\'on, la parte directa se demuestra en dos etapas:
  %
  \begin{itemize}
  \item Con  un caso particular  sobre $\X$  e $\Y$ de  cardenal 3 cada  unos se
    proba  de que  la desigualdad  no se  puede cumplir,  salvo si  la funci\'on
    entropica $\phi'$ satisface a una ecuaci\'on funcional.
  %
  \item la sola soluci\'on admisible de esta ecuaci\'on se recuce a $\phi(x) = -
    x \ln x$.
  \end{itemize}
  %
  {\bf Etapa 1}: Sea el vector de probabildad
  %
  \[
  p_{X,Y}  =  p_X  \otimes   p_Y  -  c  \begin{bmatrix}  1\\-1\\0  \end{bmatrix}
  \otimes  \begin{bmatrix} 1\\-1\\0 \end{bmatrix}  \qquad \mbox{con}  \qquad p_X
  = \begin{bmatrix} a\\\alpha\\1-a-\alpha \end{bmatrix} \quad \mbox{y} \quad p_Y
  = \begin{bmatrix} b\\\beta\\1-b-\beta \end{bmatrix}
  \]
  %
  donde $(a,\alpha,b,\beta) \in D$,
  %
  \[
  D = \{  a, \alpha, b, \beta: \quad 0  < a,b < 1 \quad \wedge  \quad 0 < \alpha
  \le  1-a \quad  \wedge  \quad 0  < \beta  \le  1-b \}
  \]
  %
  y \ $c \in C_{a,\alpha,b,\beta}$,
  %
  \[
  C_{a,\alpha,b,\beta} = \big[ - 1 + \max\big\{ a b , \alpha \beta , 1 - a \beta
  , 1 - \alpha b \big\} \, , \, \min\big\{  a b , \alpha \beta , 1 - a \beta , 1
  -\alpha b \big\} \big]
  \]
  %
  Ahora, si $\phi$ es  convexa (resp. concava)
  %
  \[
  \forall \, u,v \quad \phi(v)  - \phi(u) \:  \ge \:  (v-u) \, \phi'(u),
  \]
  %
  \ie la variaci\'on  (cuerda) es mayor que la derivada  en $b$, como ilustrado
  figura~\ref{fig:SZ:ConvexidadDerivada}   (desigualdad  reversa   para  $\phi$
  concava).
  %
  \begin{figure}[h!]
  %
  \begin{center} \input{TIKZ_SZ/ConvexidadDerivada} \end{center}
  %
  \leyenda{$\phi$ estrictamente convexa:  la variaci\'on (cuerda) $\frac{\phi(v)
      -  \phi(u)}{v-u}$ es  mayor que  la derivada  $\phi'(u)$.  Aplicado  a dos
    distribuciones $p$ y $q$, de componentes $p_i$ y $q_i$, con $u = p_i$ y $v =
    q_i$  y  sumando, se  obtiene  $\hphi[q]  -  \hphi[p] \ge  \sum_i  (p_i-q_i)
    \phi'(p_i)$ con $\hphi \equiv H_{(\id,\phi)}, \: \id$ siendo la identidad.}
  %
  \label{fig:SZ:ConvexidadDerivada}
  \end{figure}
  
  Aplicamos esta desigualdad a \ $u = p_{X,Y}(x,y)$ \ y \ $v = p_X(x)
  p_Y(y)$ \ y sumamos en  $x, y$, \ para \ $(a,b) \in ( 0  \, , \, 1)^2$ \ (para
  que  $C_{a,\alpha,b,\beta}$  \ no  sea  reducido  a \  $\{0\}$),  y  \ $c  \in
  \mathring{C}_{a,\alpha,b,\beta}$  \  donde \  $\mathring{\cdot}$  \ denota  el
  interior de un conjunto, se obtiene para $\phi$ convexa,
  %
  \[
  \hphi[p_X \otimes p_Y] - \hphi[p_{X,Y}] \: \le \: c \: g(a,\alpha,b,\beta,c),
  \]
  %
  (para $\phi$  concava se  remplaza $\hphi$ por  $-H_{-\phi}$ con  la igualdad
  inversa), donde
  %
  \begin{equation}
  g(a , \alpha , b , \beta , c) = \phi'\big( a b + c \big) + \phi'\big( \alpha
  \beta + c \big) - \phi'\big( a \beta - c \big) - \phi'\big( \alpha b - c \big).
  \end{equation}
  %
  Si existe \ $(s,u,t,v) \in \mathring{D}$  \ tal que \ $g(s,u,t,v,0) \ne 0$. De
  la continuidad de $\phi'$, la funci\'on  $g$ es continua, y entonces exista un
  vecinaje  \ $V_0  \subset \mathring{C}_{s,u,t,v}$  \  de \  $0$ \  tal que  la
  funci\'on  \ $c  \mapsto  g(x,u,y,v,c)$ \  tiene  un signo  constante sobre  \
  $V_0$. \ Eso permite  concluir que \ $c \mapsto c \,  g(x,u,y,v,c)$ \ no tiene
  un  signo constante  sobre  \ $V_0$,  \ y  entonces  de concluire  que, de  la
  desigualdad  dedibo   a  la  concavidad  de  $\phi$   (resp.   convexidad),  \
  $\hphi[p_{X,Y}]$ puede ser major  (resp.  menor) que $\hphi[p_X \otimes p_Y]$,
  y   entonces,  con   la  crecencia   (resp.   decrecencia)   de  $h$   que  si
  $g(a,\alpha,b,\beta,0)$  no   es  identicamente  cero   sobre  $\mathring{D}$,
  $\hhphi$ no puede ser subadditiva (conjunta vs product of marginales).

  {\bf Etapa 2}.  Si  $g(a,\alpha,b,\beta,0) = 0$ sobre $\mathring{D}$, entonces
  $\phi'$ satisface la ecuaci\'on funcional
  %
  \[
  \phi'\big( a  b \big)  + \phi'\big(  \alpha \beta \big)  - \phi'\big(  a \beta
  \big) - \phi'\big( \alpha b \big) = 0,
  \]
  %
  as\'i que  no se  puede usar el  argumento de  la etata~1 para  concluir.  Sin
  embargo,     se     puede     solucionar    esta     ecuaci\'on     funcional,
  siguiendo~\cite[\S~6]{DarJar79} donde una  ecuaci\'on funcional muy similar es
  estudiada.  Por  eso, se fija \  $(a,b) \in (0 \,  , \, 1)^2$, \  se deriva la
  identidad precediente  con respeto a \  $\alpha$ \ se  multiplica el resultado
  por $\alpha$ \ para obtener
  %
  \[
  \alpha \beta  \, \phi''(\alpha  \beta) = \alpha  b \, \phi''(\alpha  b) \qquad
  \mbox{for} \qquad (\alpha,\beta) \in (0 \, , \, 1-a) \times (0 \, , \, 1-b).
  \]
  %
  Eso significa  de que $x \,  \phi''(x)$ es constante sobre  $x \in (0  \, , \,
  (1-a)  \max\{b,1-b\})$, y  para cualquier  par $(a,b)  \in (0  \, ,  \, 1)^2$.
  Entonces, $x \, \phi''(x)$ es constante sobre $x \in (0 \, , \, 1 )$, es decir
  que $\phi$ es necesariamente de la forma $\phi(x) = \lambda \, x \ln x + \mu x
  + \nu$. Debido  a la continuidad de $\phi$, queda valide  sobre el cerrado $[0
  \, , \, 1]$.  De que se aplica  a un vector de probabilidad, sumando a uno, se
  puede reducir el problema a $\mu =  0$ (poniendo $\mu$ en $\nu$ sin cambiar el
  valor  de entropia).   Ademas,  la  constante $\nu$  no  altera la  concavidad
  (resp. convexidad) de $\phi$, as\'i que se la puede translatar en la funci\'on
  $h$   (sin  cambiar   la  monotonicidad).    Para  que   $\phi$   sea  convexa
  (resp. concava)  hace falta tener $\lambda  > 0$ (resp.  $\lambda  < 0$) as\'i
  que,  sin perdida  de generalidad,  $\lambda$  puede ser  puesta tambi\'en  en
  $h$. Tomar \ $\phi (x) = x \, \ln x$ \ con \ $h$ \ creciente o \ $\phi (x) = -
  x \, \ln x$ \ con \  $h$ \ decreciente es completamente equivalente, as\'i que
  se puede fijar $\phi (x) =  x \, \ln x$ satisfaciendo la ecuaci\'on funcional,
  y $h$ creciente.

  $\hphi  =   H$  siendo  subadditiva  (propidad~\ref{prop:SZ:subaditividad}),
  cualquier funci\'on creciente de $H$  va obviamente quedar subadditiva, lo que
  cierra la prueba.
  %
\end{proof}
%
Al rev\'es, a partir de $p_{XY}  = \frac12 \begin{bmatrix} 1 & 0 \end{bmatrix}^t
\otimes\begin{bmatrix}  1 &  0  \end{bmatrix}^t +  \frac12  \begin{bmatrix} 0  &
  1  \end{bmatrix}^t \otimes\begin{bmatrix}  0 &  1 \end{bmatrix}^t$  se obtiene
$p_X = p_Y =  \frac12 \begin{bmatrix} 1 & 1 \end{bmatrix}^t$ \  y entonces \ (i)
$\hhphi[p_{XY}]  =  h\left(  -  2  \, \phi\left(  \frac12  \right)  \right)$,  \
$\hhphi[p_X \otimes p_Y] = h\left( -  4 \, \phi\left( \frac14 \right) \right)$ \
y \ $\hhphi[p_X] + \hhphi[p_Y] = 2  \, h\left( - 2 \, \phi\left( \frac12 \right)
\right)$, \  as\'i que, en este  ejemplo \ $\hhphi[p_{XY}]  > \hhphi[p_X \otimes
  p_Y]$  \   (consecuencia  de  la  Schur-convavidad)  y   \  $\hhphi[p_{XY}]  >
\hhphi[p_X]   +   \hhphi[p_Y]$:   Tampoco   las   $(h,\phi)$-entropia   no   son
super-additivas.

\

La  definici\'on  de entropias  generalizadas  condicionales  aparece mucho  m\'as
problematico. Por  ejemplo, si  se define  a la Shannon,  es decir  definiendo \
$\hhphi[X|Y]$ \ tomando \ $\sum_{y \in \Y} p_Y(y) \hhphi[p_{X|Y}(\cdot,y)]$ \ se
pierde la regla de cadena~\ref{prop:SZ:cadena}. Como se lo ha visto, en el marco
de la entropia de Havdra-Charv\'at-Dar\'oczy,  se conserva la regla de cadena si
se remplaza  $p_Y$ por su  potencia $p_Y^\beta$.  Sin embargo,  generalizar este
esquema en el caso general  falla (la gracia en Havdra-Charv\'at-Dar\'oczy viene
de  la  propiedad  de  morfismo   de  la  exponencial  y  del  logaritmo).  Como
consecuencia,  generalizar la  noci\'on se  vuelve problematico  tambi\'en.  Por
ejemple  se  pierde  el  diagrama  de  Venn aparte  si  se  define  la  entropia
condicional  a  partir  de  la  regla  de  cadena. Pero  en  este  caso,  si  la
superadditividad garantiza la positividad  de la entropia condicional, se pierde
la   propiedad~\ref{prop:SZ:independenciacondicional}    por   perdida   de   la
additividad,       y      por       consecuencia      la       propiedad      de
positividad/independencia~\ref{prop:SZ:Ipositive}  de  una  informaci\'on  mutua
construida sobre un  modelo diagrama de Venn. Veremos  en la secci\'on siguiente
que un tercero camino puede ser usar divergencia.
% \SZ{ver con detalles \ref{prop:SZ:condicionar} (condiconar)} \SZ{VajVas85 pour
%   la Schur-concavite}

\

Como en el caso de Shannon, se puede extender la generalizaci\'on de la entropia
al  casi  de  vectores  aleatorios  admitiendo  una  densidad  de  probabilidad,
remplazando la suma por una integraci\'on.

\begin{definicion}[$(h,\phi)$-entropia diferencial]
  \label{def:SZ:HPhiEntropiaDiferencial}
  %
  Sea $X$ una  variable aleatoria definida sobre un  espacio $d$-dimensional $\X
  \subseteq \Rset^d$ y sea $p_X(x)$ la densidad (distribuci\'on) de probabilidad
  de $X$, La $(h,\phi)$-entropia diferencial de la variable $X$ es definida por
  %
  \[
  \hhphi[p_X] =  \hhphi[X] = h\left( -  \int_\X \phi\left( p_X(x)  \right) \, dx
  \right)
  \]
  %
  con  $h$  \   y  \  $\phi$  cumpliendo  los   requisitos  de  la  definici\'on
  discreta~\ref{def:SZ:HPhiEntropia}  (de   $\phi(0)$,  se  puede   escribir  la
  integraci\'on en $\Rset^d$).
\end{definicion}

De nuevo para $X =  (X_1,\ldots,X_d)$, la $(h,\phi)$-entropia diferencial de $X$
es una $(h,\phi)$-entropia diferencial conjunta de los $X_i$.

La  versi\'on  diferencial de  la  $(h,\phi)$-entropia  comparte obviamente  las
mismas debilidades  del caso particular de  Shannon: se pierden  la propiedad de
invarianza    por   transformaci\'on    biyectiva~\ref{prop:SZ:biyeccion},   \ie
independencia  de los estados,  la positividad~\ref{prop:SZ:positividad},  la de
cota  superior~\ref{prop:SZ:cotamaxima}  (salvo si  se  pone  vinculos, ver  m\'as
adelante), en adici\'on de las que ya la versi\'on discreta perdi\'o.

Sin embargo, se conservan unas propidedades, y entre otros si $h$ es concava, la
$(h,\phi)$-entropia  diferencia el  concava~\ref{prop:SZ:concavidadHPhi}.  M\'as
sopredentemente a  primer vista,  se conserva la  $(h,\phi)$-entropia diferencia
bajo un rearreglo~\ref{prop:SZ:permutacionC},
%
\[
\hhphi[p_X^\downarrow] = \hhphi[p_X]
\]
%
\noindent De  hecho, como evocado en el  caso de Shannon, eso  fue probado entre
otros  en~\cite{LieLos01} o  \cite[Lema~7.2]{WanMad04}~\footnote{Recuerdense que
  en~\cite[Sec.~3.3]{LieLos01} lo muestran para $\phi$ d es la diferencia de dos
  funciones monotonas, siendo una funci\'on convexa un caso particular.}.

Se  prob\'o  en~~\cite{Cho74} o~\cite[Prop.~7.3]{WanMad04}  que  se conserva  la
Schur-concavidad~\ref{prop:SZ:Schurconcavidad}              para             las
$\phi$-entropias. Entonces,  de $h$  creciente (para $\phi$  concava desigualdad
reversa  para  la  integral, pero  $h$  es  decreciente),  se generaliza  a  las
$(h,\phi)$-entropias,\ie
%
\[
p \prec  q  \quad \Rightarrow  \quad \hhphi[p]  \ge \hhphi[q]
\]

\SZ{Quid subaditividad ssi fct creciente de Shannon?}

% ================================= Csiszar & Bregman

\subseccion{Divergencias y propiedades}
\label{sec:SZ:Czizar}

% ----- Jensen-Shannon
\paragraph{Primer  generalizaciones, saliendo  de Shannon  y  Kullback-Leibler -
  divergencia de Jensen-Shannon}

Como  se  lo ha  visto  tratando  de la  entropia  relativa,  la divergencia  de
Kullback-Leibler no define una distancia entre distribuciones de probabilidades,
siendo no simetrica  entre otros. Un primer paso para  recuperar la simetria sin
perder la positividad de  esta medida informacional fue simetrizarla, definiendo
lo   que   es  conocido   como   {\it  $J$-divergencia}~\cite{KulLei51,   Kul68,
  Lin91}~\footnote{Esta     expresi\'on     aparece     en~\cite[Ec.~(1)]{Jef46}
  o~\cite{Jef48},   antes   del  la   introducc\'ion   de   la  divergencia   de
  Kullback-Leibler  en el  campo de  le estimaci\'on  Bayesiana,  Jeffrey siendo
  citado por Kullback y Leibler.},
%
\[
D_J(q\|p) = \Dkl[p]{q} + \Dkl[p]{q}
\]
%
\noindent  Esta  versi\'on  simetrizada  de la  divergencia  queda  naturalmente
positiva,  pero sufre  todav\'ia  de  unas debilidades  de  $\Dkl{}$. Esta  bien
definida  siempre  que   el  soporte  de  $p$  es  incluido  en   lo  de  $q$  y
vice-versa. Ademas, no cumple tampoco  la desigualdad triangular. A pesar de sus
debilidades, se  us\'o bastante  en problemas de  discriminaci\'on, debido  a su
positividad con igualdad  si y solamente si $p = q$  (propiedad herida del hecho
de que la suma de terminos positivos es nula si y solo si cada uno vale cero).

Unas decadas  despues, Lin introdujo  lo que llam\'o  $K$-divergencia directada,
$K(p,q)   =  \Dkl[p]{\frac{p+q}{2}}$,   su  versi\'on   simetrizada,   antes  de
generalizarla    bajo     la    terminologia    de     {\it    divergencia    de
  Jensen}~\cite{Lin91}~\footnote{De  hecho, apareci\'o implicitamente  en varios
  trabajos anteriores, por ejemplo  en mecanica cu\'antica~\cite{Hol73, Hol11} o
  en reconocimiento de patrones~\cite{WonYou85}}.
%La introdujo  Lin  a
%  trav\'es de  $\Dkl$, sin los factores  $\frac12$, pero desde  este trabajo, se
%  usa casi siempre la formulaci\'on dada. La formulaci\'ion con la entropia (con
%  un factor 2) aparece como consecuencia y no una definici\'on.}
%
\begin{eqnarray*}
\Djs(p_1,p_2)  &  = &
%
\pi_1 \Dkl[p_1]{\pi_1 p_1 + \pi_2 p_2} + \pi_2 \Dkl[p_2]{\pi_1 p_1 + \pi_2
p_2}\\[2.5mm]
%
& = & H(\pi_1 p_1 + \pi_2 p_2) - \pi_1 H(p_1) - \pi_2 H(p_2) \qquad \pi = [\pi_1 \quad
\pi_2], \quad 0 \le \pi_1 = 1-\pi_2 \le 1
\end{eqnarray*}
%
\noindent $\Djs$ heride obviamente de  $\Dkl{}$ su positividad con igualdad si y
solamente si $p_1 =  p_2$.  La misma propiedad puede ser visto  a trav\'es de la
desigualdad  de Jensen,  dando este  nombre  a la  medida. Ademas,  se quita  el
problema de  definici\'on, siendo de que el  soporte de $\pi_1 p_1  + \pi_2 p_2$
siempre contiene el de $p_1$ y el  de $p_2$. No es simetrica en general, pero se
obtiene  esta propiedad  cuando $\pi  = \pi_{\mathrm{u}}  \equiv  [\frac12 \quad
\frac12]^t$. Ademas,  en este caso, a pesar  de que la divergencia  no cumpla la
desigualdad           triangular,          aparece           que          $\Big(
J_{\mathrm{js}}^{\pi_{\mathrm{u}}}(p_1,p_2) \Big)^s, \quad 0 < s \le \frac12$ es
una  metrica~\cite{OsaBus18}  o~\cite[para  $s =  \frac12$]{EndSch03,  OesVaj03,
  KafOes91}.   Si  puede  parecer  m\'as  logico definir  tal  divergencia  con  a
priori/proporciones  $\pi_i$ iguales, de  hecho la  versi\'on no  simetrica, con
pesos $\pi_i$ vuelve natural en el marco de la discriminaci\'on donde apareci\'o
implicitamente  esta  cantidad.   En  particular  cuando estamos  frente  a  dos
hypotesis  $i  =  1,  2$  o  clases,  a las  cuales  la  distribuci\'on  de  las
observaciones  es  $p_i$,  con  probabilidad  a priori  $\pi_i$.   A  partir  de
observaciones  $x$  hay   que  elegir  si  eran  sorteando   de  $p_1$  o  $p_2$
(distribuciones de  sampleos, \ie condicionalmente a la  hypotesis).  El enfoque
Bayesiano  m\'as   natural  consiste   maximizar  la  probabilidad   a  posteriori
(probabilidad de estar en hypotesis  $i$ condicionalmente a la observaci\'on), y
se prueba  que la probabilidad de  error es dada  por $P_e = \sum_x  \min( \pi_1
p_1(x)   \,  ,  \,   \pi_2  p_2(x)   )$  (o   con  una   integral  en   el  caso
continuo)~\cite{Kay93}. Prob\'o Lin de que
%
\[
\frac14 \left(  H_2(\pi) -  \Djs(p_1,p_2) \right)^2 \le  P_e \le  \frac12 \left(
  H_2(\pi) - \Djs(p_1,p_2) \right)
\]
%
con el logaritmo de base 2 en  la definici\'on de $\Djs$, lo que da naturalmente
un  rol operacional  a esta  divergencia.  Incidamente, de  esta desigualdad  es
inmediato ver de que $\Djs(p_1,p_2) \le H_2(\pi) - 2 P_e \le H_2(\pi)$:
%
\[
0 \le \Djs(p_1,p_2) \le \log(2)
\]
%
\noindent (cota igual a 1 usando el logaritmo de base 2).

Un otro  vinculo natural  entre la divergencia  de Jensen-Shannon y  las medidas
informacionales a la Shannon viene todav\'ia del campo de la clasificaci\'on. Si
unos datos  pueden provenir  de una distribuci\'on  $p_i$, $i  = 1, 2$,  con una
probabilidad  $\pi_i$, la variable  aleatoria $X$  dada por  los datos  tiene la
distribuci\'on   de   mezcla   $p   =   \sum_i   \pi_i   p_i$   como   ilustrado
figura~\ref{fig:SZ:Concavidad}-(b). Sea $Z$  la variable aleatoria binaria sobre
$\{ 1 ,  2 \}$ tal que $\Pr[Z  = i] = \pi_i$, variable de  selecci\'on entre las
distribuciones  $p_i$ (ej.  la  moneda de  la  figura). Por  definici\'on de  la
entropia  condicional,  $H(X|Z)  =  \sum_i  \pi_i  H(X|Z =  i)  =  \sum_i  \pi_i
H(p_i)$. De $\Djs(p_1,p_2) = H(p) - \sum_i H(p_i)$ viene $\Djs(p_1,p_2) = H(X) -
H(X|Z)$, es decir
%
\[
\Djs(p_1,p_2) = I(X;Z)
\]
%
La  divergencia   de  Jensen-Shannon  mide  la  informaci\'on   mutua  entre  la
observaci\'on $X$ y la variable de  selecci\'on $Z$, justificando aun m\'as su uso
natural     en    problemas    de     clasificaci\'on    o     selecci\'on    de
modelos. Incidentalmente, de $I(X;Z) = H(Z)  - H(Z|X) \le H(Z) \le \log(2)$ ($Z$
siendo discreta) se recupera la cota mayor de $\Djs$, y una cota aun m\'as fina
%
\[
\Djs(p_1,p_2) \le H(\pi)
\]
%
Se  encuentran otras  desigualdades  implicando $\Djs$  y  $D_J$ o  $\Djs$ y  la
distancia  $L^1$  entre  distribuciones   o  divergencia  de  variaci\'on  total
en~\cite{Lin91}.
%~\cite{SchEla03}.

M\'as alla, en  el campo de la  clasificaci\'on, se puedre tratar de  m\'as de dos
clases, dando lugar a la  generalizaci\'on de la divergencia de Jensen-Shannon a
$n$  distribucionese   probabilidad  y   $\pi$  un  $n$-componentes   vector  de
probabilidad,
%
\[
\Djs(p_1,\ldots,p_n) = H\left( \sum_i \pi_i p_i\right) - \sum_i \pi_i H(p_i)
\]
%
De la desigualdad de Jensen, esta cantitad queda positiva con igualdad si y solo
si todos los $p_i$ son iguales. Se conserva una cota superior
%
\[
\Djs(p_1,\ldots,p_n) \le H(\pi) \le \log(n)
\]
%
\noindent as\'i que $\Djs(p_1,p_2) = I(X;Z)$ con $X$ de distribuci\'on la mezcla
$\sum_i \pi_i p_i$ y $Z$ definida sobre $\{1,\ldots,n\}$ variable de selecci\'on
de distribuci\'on $\pi$.

\SZ{convexidad?}

% ----- f-Jensen
\paragraph{Clase de Burbea-Rao o   $f$-Jensen}
%$\phi$-Jensen divergencia y generalized $f$-Jensen}

\SZ{ (0)  Jensen generalizada  caso H  phi Ref. Nielsen ArXiv 2010, Nielsen Boltz 2011 -> cualquier funcion concava $\hgene$ de la distribucion de proba., en particular h phi

  Se  ilustra  a  que  corresponde  esta  cantidad  con  respeto  a  $f$  en  la
  figura~\ref{fig:SZ:BregmanFJensen} m\'as adelante.  }

% ----- (h,phi)-divergence
\paragraph{Clase de Csisz\'ar y versi\'on simetrizada}

\SZ{(1) Extension a la Renyi,

(2) a  la HC/D/T, Cressie Reads, Cressie Pardo, Vajda;


(3) generalization Czizar (et cf Burbea Rao aussi avec csiszar), Ali-Silvey, Ben-Tal

(4) generalization
Czizar Vajda, et voir avec h phi  avant meme Salicru, Ben-Tal

BenTal livre cap. 17 p. 255 

Csiszar 1995 dans appli

autres  de Csizsar  2012  versikon Bregman; 

gupta Sharma  1976

BoeLub79,
Vajda72,
Salicru94
Orsak et  Paris;
Voir OesVaj en termes de metrique pour les Arimoto, Vajda 2009, Kafka 91
Bas89, Bas13,
application a le test d'adequation
Pardo 99; MenMor97:5, Cf Pardo 2006 et ref. }

\

\paragraph{La clase de las divergencias  de Bregman} \ Estas divergencias fueron
intoducidos  en el  campo de  la  programaci\'on lineal  convexa, para  resolver
problemas de  minimizaci\'on convexa~\footnote{A\'un que aparece  en una revista
  de matematica  y f\'isica matematica, una  gracia del papel de  Bregman es que
  toma  el  ejemplo  de  maximizaci\'on  de  la entropia  de  Shannon  sujeto  a
  momentos\ldots}~\cite{Bre67}, pero  con aplicaciones en  varios campos~\cite[y
ref.]{Bas89, Bas13}:
%
\begin{definicion}[Divergencia de Bregman]
  Sea \ $f: \Omega  \subset \Rset^m \mapsto \Rset$ \ convexa y  de clase $C^1$ \
  sobre  \ $\Omega$,  \  un cerrado  convexo  de $\Rset^d$.   La divergencia  de
  Bregman de  un punto  \ $v \in  \Omega$ \  relativamente a un  punto \  $u \in
  \Omega$ \ es definida por
  %
  \[
  B_f(v\|u) = f(v) - f(u) - (v-u)^t \nabla f(u)
  \]
  %
  Dicho de otra  manera, $B_f$ corresponde al desarollo de Taylor  al orden 1 de
  $f$ en  la referencia  $u$.  Se  ilustra a que  corresponde esta  cantidad con
  respeto a $f$ en la figura~\ref{fig:SZ:BregmanFJensen} m\'as adelante.
\end{definicion}
%
Esta  definici\'on fue  generalizada  a funciones  actuando  sobre espacios  m\'as
generales (ej.  actuando  sobre matrices o operadores en  espacios de Hilbert de
dimensi\'on infinita)~\cite{Pet07,  NieNoc17}. En lo  que nos concierna  en este
capitulo,  tratando posiblemente  de densidad  de probabilidad,  nos  interesa a
funciones de funciones:
%
\begin{definicion}[Divergencia de Bregman funcionale]
  Sea \ $f: \Omega \mapsto \Rset$ \ convexa y de clase $C^1$ \ sobre \ $\Omega$,
  \ un cerrado convexo de un espacio de Banach.  La divergencia de Bregman de un
  ``punto'' \ $v \in  \Omega$ \ relativamente a un ``punto'' \  $u \in \Omega$ \
  es definida por
  %
  \[
  B_f(v\|u) = f(v) - f(u) - \lim_{t \to 0}\frac{f( u + t (v-u) ) - f(u)}{t}
  \]
  %
\end{definicion}
%
En  el  caso  de que  $\Omega  \subset  \Rset^d$  se recupera  sencillamente  la
definici\'on original.
% $f$ siendo convexa, inmediatemente
%%
%\[
%B_f(v\|u) \ge 0 \qquad \mbox{con igualdad sii} \quad v = u
%\]

Para $(h,\phi)$-entropias discretas \underline{concavas} (ej.  con $h$ concava),
se puede entonces asociar una divergencia de Bregman
%
\begin{eqnarray*}
\bhphi[q]{p} \equiv B_{-\hhphi}(q\|p)
& = & \hhphi[p] - \hhphi[q] - (p-q)^t \nabla \hhphi[p]\\[2.5mm]
%
%& = & \hhphi[p] - \hhphi[q] - \sum_{i=1}^n (p(x_i)-q(x_i))
% \frac{\partial}{\partial t_i} \hhphi[p]\\[2.5mm]
%
%& = & \hhphi[p] - \hhphi[q] - h'\left( \sum_j \phi(p_j) \right) \sum_i
%(p_i-q_i) \phi'(p_i)
%& = & \hhphi[p] - \hhphi[q] - h'\big(\hphi[p] \big) \sum_{i=1} (q(x)-p(x)) \phi'(p(x))
& = & \hhphi[p] - \hhphi[q] - h'\big(\hphi[p] \big) (q-p)^t \phi'(p)
\end{eqnarray*}
%
%donde    $\phi'(u)    \equiv    \begin{bmatrix}    \phi'(u_1)   &    \cdots    &
%  \phi'(u_n)  \end{bmatrix}^t$  vector  de   las  derivada  de  $\phi$  aplicada
%componente a componente.
% $t_i  \equiv p(x_i)$. 
Cuando $h  \equiv \id$,  se notar\'a $\bphi{}$  y es  equivalente a salir  de la
definici\'on inicial con  \ $\Omega = [0 \, ;  \, 1]$, $u$ \ y \  $v = q(y_i)$ \
$i$-esima  componente  de  \ $p$  \  y  \  $q$  \  respectivamente, y  sumar  la
divergencia obtenida sobre $i$.

En el caso continuo, para las $(h,\phi)$-entropias, se obtiene
%
\[
\bhphi[q]{p} =  \hhphi[p] - \hhphi[q] -  h'\big(\hphi[p] \big) \int_\X  ( q(x) -
p(x) ) \phi'(p(x)) \, dx
\]
%
De nuevo, cuando $h \equiv \id$,  se notar\'a $\bphi{}$ y es equivalente a salir
de la  definici\'on inicial  $u =  p(x)$, \ $v  = q(x)$  y sumar  la divergencia
obtenida sobre \ $\X$.

Notablemente, cuando $\phi(x) = x \log x$ se recupera de nuevo la divergencia de
Kullback-Leibler: esta \'ultima partenece  simultaneamente a la clase de Csiszar
y a la de  Bregman y es la sola en este  caso~\cite{Csi91}.  \SZ{Sin embargo, se
  puede ver  que si  las $f$-Jensen se  escriben como combinaciones  convexas de
  divergencias de Bregman
\[
J_f^\pi(p_1,p_2) = \pi_1  B_f(p_1 \| \pi_1 p_1 + \pi_2 p_2)  + \pi_2 \B_f(p_1 \|
\pi_1 p_1 + \pi_2 p_2)
\]
%
\cite{Zha04, NieBol11, NieNoc17}}

La figura~\ref{fig:SZ:BregmanFJensen} ilustra  a que corresponden \ $D_f$  \ y \
$J_f$ con respeto a la funci\'on convexa $f$.
%
\begin{figure}[h!]
%
\begin{center} \input{TIKZ_SZ/BregmanFJensen} \end{center}
%
\leyenda{$f$  estrictamente  convexa.   Las  cantidad positiva  marcada  por  la
  dupla-flecha   representan  respectivamente   la  divergencia   de  $f$-Jensen
  $J_f^\pi(u_1,u_2)$, diferencia entre la combinaci\"'on convexa de los $f(u_i)$
  y $f$ de la combinaci\'on convexa de  los $u_i$, \ y la divergencia de Bregman
  \ $B_f(u_2\|u_1)$ diferencia entre el valor en $u_2$ (punto de evaluaci\'on) y
  la  tangente  en  $u_1$  (punto  referencia). Para  $J_f^\pi$,  se  toma  como
  referencia $\pi_1 u_1 + \pi_2 u_2$, se calcula $D_f$ en los $u_i$ y se toma la
  combinaci\'on convexa.}
  %
\label{fig:SZ:BregmanFJensen}
\end{figure}

La divergencia de Bregman tiene las propiedades siguientes
%
\begin{enumerate}
\item Positividad:
  %
  \[
  B_f(q\|p) \ge 0 \quad \mbox{con igualdad si y solamente si} \quad p = q
  \]
  %
  Esta propiedad  es la  consecuencia directa de  la convexidad estricta de  $f$, como
  ilustrado \SZ{figura~\ref{fig:SZ:ConvexidadDerivada}.}
%
\item  $B_f(q\|p)$ es  convexa con  respeto a  $q$, pero  no  necesariamente con
  respeto a $p$. Es tambi\'en consecuencia directa de la convexidad de $f$.
%
\item  Pensando a  $B_f$ con  respeto a  $f$,  es lineal  en el  sentido de  que
  $B_{\lambda f_1 + \lambda_2 f_2} = \lambda_1 B_{f_1} + \lambda_2 B_{f_2}$ (con
  $f_i$ convexas y $\lambda_i \ge 0$).
%
  % Dualidad:  si $\phi$  tiene un  convex conjugado  $\phi^*$  (transformada de
  % Legendre)  D^{\mathrm{b}}_{\phi^*}\left(  \left.  -\nabla \hphi[q]  \right\|
  %   -\nabla \hphi[p] \right) = \bphi[q]{p}$.
\end{enumerate}


\SZ{Contiene squared  Mahalanobis distance $u^t  Q u$  con distancia
  euclideana  $Q  =  I$.  Itakura-Saito  cuando  $\phi(u)  =  -  \log  u$  (Burg
  entropy)~\cite{toto}.}


\


\SZ{FriSri08 pour Bregman; reapparition Fisher comme courbure, cf Varma, Jizba, MenMor97...}


% ================================= Identidades

\subseccion{?`Como se generalizan las identidades y desigualdades?}

\paragraph{Principio de entropia m\'axima} Si este principio naci\'o en el marco
de la termodynamica o f\'isica,  con la entropia de Shannon (Boltzman), tratando
de las nociones  generalizadas de incertas, vuelve natural  preguntarse sobre la
extensi\'on de este  problema en el marco general. \SZ{Tal  estudio fue hecho en
  varios trabajos~\cite{ref} nous, Kesavan, Kagan 63}.

El problema se formaliza como en  el caso Shannon, buscando la entropia m\'axima
sujeto  a  vinculos: sea  $X$  variable  aleatoria  viviendo sobre  $\X  \subset
\Rset^d$ con $K$ momentos \ $\Esp\left[ M_k(X) \right] = m_k$ \ fijos, con $M_x:
\X  \to \Rset$,  el problema  de $(h,phi)$-entropia  m\'axima se  formula  de la
manera siguiente en  el caso continuo (es el caso  discreto, hay que re-emplazar
integrales por  sumas): sean  \ $M(x) =  \begin{bmatrix} 1  & M_1(x) &  \cdots &
  M_K(x)  \end{bmatrix}^t$ \  y  \  $m =  \begin{bmatrix}  1 &  m_1  & \cdots  &
  m_K \end{bmatrix}^t$, \ se busca,
%
\[
p^* = \argmax_p \hhphi[p] \qquad \mbox{sujeto a} \qquad p \ge 0, \quad \int_\X M(x)
\, p(x) \, dx = m
\]
%
donde   los  dos  primeros   vinculos  aseguran   de  que   $p^*$  (positividad,
normalizaci\'on) sea  una distribuci\'on de  probabilidad. Si $\phi$  es convexa
(resp.   concava), $h$  es creciente  (resp.  decreciente)  as\'i  que maximizar
$\hhphi$ es  equivalente a maximizar $\hphi$ (resp.   $H_{-\phi}$).  Sin perdida
de generalidad,  se puede considerar la  situaci\'on $\phi$ convexa.  Como en el
caso de  Shannon, introduciendo factores de Lagrange  $\lambda = \begin{bmatrix}
  \lambda_0  & \lambda_1  & \cdots  & \lambda_K  \end{bmatrix}^t$ para  tener en
cuenta los vinculos, el problema variacional consiste a resolver~\cite{GelFom63,
  Bru04, Mil00, CamMar09, CovTho06}
%
\[
p^* =  \argmax_p \int_\X \left(  - \phi\left( p(x)  \right) + \lambda^t  M(x) \,
  p(x) \right) dx
\]
%
donde $\lambda$ ser\'a  determinado para satisfacer los vinculos.   De nuevo, de
la ecuaci\'on de Euler-Lagrange~\cite{GelFom63,  Bru04} se obtiene la ecuaci\'on
$- \phi'(p(x)) + \lambda^t M(x) = 0$. La funci\'on entropica $\phi$ es concava y
de clase $C^2$, as\'i que $\phi'$ es continua decreciente, y de la monotonicidad
es invertible. Entonces,
%
\[
p^*(x) = \phi'^{-1}\left( \lambda^t M(x) \right)
\]
%
con $\lambda$ tal que se  satisfacen los vinculos de normalizaci\'on y momentos.
Si el resultado no es positivo en  $\X$, de las condiciones KKT, $p^*(x) = \Big(
\phi'^{-1}\left(  \lambda^t M(x)  \right) \Big)_+$.   Estas  distribuci\'ones no
caen  en  general en  la  familia exponencial.  De  una  forma, usando  entropia
generales permite escaparse de esta familia.

Como  en el  caso de  Shannon, queda  obviamente  el hecho  de que  no se  puede
determinar $\lambda$ tal  que se satisfacen todos los  vinculos (y en particular
la de normalizaci\'on).

Tal como en el caso Shannon, existe una prueba informacional:
%
\begin{lema}
  Sea \ $\displaystyle \P_m = \left\{ p \ge 0: \: \int_\X M_k(x) \, p^*(x) \, dx
    =  m \right\}$  \ y  \ $p^*  \in \P_m$  \ que  satisfaga \  $\phi'(p^*(x)) =
  \lambda^t M(x)$. Entonces
  %
  \[
  \forall  \, p  \in  \P_m,  \quad \hhphi[p]  \le  \hhphi[p^*] \qquad  \mbox{con
    igualdad ssi} \quad p = p^*
  \]
  %
\end{lema}
\begin{proof}
  Sin  perdida  de  generalidad,   consideramos  $\phi$  convexa.  Calcuando  la
  divergencia de Bregman asociado a $\phi$ de $p$ relativamente a $p^*$ da
  %
  \begin{eqnarray*}
  \bphi[p]{p^*} & = & \hphi[p^*] - \hphi[p] - \int_\X \big( p(x) - p^*(x) \big)
  \, \phi'\left( p^*(x) \right) \, dx\\[2.5mm]
  %
  & = & \hphi[p^*] - \hphi[p] - \lambda^t \int_\X \big( p(x) - p^*(x) \big)
  \, M(x) \, dx\\[2.5mm]
  %
  & = & \hphi[p^*] - \hphi[p]
  \end{eqnarray*}
  %
  siendo \  $p$ \ y \  $p^*$ \ en \  $\P_m$. El resulta proviene  entonces de la
  positividad de la divergencia de Bregman,  con igualdad si y solamente si $p =
  p^*$ conjuntamente a la crecencia de $h$.
\end{proof}
%
Este lema  prueba que, dando vinculos ``razonables'',  la $(h,\phi)$-entropia es
acotada por arriba, y que se alcanza la cota. Por ejemplo,
%
\begin{itemize}
\item Con  $K =  0$ \  y \  $\X$ \ de  volumen finito  \ $|\X|  < +  \infty$, la
  distribuci\'on de  $(h,\phi)$-entropia m\'axima es  la distribuci\'on uniforme
  en el caso discreto tal como en el caso continuo.
%
\item \SZ{Con  $K =  1$, \ $\X  = \Rset^d$  \ y \  $M(x) = x  x^t$ (visto  con $d^2$
  vinculos), y $\phi(u) = u^\beta$ (R\'enyi o Havrda-Charv\'at-Dar\'oczy), la  distribuci\'on  de  entropia  m\'axima  es \SZ{Student; Costa and son on. Gausiana se recupera caso limite}.}
\end{itemize}

\SZ{
   On the theory  of Fisher's
  amount  of  information Sov.  Math.  Dokl., 4  (1963),  pp.  991-993, etc,  la
  codificaci\'on a la Renyi (Cambell,  Hooda 2001, Bercher) 

\

 y la cuantificacion
  fina; EPI generalizada por Madiman, etc. Lutwak, Bercher etc., Kagan; Boeke 77
  An extension  of the Fisher information  measure I. Csisz\'ar,  P. Elias (Eds.),
  Topics   in  Information  Theory,   North-Holland,  Berlin/New   York  (1977),
  pp. 113-123  o Hammad o  Vajda 73 o  Ferentinos81 en el marco  Fisher; Kesavan
  gene MaxEnt}

\SZ{Revisite capacite a la Daroczy? codage; parler de la quantification fine et HCD}
