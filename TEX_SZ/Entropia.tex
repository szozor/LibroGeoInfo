\seccion{Entrop\'ia como medida de ignorancia}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%ncerteza}
\label{Sec:SZ:Entropia}

% ================================= Axiomas

\subseccion{Entrop\'ia de Shannon, propiedades}
\label{Ssec:SZ:DefinicionShannon}

Uno de los primeros trabajos donde  se formaliza la noci\'on de informaci\'on de
una  cadena  de  s\'imbolos  es  debido a  Ralph  Hartley~\cite{Har28}.   En  su
art\'iculo,  Hartley defini\'o  la informaci\'on  de una  secuencia como  siendo
proporcional a su longitud.  M\'as  precisamente, para s\'imbolos de un alfabeto
$\X$  de cardenal  finito  \ $|\X|  <  +\infty$, existen  \  $|\X|^n$ \  cadenas
distintas de  longitud \ $n$.   Se defini\'o  la informaci\'on de  tales cadenas
como \  $K n$  (con $K$ dependiente  de \ $|\X|$).   Para ser  consistentes, dos
conjuntos  $\X_1^{n_1}$ y  $\X_2^{n_2}$  del mismo  tama\~no  \ $|\X_1|^{n_1}  =
|\X_2|^{n_2}$  \ deben  contener  la misma  informaci\'on.   De este  requisito,
Hartley  defin\'o la  informaci\'on como  $H =  \log |\X|^n$  donde la  base del
logaritmo  es  arbitraria.  Dicho  de  otra  manera,  tomando en  particular  el
logaritmo de base 2, esta informaci\'on es  nada m\'as que los n\'umeros de bits
(0-1) necesarios para codificar todas las  cadenas de longitud $n$ de s\'imbolos
de  un alfabeto  $\X$ de  cardenal $|\X|$.   La informaci\'on  de Hartley  es el
equivalente de  la entrop\'ia  de Boltzmann de  la mec\'anica  estad\'istica, la
famosa f\'ormula~\footnote{Esta  f\'ormula se encuentra  grabada el la  tumba de
Ludwig  Boltzmann (1844-1906)  en  Viena  donde fue  enterrado  despu\'es de  su
suicidio en Duino (cerca de Trieste y del ICTP--Centro internacional de F\'isica
T\'eorica)  el  5 de  septiembre  de  1906, sin  razones  aparentes~\cite{Cer08,
ArrPer13,  ArrPer15}.}   $S  =  k_B  \log  W$,  donde  $k_B  =  1.380689  \times
10^{-23}  \operatorname{J.K}^{-1}$  es  la constante  de  Boltzmann~\cite{Bol96,
Bol98, Jay65, Mer10, Mer18}.

Una debilidad del enfoque de Hartley es que considera impl\'icitamente que en un
mensaje cada cadena  de longitud dada puede aparecer con  la misma frecuencia, o
probabilidad, $1/|\X|^n$ (en Boltzmann, corresponde a la misma probabilidad para
cada  configuraci\'on), siendo  la  informaci\'on menos  el  logaritmo de  estas
probabilidades.  Sin  embargo, parece  m\'as l\'ogico considerar  que secuencias
muy frecuentes  no llevan mucha  informaci\'on (se sabe que  aparecen), mientras
que las que  aparecen raramente llevan m\'as informaci\'on  (hay m\'as sorpresa,
m\'as incerteza  en observarlas).  Volviendo  a los s\'imbolos  elementales $x$,
vistos  como aleatorios  (o  valores, o  estados que  puede  tomar una  variable
aleatoria), la  (falta de)  informaci\'on o incerteza  va a  estar \'intimamente
vinculada a la probabilidad de aparici\'on de estos s\'imbolos $x$. Siguiendo la
idea de Hartley, la informaci\'on elemental asociada al estado $x$ va a ser $-
\log p(x)$, donde $p(x)$ es la  probabilidad de aparici\'on de $x$.  Se define la
incerteza asociada a la variable  aleatoria como el promedio estad\'istico sobre
todos   los   estados   posibles  $x$~\cite{Sha48,   ShaWea64}~\footnote{En   la
misma  \'epoca   que  Shannon,  independientemente,   medidades  informacionales
aparecieron en c\'alculos  de capacidad de canal en varios  trabajos como los de
los    ingenieros    franceses    Andr\'e   Clavier~\cite{Cla48}    o    Jacques
Laplume~\cite{Lap48},    o   en    el   libro    del   estadounidense    Norbert
Wiener~\cite[Cap.~III]{Wie48}  entre  varios   otros  (ver~\cite[y  Ref.]{Ver98,
Lun02, RioMag14, FlaRio16, RioFla17, Che17}).}.
%
\begin{definicion}[Entrop\'ia de Shannon]
\label{Def:SZ:Shannon}
%
  Sea  $X$  una  variable  aleatoria  definida  sobre  un  alfabeto  discreto  \
  $X(\Omega) = \X  = \{ x_1 \,  , \, \ldots \,  , \, x_{|\X|} \}$  \ de cardenal
  $|\X|  < +  \infty$ finito.  Sea $p_X$  la distribuci\'on  de probabilidad  de
  $X$, \ie $  \forall \, x \in \X,  \quad p_X(x) = P(X = x)$.   La entrop\'ia de
  Shannon de la  variable~\footnote{Recordar que $X \sim  p_X$ significa ``$X$''
  distribuida de acuerdo a la distribuci\'on de probabilidad $p_X$} $X \sim p_X$
  se define como
  %
  \[
    H(X) = H(p_X) = - \sum_{x \in \X} p_X(x) \, \log p_X(x),
  \]
  %
  con   la   convenci\'on   \   $0   \log    0   \igualc   0$   \   (a   partir   de
  $\displaystyle \lim_{t \to 0^+} \, t \log t = 0$).
\end{definicion}
%
\noindent La  base del logaritmo es  arbitraria: si es $\log_2$  el logaritmo de
base 2, $H$ est\'a  dada en unidades binarias o bits  (se encuentra tambi\'en la
denominaci\'on Shannons);  si se usa el  logaritmo natural $\ln$, $H$  est\'a en
unidades naturales o nats; si es el de base 10, $H$ se da en d\'igitos decimales
o dits  (se encuentra  tambi\'en la  denominaci\'on bans  o Hartleys).   En este
libro, se  usar\'a $H$ sin especificar  la base del logaritmo.   Si es necesario
que tenga una base $a$ dada, se denotar\'a la entrop\'ia correspondiente $H_a$ y
se  especificar\'a  la  base  del  logaritmo  $\log_a$.   Notar  que  $\log_a  x
= \frac{\log x}{\log a}$, dando
%
\[
H_a(X)  =  H_b(X)  \log_a b.
\]
%
En  lo que  sigue,  a\'un cuando,  rigurosamente,  $H$ es  una  funcional de  la
distribuci\'on  de  probabilidad  $p_X$  y  no  de  la  variable  $X$,  usaremos
indistintamente  tanto  la notaci\'on  $H(p_X)$  como  $H(X)$ seg\'un  convenga.
Adem\'as,   $p_X$  podr\'a   denotar   indistintamente   la  distribuci\'on   de
probabilidad, o el  vector de probabilidad $p_X  \equiv \begin{bmatrix} p_X(x_1)
&  \cdots  & p_X(x_{|\X|})  \end{bmatrix}^t$.   Tambi\'en,  usaremos $p_i  \equiv
p_X(x_i)$ por simplificaci\'on de escritura en algunas expresiones.

\modif{$H$  es  el   equivalente  de  la  entrop\'ia  de   Gibbs  en  mec\'anica
  estad\'istica (denotada  \ $S$  \ en este  marco), como lo  hemos visto  en la
  secci\'on~\ref{Ssec:MP:FamiliaExponencial}.  Precisamente,  se reconoce  en la
  f\'ormula de le entrop\'ia de Shannon la forma ya vista tratando de la familia
  exponencial, secci\'on~\ref{Ssec:MP:FamiliaExponencial}, como fluctuaciones de
  la energ\'ia libre. En la secci\'on mencionada,  la vimos en el contexto de la
  ley  de Boltzmann,  dada  una funci\'on  energ\'ia.  En  el  contexto de  este
  cap\'itulo,  quitamos  el  contexto  de   la  mec\'anica  estad\'istica  y  la
  definici\'on es dada para cualquier densidad de probabilidad. Sin embargo, los
  v\'inculos entre  ambas son  impotante, como  lo vamos a  ver tambi\'en  en el
  problema  de  entrop\'ia  m\'axima  (ver   tambi\'en  el  ep\'igrafo  de  este
  cap\'itulo).}    La  letra   $H$   viene  del   teorema-H   debido  a   Ludwig
  Boltzmann~\cite{Jay65,  Mer10, Mer18}\modif{,  a  pesar de  que en  mec\'anica
  estad\'istica se  encuentra frecuentemente la letra\ldots  $S$ (sin relaci\'on
  con Shannon) y tambi\'en en mec\'anica c\'uantica tratando de la entrop\'ia de
  von Neuman (ver m\'as adelante).}

\modif{La entrop\'ia de Shannon} $H$ tiene propiedades notables que corresponden
a las que se puede exigir  a una medida de ignorancia (o incerteza)~\cite{Sha48,
  ShaWea64, CovTho06, Rio07, DemCov91, Joh04}.
%
\begin{propiedades}
\item\label{Prop:SZ:continuidad} {\it Continuidad:}  vista como una funci\'on de
  \ $|\X|$ \ variables  \ $p_i = p_X(x_i)$, \ $H$ \  es continua con respecto a
  los $p_i$.
%
\setcounter{PropPermutacion}{\value{enumi}}
\item\label{Prop:SZ:permutacion}   {\it  Invarianza bajo   permutaci\'on:}
  obviamente,  la  entrop\'ia  es  invariante  bajo  una  permutaci\'on  de  las
  probabilidades, \ie
  %
  \[
  \mbox{para   cualquier   permutaci\'on   }   \sigma:   \X   \to   \X,   \quad
  H(p_{\sigma(X)})   =  H(p_X)   \quad  \mbox{con}   \quad   p_{\sigma(X)}(x)  =
  p_X(\sigma(x)),
  \]
  %
  lo que se escribe tambi\'en $H(\sigma(X)) = H(X)$.  En particular, denotando
  $p_X^\downarrow$  el  vector de  probabilidades  obtenido  a partir  de  $p_X$
  clasificando  las probabilidades  en  orden  decreciente, $p_1^\downarrow  \ge
  p_2^\downarrow \ge  \cdots \ge p_{|\X|}^\downarrow$ donde  $p_i^\downarrow$ es
  la     $i$-\'esima     componente      de     $p_X^\downarrow$     \modif{(ver
  secci\'on~\ref{Ssec:MP:VADiscreta},  definici\'on~\ref{Def:MP:Mayorizacion})},
  se tiene
  %
  \[
  H(p_X^\downarrow) = H(p_X).
  \]
%
\setcounter{PropBiyeccion}{\value{enumi}}
\item\label{Prop:SZ:biyeccion}   {\it  Invarianza   bajo   transformaci\'on
    biyectiva:}   la  entrop\'ia   es  invariante   bajo  una   transformaci\'on
  biyectiva, \ie
  %
  \[
  \mbox{para cualquier  funci\'on biyectiva } g:  \X \to g(\X),  \quad H(g(X)) =
  H(X).
  \]
  %
  Mediante  la  transformaci\'on  los  estados  cambian,  pero  no  cambia  la
  distribuci\'on de probabilidad vinculada al alfabeto transformado.  Tomando el
  ejemplo de  un dado, la  incerteza vinculada al dado  no debe depender  de los
  s\'imbolos  escritos  sobre las  caras,  sean  n\'umeros enteros  o  cualquier
  conjunto de letras.
%
\item\label{Prop:SZ:positividad} {\it Positividad:} la entrop\'ia est\'a acotada por abajo,
  %
  \[
  H(X) \ge 0,
  \]
  %
  con igualdad si y solamente si \ $X$ \ est determinista, \ie
  %
  \[
  H(X)  =  0 \quad  \mbox{ssi}  \quad \exists \, x_j \in \X, \:\: p_X(x) = \uno_j
  \]
  %
  En  otras palabras,  cuando $X$  no  es aleatoria,  \ie  $X =  x_j$, no  hay
  ignorancia, o  la observaci\'on no  lleva informaci\'on (se  sabe lo que  va a
  salir, sin duda): $H = 0$.  La  positividad es consecuencia de $p_X(x) \le 1$,
  dando $- p_X(x) \log p_X(x) \ge 0$.  Adem\'as, la suma de t\'erminos positivos
  vale cero  si y  solamente si  cada t\'ermino de  la suma  vale cero,  dando \
  $p_X(x) =  0$ \ o  \ $p_X(x) =  1$. Se concluye  de que solo  una probabilidad
  puede ser 1, las otras 0 $p_X$ siendo una distribuci\'on de probabilidad (suma
  a 1).
%
\setcounter{PropCotamaxima}{\value{enumi}}
\item\label{Prop:SZ:cotamaxima} {\it Maximalidad:}  la entrop\'ia est\'a acotada por
  arriba,
  %
  \[
  H(X) \le \log |\X|,
  \]
  %
  con igualdad si y solamente si $X$ es uniforme sobre $\X$, \ie
  %
  \[
  H(X) =  \log |\X| \quad \mbox{sii}  \quad \forall \,  x \in \X, \:  p_X(x) =
  \frac{1}{|\X|}.
  \]
  %
  En otras  palabras, la incerteza es  m\'axima cuando cualquier estado  \ $x$ \
  puede  aparecer con  la misma  probabilidad; el  sistema siendo  completamente
  ``desorganisado'',  de una  forma  una observaci\'on  (un  sampleo) lleva  una
  informaci\'on importante sobre  el sistema que genera $X$ al  opuesto del caso
  deterministico (se sabe que val $X$, as\'i que no nos informa un sampleo).  La
  cota m\'axima resuelta  de la maximizaci\'on de $H$ sujeto  a $\sum_x p_X(x) =
  1$,  es decir,  con  la t\'ecnica  del  Lagrangiano para  tomar  en cuenta  el
  v\'inculo~\cite{Mil00, CamMar09}, notando $p_i  = p_X(x_i)$, hay que maximizar
  $\sum_i (- p_i \log p_i + \eta \, p_i)$ donde el factor de Lagrange \ $\eta$ \
  se determinar\'a para  satisfacer el v\'inculo. Cancelado la  derivada en cada
  $p_j$, se obtiene sencillamente que $\log p_j$ debe ser constante (la derivada
  secunda   es   negativa  sobre   $[0   \;   1]$),  dando   la   distribuci\'on
  uniforme.\newline  La figura  Fig.~\ref{Fig:SZ:EntropiaBinaria} representa  la
  entrop\'ia  de  un   sistema  con  dos  estados,  de   probabilidades  \  $p_X
  = \begin{bmatrix} 1-p & p \end{bmatrix}^t$  \ (ley de Bernoulli de par\'ametro
  $p$), entrop\'ia  a veces dicha  {\it binaria}, en funci\'on  de $p \in  [0 \;
  1]$.  Esta figura  ilustra las cotas inferior  y superior de $H$ ($p  = 0$ \'o
  $1$, $p =  \frac12$) as\'i como la invarianza bajo  una permutaci\'on ($h(p) =
  H\left(    \begin{bmatrix}    1-p    &    p    \end{bmatrix}^t    \right)    =
  H\left( \begin{bmatrix} p & 1-p \end{bmatrix}^t \right)  = h(1-p) = - p \log p
  - (1-p) \log (1-p)$).
  %
  \begin{figure}[h!]
  %
  \begin{center}\input{TIKZ_SZ/EntropiaBinaria}\end{center}
  %
  \leyenda{Entrop\'ia  binaria   (de  una  variable  de   Bernoulli)  $h(p)  =
  H\left(  \begin{bmatrix} 1-p  &  p \end{bmatrix}^t  \right)$  en funci\'on  de
  $p \in [0 \; 1]$.}
  %
  \label{Fig:SZ:EntropiaBinaria}
  \end{figure}
%
\item\label{Prop:SZ:expansabilidad} {\it Expansibilidad:}  A\~nadir un estado de
  probabilidad nula no cambia  la entrop\'ia, \ie sean \ $X$  \ definido sobre \
  $\X$    \     y    \    $\widetilde{X}$    \     sobre    \    $\widetilde{\X}
  = \X \cup \{ \widetilde{x}_0 \}, \:\: \widetilde{x}_0 \notin \X$, con
  %
  \[
  p_{\widetilde{X}}(x)      =       p_X(x)      \quad       \mbox{si}      \quad
  x   \in  \X,   \quad  \mbox{y}   \quad  p_{\widetilde{X}}(\widetilde{x}_0)   =
  0, \quad \mbox{entonces} \quad H(p_{\widetilde{X}}) = H(p_X).
  \]
  %
  Esta propiedad  es obvia,  consecuencia de  la convenci\'on $0  \log \igualc  0$ que
  viene de $\displaystyle \lim_{t \to 0^+} \, t \log t = 0$.
%
\setcounter{PropRecursividad}{\value{enumi}}
\item\label{Prop:SZ:recursividad} {\it Recursividad:} Agrupar dos estados baja la
  entrop\'ia en  una cantidad igual a  la entrop\'ia interna de  los dos estados
  por la probabilidad de ocurrencia de este  par de estados. De la invarianza de
  la  entrop\'ia  por  permutaci\'on,  sin p\'erdida  de  generalidad  se  puede
  considerar que los  estados que se agrupan  son los dos \'ultimos,  \ie sean \
  $X$ \  definido sobre  \ $\X$  \ con  $|\X| > 2$,  y \  $\breve{X}$ \  sobre \
  $\breve{\X}$ \ tales que
  %\{ x_1 , \ldots , x_{|\X|-2} , \breve{x}_{|\X|-1}\},   %1 \le i \le |\X|-2
  %
  \[
  \left\{  \begin{array}{l}\breve{\X}  = \big( \X \setminus \{ x_{|\X|-1} , x_{|\X|} \} \big) \cup \{ \breve{x}_{|\X|-1} \}
  \quad \mbox{con el estado interno} \quad
      \breve{x}_{|\X|-1}   =  \{   x_{|\X|-1} \,  , \,   x_{|\X|}  \},\\[2.5mm]
      p_{\breve{X}}(x_i) = p_X(x_i), \quad \forall \, x_i \in \X \setminus \{ x_{|\X|-1} , x_{|\X|} \}
      \quad \mbox{y}
      \quad p_{\breve{X}}(\breve{x}_{|\X|-1}) = p_X(x_{|\X|-1}) +
      p_X(x_{|\X|})  \quad  \mbox{distribuci\'on  sobre  }  \breve{\X}\\[2.5mm]
      \displaystyle   \breve{q}(x_j)  =   \frac{p_X(x_j)}{p_X(x_{|\X|-1})  +
        p_X(x_{|\X|})},  \quad j =  |\X|-1, |\X|  \quad \mbox{distribuci\'on
        del estado interno}\end{array}\right.
  \]
  %
  entonces,
  %
  \[
  H(p_X)  =   H(p_{\breve{X}})  +  p_{\breve{X}}(\breve{x}_{|\X|-1})  \,
  H(\breve{q}),
  \]
  %
  %lo que se escribe tambi\'en
  %
  %\[
  %H(p_1,\ldots,p_{|\X|})  =  H(p_1,\ldots,p_{|\X|-2},p_{|\X|-1}+p_{|\X|})  +
  %\left(             p_{|\X|-1}+p_{|\X|}            \right)            H\left(
  %  \frac{p_{|\X|-1}}{p_{|\X|-1}+p_{|\X|}}                                  ,
  %  \frac{p_{|\X|}}{p_{|\X|-1}+p_{|\X|}}\right).
  %H(p_1,\ldots,p_{|\X|})  =  H(p_1,\ldots,p_{|\X|-2},p_{|\X|-1}+p_{|\X|})  +
  %\left(             p_{|\X|-1}+p_{|\X|}            \right)            H\left(
  %  \frac{p_{|\X|-1}}{p_{|\X|-1}+p_{|\X|}}                                  ,
  %  \frac{p_{|\X|}}{p_{|\X|-1}+p_{|\X|}}\right).
  %\]
  %
  Esta relaci\'on que viene de $a \log  a + b  \log b = (a+b)  \left( \frac{a}{a+b}
    \log\left(  \frac{a}{a+b} \right)  + \frac{b}{a+b}  \log\left( \frac{b}{a+b}
    \right)   +  \log(  a   +   b  )\right)$,   es   ilustrada   en  la   figura
  Fig.~\ref{Fig:SZ:Recursividad} (para $|\X| > 2$).\newline
  %
  \begin{figure}[h!]
  %
  \begin{center} \input{TIKZ_SZ/Recursividad} \end{center}
  %
  \leyenda{Ilustraci\'on de  la propiedad  de recursividad, que  cuantifica c\'omo
    decrece  la  entrop\'ia  en  un  conjunto  cuando  se  agrupan  dos  estados,
    relacionando   la  entrop\'ia   total,  la   entrop\'ia  despu\'es   de  la
    agrupaci\'on y la entrop\'ia interna de los dos estados agrupados.}
  %
  \label{Fig:SZ:Recursividad}
  \end{figure}
%
\setcounter{PropConcavidad}{\value{enumi}}
\item\label{Prop:SZ:concavidad}  {\it Concavidad:}  la  entrop\'ia es  c\'oncava
  ($-H$ es convexa, ver definici\'on~\ref{Def:MP:convexa}),
  %  pagina~\pageref{Def:MP:convexa}),  
  en el  sentido de que la  entrop\'ia de una combinaci\'on  convexa (mezcla) de
  distribuciones de probabilidad  es siempre mayor o igual  que la combinaci\'on
  convexa de entrop\'ias:
  %
  \[
  \forall      \:     \pi      =     \begin{bmatrix}      \pi_1     &      cdots
  & \pi_n \end{bmatrix}^t \in \Simp{n-1}
  %, \quad  0 \le \pi_k \le  1, \quad \sum_{k=1}^n \pi_k  = 1
  \quad \mbox{and  cualquier  conjunto de  distribuciones} \quad  \{
  p_{(k)} \}_{k=1}^n \in \Simp{\alpha-1}^n, \: \alpha > 1,
  \]
  %
  \[
  H\left( \sum_{k=1}^n \pi_k \, p_{(k)} \right) \ge \sum_{k=1}^n \pi_k H(p_{(k)}).
  \]
  %
  Esta relaci\'on es conocida tambi\'en como desigualdad de Jensen~\cite{Jen06}.
  Es una consecuencia directa de la  convexidad de la funci\'on $\phi: t \mapsto
  t \log  t$, como se ilustra en la  figura Fig.~\ref{Fig:SZ:Concavidad}-(a).  La
  figura  Fig.~\ref{Fig:SZ:Concavidad}-(b)  ilustra como  se  puede obtener  una
  mezcla  de distribuciones  de dos  probabilidad $p_{(1)}$  (dado  izquierda) y
  $p_{(2)}$ (dado  derecho) haciendo  una elecci\'on aleatoria  a partir  de una
  moneda en  este ejemplo (probabilidad  $\pi_1 = 1  - \pi_2$ de elegir  el dado
  izquierda).\newline
  %
  \begin{figure}[h!]
  %
  \begin{center} \input{TIKZ_SZ/Concavidad} \end{center}
  %
  \leyenda{(a) $\phi(t) =  t \log t$ es convexa: la  curva est\'a siempre debajo
  de sus cuerdas. Dicho de otra manera, sean los puntos \ $A_1$ \ de coordenadas
  $(t_1  \,  ,   \,  \phi(t_1)$  \  y   \  $A_2$  \  de   coordenadas  $(t_2  \,
  , \,  \phi(t_2)$: cualquier punto \  $A_\pi$ \ de coordenadas  \ $\left( \pi_1
  t_1 +  \pi_2 t_2 \,  , \, \phi(\pi_1  t_1 + \pi_2 t_2)  \right)$ \ con  \ $\pi
  =  \begin{bmatrix} \pi_1  & \pi_2  \end{bmatrix}^t \in  \Simp{1}$ \  queda por
  abajo del segmento \  $\left[ A_1 \; A_2 \right]$, \ie  $\phi(\pi_1 t_1 + \pi_2
  t_2) \le  \pi_1 \phi(t_1) + \pi_2  \phi(t_2)$. Por recurencia se  generaliza a
  m\'as de  dos punto $t_1,  \ldots ,  t_n, \: n  \in \Nset_0$ y  cualquier $\pi
  = \begin{bmatrix}  \pi_1 &  \cdots & \pi_n  \end{bmatrix}^t \in  \Simp{n-1}$ \
  as\'i como, fijando  $x \in \X$, tomando puntos $t_k  = p_{(k)}(x)$, aplicando
  la desigualdad y sumando cada lado sobre  $x \in \X$ se obtiene la desigualdad
  de Jensen (desigualdad al rev\'es por el signo ``-'' delante de la funci\'on \
  $t \log t$ \  en la definici\'on de la entrop\'ia).   (b) Ilustraci\'on de una
  distribuci\'on de  mezcla de dos  distribuciones, ac\'a mezclando  $p_{(1)}$ y
  $p_{(2)}$ a partir de una tercera variable aleatoria (ac\'a de Bernoulli).}
  %
  \label{Fig:SZ:Concavidad}
  \end{figure}
  %
  \SZ{Ver para dibujar la figura en tikz (no importar figuras extrenas)}
%
\setcounter{PropSchurConcavidad}{\value{enumi}}
\item\label{Prop:SZ:Schurconcavidad}  {\it Concavidad de Schur:}  como se  puede
  querrer, cuanto  m\'as ``concentrada'' es una  distribuci\'on de probabilidad,
  menos ignorancia hay, y entonces m\'as peque\~na debe ser la entrop\'ia.  Esta
  propiedad intuitiva se resume a partir  de la noci\'on de mayorizaci\'on vista
  en la definici\'on~\ref{Def:MP:Mayorizacion},
%  pagina~\pageref{Def:MP:Mayorizacion}
  (recuerdense  que si  los  vectores  no tienen  el  mismo  tama\~no, el  m\'as
  peque\~no es completado por ceros;  es equivalente a a\~nadir estados fictivos
  de probabilidad nula, lo que no cambia la entrop\'ia).
  %
  La  concavidad de Schur  se  traduce  por  la  relaci\'on
  %
  \[
  p \prec  q \quad \Rightarrow  \quad H(p) \ge  H(q).
  \]
  %
  Notar que  las cotas sobre  $H$ pueden ser  vistas como consecuencias  de esta
  desigualdad:  la distribuci\'on  cierta mayoriza  cualquiera distribuci\'on  y
  cualquiera distribuci\'on mayoriza la uniforme~\cite[p.~9, (6)-(8)]{MarOlk11}.
  Adem\'as, de la concavidad de Schur se obtiene que
  %
  \[
  H\left( \begin{bmatrix}  \frac{1}{|\X|} & \cdots  & \frac{1}{|\X|} \end{bmatrix}^t
  \right) \quad \mbox{es una funci\'on creciente de } |\X|.
  \]
  %
  La prueba de la  concavidad de Schur se apoya sobre la  desigualdad de Schur o
  Hardy-Littlewood-P\'olya    o     Karamata~\cite{Sch23,    HarLit29,    Kar32,
  HarLit52},~\cite[Cap.~3,  Prop.~C.1]{MarOlk11} o~\cite[Teorema~II.3.1]{Bha97}:
  $p  \prec q  \: \Rightarrow  \: \sum_i  \phi(p_i) \le  \sum_i \phi(q_i)$  para
  cualquiera funci\'on  $\phi$ convexa.  Basta  considerar $\phi(t) = t  \log t$
  para concluir  (se reversa  la desigualdad  por el signo  ``-'' delante  de la
  funci\'on $\phi$ en la definici\'on de la entrop\'ia).
\end{propiedades}
%
\SZ{
Ver Schur-Ostrowski f sym, Scur-convexe ssi $(xi  - xj) (df/dx_i - df/dxj) \ge 0,
 1 \le i \ne j \le alpha$}


En muchos  casos, uno tiene que  trabajar con varias  variables aleatorias. Para
simplificar las notaciones, consideramos  un par de variables \ $X$ \  e \ $Y$ \
definidas respectivamente sobre los alfabetos \ $\X$  \ e \ $\Y$ \ de cardenales \
$|\X|$ \ e \ $|\Y|$. Este par de variables puede ser vista como
una variable $\begin{bmatrix} X & Y\end{bmatrix}^t$ definida sobre el alfabeto $\X
\times  \Y$  de  cardenal \ $|\X| |\Y|$ \ as\'i como  se define  naturalmente  la
entrop\'ia  para  esta  variable;  tal entrop\'ia  es  llamada  {\it  entrop\'ia
  conjunta} de \ $X$ \ e \ $Y$:
%
\begin{definicion}[Entrop\'ia conjunta]
\label{Def:SZ:EntropiaConjunta}
%
  Sean \ $X$ \ e \ $Y$  \ dos variables aleatorias definidas sobre los alfabetos
  discretos \ $\X$  \ e \ $\Y$, de cardenales  \ $|\X| < +\infty$ \ y  \ $|\Y| <
  +\infty$  \  respectivamente.    Sea  \  $p_{X,Y}$  \   la  distribuci\'on  de
  probabilidad  conjunta   de  \   $X$  \  e   \  $Y$,  \   \ie  $   \forall  \,
  (x,y)  \in \X  \times  \Y, \quad  p_{X,Y}(x,y)  = P\big(  (X =  x)  \cap (Y  =
  y) \big)$.   La entrop\'ia conjunta de  Shannon de las  variables \ $X$ \  e \
  $Y$ \ se define como
  %
  \[
  H(p_{X,Y}) =  H(X,Y) = -  \sum_{(x,y) \in \X  \times \Y} p_{X,Y}(x,y)  \, \log
  p_{X,Y}(x,y),
  \]
  %
  con la convenci\'on \ $0 \log 0 \igualc 0$.
\end{definicion}

A partir  de esta  definici\'on, aparecen otras  propiedades importantes,  si no
fundamentales, de la entrop\'ia de Shannon.
%
\begin{propiedades}
\item\label{Prop:SZ:aditividad} {\it Aditividad:}  la entrop\'ia conjunta de dos
  variables  aleatorias \  $X$ \  e \  $Y$ \  independientes es  la suma  de las
  entrop\'ias individuales, y rec\'iprocamente:
  %
  \[
  X \: \mbox{e} \: Y \:\ \mbox{independientes} \quad \Leftrightarrow \quad H(X,Y)
  =  H(X) +  H(Y).
  \]
  %
  Dicho de otra  manera, para dos variables aleatorias, la  incerteza global es
  la  suma  de  las  incertezas  de cada  variable  individual.   La  propiedad
  ``$\Rightarrow$''  es consecuencia  directa  de \  $p_{X,Y}(x,y)  = p_X(x)  \,
  p_Y(y)$.   Se va  a  probar en  la secci\'on  siguiente  la rec\'iproca.  Esta
  propiedad se escribe tambi\'en
  %
  \[
  H\left( p_X \otimes p_Y \right) = H\left( p_X \right) + H\left( p_Y \right),
  \]
  %
  donde  $\otimes$ es el  producto \modif{externo (ver notaciones)}
% de  Kronecker~\footnote{Recuerdense de  que \
%    $p_X  \otimes p_Y$ es  un vector  de tama\~no  $\alpha \beta$  de componente
%    $(i-1)  \alpha +  j$-\'esima \  $p_X(x_i)  \, p_Y(y_j),  \quad 1  \le i  \le
%    \alpha, \: 1 \le  j \le \beta$.  Se lo puedo ver  tambi\'en como un producto
%    tensorial  o externo  de \  $p_X$ \  definido sobre  \ $\X$  \ y  \  $p_Y$ \
%    definido sobre \  $\Y$, \ el producto tensorial siendo  definido sobre \ $\X
%    \times      Y$.       Ver      nota      de      pie~\ref{Foot:MP:Kronecker}
%    pagina~\pageref{Foot:MP:Kronecker}.}.
  Se  generaliza sencillamente  a un  conjunto de  variables aleatorias  $\{ X_k
  \}_{k=1}^n$ (o, equivalentemente a  un producto \modif{externo} de un conjunto
  de vectores de probabilidades).
%
\item\label{Prop:SZ:subaditividad} {\it  Sub-aditividad:} la entrop\'ia conjunta
  de $n$ variables  aleatorias $\{ X_k \}_{k=1}^n$ es siempre  menor o igual que la suma
  de las entrop\'ias individuales:
  %
  \[
  H(X_1,\ldots,X_n)  \,  \le \,  \sum_{k=1}^n  H(X_k)  \qquad \mbox{\ie}  \qquad
  H\left(  p_{X_1 , \ldots  , X_n}  \right) \,  \le \,  H\left(  p_{X_1} \otimes
    \cdots \otimes p_{X_n} \right) = \sum_{k=1}^n H\left( p_{X_k} \right).
  \]
  %
  Dicho de otra manera, las variables aleatorias pueden compartir informaci\'on,
  de  tal  manera  que la  entrop\'ia  global  sea  menor  que la  suma  de  las
  entrop\'ias  de  cada variable.   De  la  propiedad  anterior, se  obtiene  la
  igualdad si y solamente si las \ $X_i$ \ son independientes.
%
\item\label{Prop:SZ:superaditividad}   {\it  Super-aditividad:}   la  entrop\'ia
  conjunta de  $n$ variables aleatorias $\{  X_k \}_{k=1}^n$ es siempre  mayor o
  igual que cualesquiera de las entrop\'ias individuales:
  %
  \[
  H(X_1,\ldots,X_n) \, \ge \, \max_{1 \le k \le n} H(X_k).
  \]
\end{propiedades}

Es importante notar que existen varios  enfoques basados en una serie de axiomas
que dan  lugar a la definici\'on  de la entrop\'ia. Estos  axiomas son conocidos
como {\it axiomas de Shannon-Khinchin}; estos requisitos (m\'as que propiedades)
son            la           continuidad~\ref{Prop:SZ:continuidad},            la
maximalidad~\ref{Prop:SZ:cotamaxima},                                         la
expansabilidad~\ref{Prop:SZ:expansabilidad}                 y                 la
aditividad~\ref{Prop:SZ:aditividad}.  Existen varios  otros conjuntos de axiomas
que conducen tambi\'en a la entrop\'ia de Shannon (ver~\cite[Sec.~6]{Sha48} o
\cite{ShaWea64, Fad56, Fad58, Khi57, Ren61}, entre otros).

Para  una  serie de  variables  aleatorias,  $X_1, X_2,  \ldots$,  representando
s\'imbolos, se define tambi\'en una  entrop\'ia por s\'imbolo como la entrop\'ia
conjunta  dividida  por el  n\'umero  de  s\'imbolos,  $\frac{H(X_1 ,  \ldots  ,
X_n)}{n}$, as\'i como una tasa de entrop\'ia cuando $n$ va a infinito.
%
\begin{definicion}[Tasa de entrop\'ia]
\label{Def:SZ:TasaDeEntropia}
%
  Sea $X \equiv \{ X_k \}_{k  \in \Nset_0}$ una serie de variables aleatorias, o
  proceso estoc\'astico.  La tasa de entrop\'ia del proceso es definida por
  %
  \[
  \H(X) = \lim_{n \to \infty} \frac{H(X_1,\ldots,X_n)}{n}.
  \]
  %
\end{definicion}
%
\noindent Esta  cantidad siempre existe  porque $\displaystyle H(X_1 ,  \ldots ,
X_n) \le \sum_{k=1}^n H(X_k) \le \sum_{k=1}^n  \log |\X_k| \le n \max_{1 \le k
  \le n} \log |\X_k|$.
  %donde los $\alpha_i$  son los cardenales de los alfabetos  de definici\'on de
  %los $X_i$.

\

Se termina esta subsecci\'on con el caso de variables discretas definidas sobre
un  alfabeto $\X$ de  cardenal infinito  $|\X| =  + \infty$,  por ejemplo  $\X =
\Nset$.   Por analog\'ia,  se puede  siempre definir  la entrop\'ia  como  en la
definici\'on Def.~\ref{Def:SZ:Shannon}. Esta extensi\'on resulta delicada dado
 que algunas propiedades se perdien.  Por ejemplo, la entrop\'ia no queda acotada
por arriba  como se puede  probar con la  distribuci\'on de  probabilidad \
$\displaystyle p(x)  \propto \frac{1}{(x+2) \big(  \log (x+2) \big)^2},  \: x
\in \Nset$, correctamente  normalizada ($\propto$ significa ``proporcional a''):
\ $\displaystyle \frac{\log \log(x+2)}{(x+2) \big( \log (x+2) \big)^2} \ge 0$
\  y  \ la  serie  \  $\displaystyle \sum_x  \frac{1}{(x+2)  \log  (x+2)}$ \  es
divergente, luego la serie \ $\displaystyle - \sum_x p(x) \log p(x)$ \ diverge.

% ================================= Entropia diferencial

\subseccion{Entrop\'ia diferencial}
\label{Ssec:SZ:Diferencial}

Volviendo  a  la  definici\'on  Def.~\ref{Def:SZ:Shannon} de  la  entrop\'ia  de
Shannon,  usando   el  operador   $\Esp$  promedio  estad\'istico   o  esperanza
matem\'atica, se puede reescribir la entrop\'ia de Shannon de la variable $X$ de
distribuci\'on de  probabilidad $p_X$ como~\footnote{Por  definici\'on, $p_X(X)$
es una variable aleatoria, transformada de $X$ por su distribuci\'on. Se obtiene
la forma  por teorema de transferencia  Teo.~\ref{Teo:MP:Transferencia}.}  $H(X)
=  \Esp\left[ -  \log p_X(X)  \right]$.   Con este  punto de  vista, es  f\'acil
extender  la definici\'on  de entrop\'ia  a variables  aleatorias continuas  que
admiten una  densidad de probabilidad.   Esto da lugar a  lo que se  conoce como
{\it entrop\'ia diferencial}:

\begin{definicion}[Entrop\'ia diferencial]
\label{Def:SZ:EntropiaDiferencial}
%
  Sea  $X$  una   variable  aleatoria  continua  que  admite   una  densidad  de
  probabilidad \ $p_X$,  definida sobre \ $\Rset^d$,  \ y sea \  $X(\Omega) = \X
  = \{  x \in  \Rset^d: \:  p_X(x) >  0 \}  = \sop(p_X)  \subset \Rset^d$  \ el
  soporte  de \  $p_X(x)$.  La  entrop\'ia diferencial  de  la  variable $X$  se
  define como
  %
  \[
  H(p_X) = H(X) = - \int_\X p_X(x) \, \log p_X(x) \, dx
  \]
  %
  (con la  convenci\'on $0 \log  0 \igualc 0$,  se puede escribir la  integraci\'on en
  $\Rset^d$).
\end{definicion}
%
Como  en   el  caso  discreto,  con   $X  =  \begin{bmatrix}  X_1   &  \cdots  &
X_d \end{bmatrix}^d$,  la entrop\'ia de  $X$ se llama tambi\'en  {\it entrop\'ia
diferecial conjunta} de los $X_k$.

Como veremos,  la entrop\'ia  diferencial no tiene  la misma  significaci\'on de
ignorancia o  incerteza, ya  que no  depende solamente  de la  distribuci\'on de
probabilidad, sino tambi\'en de los estados.   Adem\'as, no se la puede ver como
l\'imite continuo de  un caso discreto: v\'inculando  la entrop\'ia ``l\'imite''
de  la versi\'on  cuantificada  de  $X$ y  la  entrop\'ia diferencial,  l\'imite
precisamente,  veremos que  se  llama  diferencial, a  causa  del  efecto de  la
``diferencial  $dx$''.   Para ilustrar  este  hecho,  consideramos una  variable
aleatoria escalar  \ $X$ \ y  \ $p_X$ \  su densidad de probabilidad  de soporte
$\Rset$.   Sea  \   $\Delta  >  0$  \   y  sea  el  alfabeto   $\X^\Delta  =  \{
x_k \}_{k \in  \Zset}$ \ donde los  \ $x_k$ se definen  tales que $\displaystyle
p_X(x_k) \Delta = \int_{k \Delta}^{(k+1) \Delta}  p_X(x) \, dx$, como se ilustra
en la figura Fig.~\ref{Fig:SZ:CuantificacionX}.  Se define la variable aleatoria
discreta \ $X^\Delta =  \sum_k x_k \uno_{(X \in [k \Delta \;  (k+1) \Delta ))}$ \
sobre  \ $\X^\Delta$  \  tal que  \  $P(X^\Delta =  x_k)  = p_{X^\Delta}(x_k)  =
p_X(x_k)  \Delta$.   \  Se  puede  ver  a  \  $X^\Delta$  \  como  la  versi\'on
cuantificada de \ $X$, \  con \ $X^\Delta = x_k$ \ cuando \  $X \in [k \Delta \;
(k+1) \Delta )$.   \ Al rev\'es, aunque  sea delicado, se puede  interpretar a \
$X$ \  como el ``l\'imite'' de  \ $X^\Delta$ \ cuando  \ $\Delta$ \ tiende  a 0.
Ahora bien, es claro que
%
\begin{eqnarray*}
H(X^\Delta) & = & - \sum_k p_{X^\Delta}(x_k) \log p_{X^\Delta}(x_k)\\[2.5mm]
%
& = & - \log \Delta - \sum_k \Big( p_X(x_k) \log p_X(x_k) \Big) \, \Delta
\end{eqnarray*}
%
donde usamos la condici\'on de normalizaci\'on; luego
%
\[
H(X^\Delta)  + \log  \Delta =  - \sum_k  \Big( p_X(x_k)  \log p_X(x_k)  \Big) \,
\Delta.
\]
%
Entonces,  en el  l\'imite de  $\Delta$ tendiendo  a 0,  de la  integraci\'on de
Riemann sale que
%
\[
\lim_{\Delta \to 0} \big( H(X^\Delta) + \log \Delta \big) = H(X).
\]
%
Dicho de otra manera,  la entrop\'ia diferencial de $X$ no es  el l\'imite de la
entrop\'ia  de  la  versi\'on  cuantificada  de  la  variable:  aparece  con  la
entrop\'ia  el  t\'ermino  $\log  \Delta$  que  se  puede  interpretar  como  la
``entrop\'ia de un  t\'ermino diferencial, $\log dx$''.
%
\begin{figure}[h!]
%
\begin{center} \input{TIKZ_SZ/CuantificacionX} \end{center}
%
\leyenda{Densidad  de probabilidad  $p_X$  de $X$,  construcci\'on del  alfabeto
  $\X^\Delta$ donde se define la versi\'on cuantificada $X^\Delta$ de $X$ con su
  distribuci\'on  discreta de  probabilidad  $p_{X^\Delta}$. El  \'area en  gris
  oscuro es igual aa \'area definida por el rect\'angulo en gris claro.}
%
\label{Fig:SZ:CuantificacionX}
\end{figure}

M\'as  all\'a de esta  notable diferencia  entre la  entrop\'ia y  la entrop\'ia
diferencial, la \'ultima depende de los estados,  es decir que si $Y = g(X)$ con
$g$   biyectiva,   no   se   conserva   la  entrop\'ia,   \ie   se   pierde   la
propiedad~\ref{Prop:SZ:biyeccion} del caso discreto:
%
\begin{eqnarray*}
H(Y) & = & - \int_{\Rset^d} p_Y(y) \log p_Y(y) \, dy\\[2.5mm]
%
& = &  - \int_{\Rset^d} p_Y(g(x)) \log p_Y(g(x)) \, |\Jac_g(x)| \, dx\\[2.5mm]
%
& = & - \int_{\Rset^d} p_Y(g(x)) \Big( \log \big( p_Y(g(x)) \, |\Jac_g(x)| \big) -
\log |\modif{\Jac_g(x)}| \Big) \, |\Jac_g(x)| \, dx
\end{eqnarray*}
%
donde   $\Jac_g$   es   la   matriz   Jacobiana   de   la   transformaci\'on   \
$g: \Rset^d \mapsto \Rset^d$ \ \ y  \ $|\cdot|$ representa el valor absoluto del
determinante de  la matriz \modif{(ver  notaciones)}. La secunda linea  viene de
$log p_Y(g(x)) = log  \frac{p_Y(g(x)) \, |\Jac_g(x)|}{|\Jac_g(x)|}$.  Recordando
que         $p_X(x)         =        p_Y(g(x))         |\Jac_g(x)|$         (ver
subsecci\'on~\ref{Ssec:MP:Transformacion}),
%pagina~\pageref{Ssec:MP:Transformacion}), 
se obtiene la propiedad siguiente:
%
\begin{propiedadesC}\setcounter{enumi}{\value{PropBiyeccion}}
%
\item\label{Prop:SZ:biyeccionC}
Para  cualquier biyecci\'on $g:  \Rset^d \mapsto  \Rset^d$ se tiene
  %
  \[
  H(g(X)) = H(X) + \int_{\Rset^d} p_X(x) \log |\Jac_g(x)| \, dx,
  \]
  %
  donde el \'ultimo t\'ermino, $\Esp\left[  \log |\Jac_g(X)| \right]$ no vale cero
  en general. Por ejemplo, $H$ es invariante bajo una translaci\'on,
  %
  \[
  H(X+b) = H(X) \quad \forall \: b \in \Rset^d,
  \]
  %
  pero no  es invariante  por cambio  de escala,
  %
  \[
  H(a X) = H(X) + d \, \log |a| \quad \forall \: a \in \Rset_0.
  \]
\end{propiedadesC}
%
Esta \'ultima  relaci\'on queda v\'alida  para $a$ matriz invertible.   Por esta
\'ultima  relaci\'on, se puede  ver que,  dado $X$,  cuando $|a|$  tiende a  0, la
entrop\'ia de $a X$ tiende a  $-\infty$.  Es decir que, para $a$ suficientemente
peque\~no,  se puede  tener $H(a  X)  < 0$,  as\'i  que se  pierde tambi\'en  la
positividad~\ref{Prop:SZ:positividad}.  Por  este motivo, definitivamente  no es
posible asignar la interpretaci\'on  de ignorancia/informaci\'on a la entrop\'ia
diferencial.

A veces, se usa lo que se llama potencia entr\'opica:
%
\begin{definicion}[Potencia entr\'opica]
\label{Def:SZ:PotenciaEntropica}
%
  Sea $X$  una variable aleatoria  $d$-dimensional de entrop\'ia $H(X)$  en nats
  (logaritmo     natural)~\footnote{Si    la     base    del     logaritmo    es
  $\alpha  \in \Rset_0$,  en lugar  del  exponencial se  tomar\'a obviamente  la
  potencia de $\alpha$ en  lugar de la de $e$.}. La  potencia entr\'opica de $X$
  se define como
  %
  \[
  N(X) = \frac{1}{2 \pi e} \exp\left( \frac2d H(X) \right).
  \]
\end{definicion}
%
\noindent Por construcci\'on,  $N(X) >  0$.  Adem\'as,  en el caso  continuo,
$N(a X+b)  = |a|^2 N(X)$  (v\'alido tambi\'en  para una matriz  $a$ invertible):
esta propiedad  puede justificar  la idea de  ``potencia''; adem\'as  $N(a X+b)$
tiende  naturalmente a  cero cuando  $a$ tiende  a cero.   Se recupera  as\'i la
noci\'on informacional a trav\'es de $N$ en  este contexto ($a X + b$ ``tiende''
a $b$, variable determinista).

Aun   si  se   pierde  la   propiedad  de   invarianza  bajo   una  biyecci\'on,
sorprendentemente se  conserva la  entrop\'ia bajo  el equivalente  continuo del
rearreglo.

\begin{propiedadesC}\setcounter{enumi}{\value{PropPermutacion}}
\item\label{Prop:SZ:permutacionC} {\it Invarianza  bajo un rearreglo sim\'etrico:}
Sea  $p_X$   densidad  de   probabilidad  sobre  un   abierto  de   $\Rset^d$  y
$p_X^\downarrow$ su rearreglo sim\'etrico, Def.~\ref{Def:MP:rearreglo}. Entonces,
  %
  \[
  H\left( p_X^\downarrow \right) = H(p_X).
  \]
%  donde  $p_X^\downarrow$   es  el  rearreglo  s\'imetrico   de  $p_X$  .
% pagina~\pageref{Def:MP:rearreglo}.
\end{propiedadesC}
%
\noindent
Esta propiedad es probada para la integral de cualquier funci\'on $\phi$ convexa
de    la   densidad    de    probabilidad    por   ejemplo    en~\cite{LieLos01}
o~\cite[Lema~7.2]{WanMad04}~\footnote{En~\cite[Sec.~3.3]{LieLos01}  lo  muestran
para  $\phi$ diferencia  de dos  funciones  monotonas, siendo  $\phi: t  \mapsto
t  \log t$  un caso  particular.},  y entonces  para el  caso particular  $\phi:
t \mapsto t \log t$.

Una pregunta  natural es saber  que pasa en  t\'erminos de mayorizaci\'on  en el
contexto   continuo    $d$-dimensional.    Aparece   que   la    concavidad   de
Schur~\ref{Prop:SZ:Schurconcavidad} se verifica tambi\'en en el caso continuo,
\ie
%
\[
p \prec  q  \quad \Rightarrow  \quad H(p)  \ge H(q).
\]
%
con      la      relaci\'on      de      mayorizaci\'on      continua      vista
Def.~\ref{Def:MP:MayorizacionC}.   La  desigualdad  inversa se  prueba  para  la
integral  de  cualquier funci\'on  $\phi$  convexa  de la  densidad~\cite{Cho74}
o~\cite[Prop.~7.3]{WanMad04}, en particular para $\phi: t \mapsto t \log t$.

Como se  lo ha  visto, la  entrop\'ia diferencial no  es siempre  positiva, como
consecuencia de  la propiedad~\ref{Prop:SZ:biyeccionC}.  Tambi\'en  la propiedad
de cota  superior~\ref{Prop:SZ:cotamaxima} se  pierde, en  general, salvo  si se
ponen v\'inculos:
%
\begin{propiedadesC}\setcounter{enumi}{\value{PropCotamaxima}}
\item
  \begin{enumerate}
  \item\label{Prop:SZ:cotamaximauniforme} Si $\X$ es de volumen finito $|\X| < +
    \infty$, la entrop\'ia est\'a acotada por arriba,
    %
    \[
    H(X) \le \log |\X|,
    \]
    %
    con igualdad si y solamente si $X$ es uniforme.
    %
  \item\label{Prop:SZ:cotamaximagaussiana}  Si $\X  =\Rset^d$  y  $X$ tiene  una
    matriz  de covarianza  dada \  $\Sigma_X =  \Esp\left[ X  X^t \right]  - m_X
    m_X^t$ \  donde \ $m_X  = \Esp[X]$, la  entrop\'ia est\'a  acotada por
    arriba,
    %
    \[
    H(X) \le \frac{d}{2} \log(2 \pi e) + \frac12 \log |\Sigma_X|,
    \]
    %
    con igualdad si y solamente si $X$ es gaussiana.  En particular, la potencia
    entr\'opica de la gaussiana vale $N(X) = \left| \Sigma_X \right|^{\frac1d}$,
    dando de nuevo un  rol de potencia a $N$.  Como  veremos en este cap\'itulo,
    la  distribuci\'on gaussiana  juega  un rol  central en  la  teor\'ia de  la
    informaci\'on.
  \end{enumerate}
  %
  En  ambos casos,  estas  desigualdades con  la  distribuci\'on maximizante  se
    obtienen resolviendo el problema de maximizaci\'on de la entrop\'ia sujeto a
    v\'inculos.   Trataremos  el caso  con  v\'inculos  m\'as generales  que  la
    covarianza en la subsecci\'on Sec.~\ref{Ssec:SZ:MaxEnt}.
\end{propiedadesC}

Por    \'ultimo,     se    verifican     obviamente    las     propiedades    de
concavidad~\ref{Prop:SZ:concavidad}, de aditividad~\ref{Prop:SZ:aditividad} y de
sub-aditividad~\ref{Prop:SZ:subaditividad}.   Es  interesante  notar que  de  la
desigualdad~\ref{Prop:SZ:subaditividad}, puramente entr\'opica, se puede deducir
la  desigualdad de  Hadamard, que  es  una relaci\'on  puramente matricial:  sea
$R  \in \Pos_d^+(\Rset)$  \ y  \ $R_{k,k}$  sus t\'erminos  diagonales; entonces
$\displaystyle    |R|    \le    \prod_{k=1}^d     R_{k,k}$    (viene    de    la
propiedad~\ref{Prop:SZ:subaditividad}  escrita  para  un  vector  aleatorio  $X$
gaussiano de  matriz de covarianza  $R$ (siendo  $R_{k,k}$ las varianzas  de las
componentes $X_k$) y tomando la exponencial de la desigualdad).

\

Como hemos  visto, la  entrop\'ia y  su versi\'on diferencial  no tienen  ni las
mismas propiedades,  ni completamente  la misma interpretaci\'on.   Sin embargo,
varias propiedades  se comparten y  se prueban de la  misma manera.  A  veces se
encuentra en la  literatura la notaci\'on \'unica \ $\sumint$,  para indicar que
se usa la  suma en el caso discreto  y la integraci\'on en el  caso continuo con
densidad~\cite{Rio07}.     No    obstante,    volviendo    al    fin    de    la
subsecci\'on~\ref{Ssec:MP:VAContinua} y a  la definici\'on~\ref{Def:MP:Dirac} de
medida  discreta   sobre  \  $\X   =  \{  x_j  \}_j$   \  dada  por   \  $\mu_\X
= \sum_j \delta_{x_j}$, vimos  que en el caso discreto \ $p_X$  \ es la densidad
de  la medida  de  probabilidad con  respecto  a \  $\mu_\X$.   Adem\'as, de  la
propiedad $\displaystyle \int_\Rset f(x) \  d\delta_{x_j} = f(x_j)$ se puede ver
una  suma como  una  integral con  respecto  a une  medida  discreta.  De  estas
observaciones, se  puede escribir de la  misma forma las entrop\'ias  discreta y
diferencial:
%
\begin{definicion}[Escritura \'unica de la entrop\'ia]
\label{Def:SZ:ShanonMu}
%
Sea \ $X$ \ variable aleatoria definida sobre $\X \subset \Rset^d$, que admite
una  densidad de  probabilidad \  $p_X$ \  con respecto  a una  medida \  $\mu$ \
(ej.  $\mu_\X$  \  en   el  caso  discreto  \  $\mu  =  \mu_L$   \  en  el  caso
diferencial). La entrop\'ia de $X$ con respecto a $\mu$ se escribe como
  %
  \[
  H(X) \equiv H(p_X) = - \int_\X p_X(x) \, \log p_X(x) \, d\mu(x)
  \]
  %
\end{definicion}
%
Insistimos en el hecho de que se puede entender esta definici\'on para cualquier
  $\mu$  y densidad  con  respecto a  $\mu$,  ya sea  discreta,  de Lebesgue,  o
  cualquiera.
