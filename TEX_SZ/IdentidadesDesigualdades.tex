\seccion{Unas identidades y desigualdades}
\label{Sec:SZ:Desigualdades}

\modif{\bf En esta secci\'on, por simplificaci\'on de notaciones, se olvidar\'a mencionar la medida con respecto a la cual se consideran las densidades, salvo si es necesario. Adem\'as, como lo vimos, varias cantidades informacionales no depende de esta medida (divergencias, informaci\'on de Fisher param\'etrica con $\mu$ independiente del param\'etro).}.
%vamos a considerar solamente los casos $\mu = \mu_L$ medida de Lebesgue o $\mu = \mu_\X$ discreta, \ie los casos de variables continuas o discretas. La mayoria de los resultados se generalizan sin costo adicional (queda obvio de las prubas de estos).
% Libro Loss Ruskai [Lieb]

\SZ{Desigualdades de Fano? Rioul p.  78, Cover P.~663, Sanov? Pythagorean? Gene:
  cf Zyc p60}

% ================================= Data Processing Theorem a la Shannon

\subseccion{Desigualdad de procesamiento de datos \`a la Shannon}
\label{Ssec:SZ:ProcDatosShannon}

Esta  desigualdad  traduce  que  procesando  datos,  no  se  puede  aumentar  la
informaci\'on disponible sobre  una variable. Se basa sobre  una desigualdad que
satisface la informaci\'on mutua aplicada a un proceso de Markov.

\begin{definicion}[Proceso de Markov]
\label{Def:SZ:ProcesoMarkov}
%
  Una secuencia  $X_1 \mapsto X_2 \mapsto  \ldots \mapsto X_n$ es  dicha {\it de
    Markov}   si   para  cualquier \   $i   >   1$,
  %
  \[
  \forall \: x_i, \quad P_{X_{i-1},X_{i+1}|X_i=x_i} = P_{X_{i-1}|X_i=x_i} \, P_{x_{i+1}|X_i=x_i}.
  \]
  %
  Dicho  de otra  manera,  condicionalmente  a \  $(X_i=x_i)$,  las variables  \
  $X_{i-1}$ \ y \ $X_{i+1}$ son independientes.  Eso es equivalente a
  %
  \[
  P_{X_{i+1}|(X_i,X_{i-1},\ldots)=(x_i,x_{i-1},\ldots)} = P_{X_{i+1}|X_i=x_i}.
  \]
  %
  Si  $i$ representa  un tiempo,  significa  que la  estad\'istica de  $X_{i+1}$
  conociendo todo el pasado se reduce  a esa conociendo el pasado inmediato (las
  probabilidades  dichas   de  transici\'on  \  $P_{X_{i+1}|X_i=x_i}$   \  y  la
  distribuci\'on inical  \ $P_{X_1}$  \ caracterizan completamente  el proceso).
  Es sencillo  fijarse que $X_n \mapsto  X_{n-1} \mapsto \ldots  \mapsto X_1$ es
  tambien un proceso de Markov.
\end{definicion}

\begin{teorema}[Desigualdad de procesamiento de datos]
\label{Teo:SZ:DesigualdadPreocesamientoDatos}
%
  Sea  $X \mapsto  Y \mapsto  Z$ un  proceso de  Markov. Entonces,
  %
  \[
  I(X;Y) \ge I(X;Z),
  \]
  %
  con igualdad si y solamente si $X \mapsto Z \mapsto Y$ es tambi\'en un proceso
  de Markov. En  particular, es sencillo ver que  para cualquiera funci\'on $g$,
  $X \mapsto Y \mapsto g(Y)$ es un proceso de Markov, lo que da
  %
  \[
  \forall \, g, \quad I(X;Y) \ge I(X;g(Y)).
  \]
  %
  La  \'ultima  desigualdad  se  escribe  tambi\'en  $H(X|g(Y))  \ge  H(X|Y)$  y
  significa que  procesar $Y$ no aumenta  la informaci\'on que $Y$  da sobre $X$
  (la incerteza condicional es m\'as importante).
\end{teorema}
%
\begin{proof}
  Por definici\'on  de la  informaci\'on mutua, considerando  $X$ y  la variable
  conjunta $(Y,Z)$,
  %
  \begin{eqnarray*}
  I(X ; Y,Z) & = & H(X) - H(X|Y,Z)\\[2.5mm]
  %
  & = & H(X) - H(X|Y) + H(X|Y) - H(X|Y,Z)
  \end{eqnarray*}
  %
  \noindent Por la propiedad que $Z \mapsto Y \mapsto X$ es tambi\'en un proceso
  de Markov,  es sencillo probar que  $H(X|Y,Z) = H(X|Y)$  (conciendo $Y$ sufice
  para caracterizar completamente $X$), lo que da
  %
  \[
  I(X;Y,Z) = I(X;Y).
  \]
  %
  Tambi\'en,
  %
  \begin{eqnarray*}
  I(X ; Y,Z) & = & H(X) - H(X|Z) + H(X|Z) - H(X|Y,Z)\\[2.5mm]
  %
  & = & I(X;Z) + H(X|Z) - H(X|Y,Z)
  \end{eqnarray*}
  %
  \noindent Adem\'as, escribiendo \ $\frac{p_{X|(Y,Z)=(y,z)}(x)}{p_{X|Z=z}(x)} =
  \frac{p_{X|(Y,Z)=(y,z)}(x)   \,  p_{Y|Z=z}(y)}{p_{X|Z=z}(x)   p_{Y|Z=z}(y)}  =
  \frac{p_{X,Y|Z=z}(x,y)}{p_{X|Z=z}(x) p_{Y|Z=z}(y)}$ \ se nota de que \ $H(X|Z)
  -  H(X|Y,Z)$ \  es la  divergencia de  Kullback-Leibler de  \  $p_{X,Y|Z=z}$ \
  relativamente a  \ $p_{X|Z=z} p_{Y|Z=z}$,  o informaci\'on mutua  \ $I(X;Y|Z)$
  entre \ $X$ \ e \ $Y$,  condicionalmente a \ $Z$.  Entonces, de las dos formas
  de $H(X;Y,Z)$ viene
  %
  \[
  I(X;Y) = I(X;Z) + I(X;Y|Z).
  \]
  %
  La desigualdad del teorema viene de la positividad de $I(X;Y|Z)$. Adem\'as, se
  obtiene la igualdad si  y solamente si \ $I(X;Y|Z) = 0$, es decir  \ $X$ \ e \
  $Y$ \  independientes condicionalmente a \  $Z$, lo que es  la definici\'on de
  que $X \mapsto Z \mapsto Y$ es un proceso de Markov.
\end{proof}


% ================================= Data Processing Theorem a la Fisher

\subseccion{Desigualdad de procesamiento de datos \`a la Fisher}
\label{Ssec:SZ:ProcDatosFisher}

Si  la desigualdad  de  procesamiento de  datos  es conocido  a  trav\'es de  la
entrop\'ia  de  Shannon, aparece  que  se  expresa  tambi\'en  a tra\'es  de  la
informaci\'on de Fisher:

\begin{teorema}[Desigualdad de procesamiento de datos tipo Fisher]
\label{Teo:SZ:DesigualdadProcesamientoDatosFisher}
%
  Sea  $\theta  \mapsto  X  \mapsto  Y$   un  proceso  de  Markov  con  $\theta$
  determinista y $p_{X,Y}$ \modif{densidad diferenciable} parametrizado por $\theta$, es
  decir   en  este   contexto   que,  $p_{Y|X=x}$   no   es  parametrizado   por
  $\theta$. Entonces
  %
  \[
  J_\theta(X) \ge J_\theta(Y),
  \]
  %
  con igualdad si y solamente si $\theta \mapsto Y \mapsto X$ es tambi\'en
  de Markov. En  particular,
  %
  \[
  \forall \, g, \quad J_\theta(X) \ge J_\theta(g(X)).
  \]
  %
\end{teorema}
%
\begin{proof}
  De la regla de la cadena tenemos
  %
  \[
  J_\theta(Y|X) + J_\theta(Y) = J_\theta(X|Y) + J_\theta(X).
  \]
  %
  Del hecho que, por definici\'on de un processo de Markov, \ $p_{Y|X=x}$ \ no
  es parametrizado  por $\theta$  es sencillo  ver que  $J_\theta(Y|X) =  0$, la
  prueba se cerrando  de $J_\theta(X|Y) \ge 0$. Adem\'as se  obtiene la igualdad
  si y  solamente si  $J_\theta(X|Y) =  0$, es decir  de la  ``positividad'' del
  integrante dando  la matrix  de Fisher,  si y solamente  si $p_{X|Y=y}$  no es
  parametrizado por $\theta$.
\end{proof}



% ================================= EPI

\subseccion{Desigualdad de la potencia entr\'opica}
\label{Ssec:SZ:EPI}

Sean  $X$ e  $Y$  dos variables  independientes.   Si se  conoce las  relaciones
vinculando \  $H(X,Y)$, \  $H(X)$, \ $H(Y)$,  una pregunta natural  concierna la
relaci\'on  que podr\'ia  tener \  $X+Y$  \ con  cada variable  en t\'ermino  de
entrop\'ia. La respuesta no es trivial, y el resultado general concierna el caso
de  variables continuas  sobre $\Rset^d$.   Es conocido  como desigualdad  de la
potencia entr\'opica (EPI para entropy power inequality en ingl\'es). No vincula
las entrop\'ias, sino que las potencias entr\'opicas.
%
\begin{teorema}[Desigualdad de la potencia entr\'opica]
\label{Teo:SZ:EPI}
%
  Sean  $X$  e $Y$  dos  variables  $d$-dimensionales continuas  independientes.
  Entonces
  %
  \[
  N(X + Y) \ge N(X) + N(Y),
  \]
%
  con igualdad si y  solamente si \ $X$ \ e \ $Y$  \ son gaussianas con matrices
  de covarianza proporcionales, $\Sigma_Y \propto \Sigma_X$.
  % (siempre  verdad en  el contexto  escalar).
\end{teorema}
%
\noindent     Existen    varias     formulaciones     alternativas    a     esta
desiguladad~\cite{Sha48, Lie78, CovTho06, DemCov91, Rio07}:
%
\begin{enumerate}
\item\label{EPI:SZ:EquivGauss} Sean  \ $\widetilde{X}$  \ y \  $\widetilde{Y}$ \
  gaussianas independientes  de matrices de covarianza proporcionales  y tal que
  $H(\widetilde{X}) = H(X)$ y $H(\widetilde{Y}) = H(Y)$.  Entonces
  %
  \[
  N(X+Y) \ge N\left( \widetilde{X} + \widetilde{Y} \right),
  \]
  %
  con igualdad si y solamente si \ $X$  \ e \ $Y$ \ son gaussianas. De hecho, la
  primera  formulaci\'on  es equivalente  a  $N(X+Y)  \ge N\left(  \widetilde{X}
  \right) +  N\left( \widetilde{Y}  \right) = \frac{1}{2  \pi e}  \left( \left|
      \Sigma_{\widetilde{X}}  \right|^{\frac1d} +  \left| \Sigma_{\widetilde{Y}}
    \right|^{\frac1d}     \right)    \ge     \frac{1}{2    \pi     e}    \left|
    \Sigma_{\widetilde{X}} +  \Sigma_{\widetilde{Y}} \right|^{\frac1d} = N\left(
    \widetilde{X} + \widetilde{Y} \right)$  (la \'ultima desigualdad viniendo de
  la  desigualdad matricial de  Minkowski~\cite{HarLit52, Min10}).   Se notar\'a
  que,  de la  relaci\'on uno-uno  entre  $H$ y  $N$ la  desigualdad se  escribe
  tambi\'en \[ H(X+Y) \ge H\left( \widetilde{X} + \widetilde{Y} \right).\]
%
\item\label{EPI:SZ:PresCov}    {\it    Desigualdad    de    preservaci\'on    de
    covarianza:}
  %
  \[
  \forall  \:   0  \le  a  \le   1,  \quad  H\left(   \sqrt{a} \, X  +
    \sqrt{1-a} \, Y \right) \ge a \, H(X) + (1-a) \, H(Y),
  \]
  %
  con igualdad si y solamente si \ $X$ \ e \ $Y$ \ son gaussianas con matrices de
  covarianza  proporcionales.    Claramente,  se  cumple  la   igualdad  para  \
  $\widetilde{X}$  \ e  \ $\widetilde{X}$,  entonces  $H\left( \sqrt{a}  \, X  +
    \sqrt{1-a} \, Y \right) \ge a \, H(X) + (1-a) \, H(Y) \qquad \Leftrightarrow
  \qquad H\left( \sqrt{a} \, X + \sqrt{1-a} \, Y \right) \ge H\left( \sqrt{a} \,
    \widetilde{X} + \sqrt{1-a} \, \widetilde{Y} \right) $ \ lo que es nada m\'as
  que la desigualdad anterior  reemplazando \ $X$ \ por \ $\sqrt{a}  \, X$ \ e \
  $Y$ por \ $\sqrt{1-a} \, Y$ (y vice-versa).
\end{enumerate}

La  prueba  de  esta(s)  desigualdad(es)  no es  trivial.   N\'umeras  versiones
existen,  dadas por  ejemplo  en las  referencias~\cite{Bla65, Sta59,  ShaWea64,
  Rio07,  Rio11,  Rio17,  CovTho06,  DemCov91,  Lie78,  VerGuo06}  (ver  tambien
teorema~6 de~\cite{Lie75}).   Como se  lo puede ver,  la gaussiana juega  un rol
particular en esta desigualdad, saturandola.
\SZ{Ver si la de Rioul se escribe razonablemente sencillamente.}

Una gracia de la desigualdad de la potencia entr\'opica es que puede dar lugar a
pruebas  informacionales  de  desigualdades  matriciales, como  por  ejemplo  la
desigualdad  de Minkowsky  de los  determinentes  \ $|R_1  + R_2|^{\frac1d}  \ge
|R_1|^{\frac1d}  +  |R_2|^{\frac1d}$  \  para cualquieras  matrices  $R_1,  R_2$
sim\'etricas definidas  positivas, con igualdad  si y solamente si  $R_2 \propto
R_1$  (viene de  $X$ e  $Y$ gaussianas  de covarianza  $R_1$ y  $R_2$).  Aparece
tambi\'en  para acotar  la informaci\'on  mutua  entre variables  y calcular  la
capacidad  de   un  canal  de  comunicaci\'on   como  se  lo  va   a  ver  m\'as
adelante~\cite{CovTho06, DemCov91, Rio07, Joh04}.

Se  mencionar\'a que  existe  una versi\'on  de  la desigualdad  de la  potencia
entr\'opica con rearreglo~\cite{WanMad04}:
%
\begin{teorema}[Desigualdad de la potencia entr\'opica con rearreglo]
\label{Teo:SZ:EPIRearreglo}
%
Sean  $X$ e  $Y$  dos variables  $d$-dimensionales  continuas independientes  de
densidades   de  probabilidades   $p_X$   y  $p_Y$   respectivamente.   Sean   \
$p^\downarrow_X$  \ y  \  $p^\downarrow_Y$ \  los  rearreglos de  $p_X$ y  $p_Y$
respectivamente y  denotamos \  $X^\downarrow$ \ y  \ $Y^\downarrow$  \ vectores
independientes  de  distribuci\'on  de   probabilidad  \  $p^\downarrow_X$  \  y
$p^\downarrow_Y$ \ respectivamente.  Entonces
  %
  \[
  N(X + Y) \ge N(X^\downarrow + Y^\downarrow).
  \]
\end{teorema}

Se referir\'a a~\cite[y Ref.]{MadBar07} por ejemplo para varias generalizaciones
de la desigualdad de la potencia entr\'opica.

\

Para cerrar esta secci\'ion, se mencionar\'a  de que en el caso discreto, no hay
un  resultado general  y a\'un  existen contra-ejemplos~\cite[Sec.~IV]{JohYu10}.
Existen  solamente resultados  para variables  particulares como  para variables
binarias~\cite{ShaWyn90},   leyes   binomiales~\cite{HarVig03,  ShaDas11}   (ver
tambi\'en~\cite{JohYu10, HagAbb14}).


% ================================= EPI

\subseccion{Desigualdad convolucional de Fisher}
\label{Ssec:SZ:FisherConvolucional}


Mencionamos de que existe tambi\'en una desigualdad parecida a la de la potencia
entr\'opica,  teorema~\ref{Teo:SZ:EPI}, dada en  el caso  escalar en~\cite{Joh04,
  Bla65, Zam98, DemCov91, KagYu08}:
%
\begin{teorema}[Desigualdad convolucional de Fisher]
\label{Teo:SZ:DesigualdadConvolucionalFisher}
%
Sean  $X$  e  $Y$  dos  variables  $d$-dimensionales \modif{independientes, con densidades diferenciables.}
Entonces
  %
  \[
  \forall \, a \in [0 \;  1], \quad J\left( \sqrt{a} \, X + \sqrt{1-a} \, Y
  \right) \, \le \, a \, J(X) + (1-a) \, J(Y),
  \]
  %
  con igualdad si y  solamente si \ $X$ \ e \ $Y$  \ son gaussianas con matrices
  de covarianza proporcionales, $\Sigma_Y \propto \Sigma_X$.
\end{teorema}
%
\begin{proof}
  $X$  \  e  \  $Y$  \  siendo  independientes,  tenemos  para  $W  =  X  +  Y$,
  $\displaystyle p_W(w)  = \int_\X p_X(x)  p_Y(w-x) \, d\mu(x)$ convoluci\'on  de las
  distribuciones de \ $X$ \ y de \ $Y$ (ver ejemplo~\ref{Ej:MP:Suma}).
  %pagina~\pageref{Ej:MP:Suma}).  
  Escribiendo $S_X =  \nabla_x \log p_X$ el score  de $X$ y lo mismo  para $Y$ y
  $W$,
  %
  \begin{eqnarray*}
  S_W(w) & = & \int_\X \frac{p_X(x)}{p_W(w)} \, \nabla_w p_Y(w-x) \, d\mu(x)\\[2.5mm]
  %
  & = & - \int_\X \frac{p_X(x)}{p_W(w)} \, \nabla_x p_Y(w-x) \, d\mu(x)\\[2.5mm]
  %
  & = & \int_\X \frac{p_Y(w-x)}{p_W(w)} \, \nabla_x p_X(x) \, d\mu(x)\\[2.5mm]
  %
  & = & \int_\X \frac{p_Y(w-x) p_X(x)}{p_W(w)} \, \nabla_x \log p_X(x) \, d\mu(x)\\[2.5mm]
  %
  & = & \int_\X p_{X|W=w}(w) \, \nabla_x \log p_X(x) \, d\mu(x)\\[2.5mm]
  %
  & = & \Esp\left[ \left. S_X(X) \right| W = w \right]
  \end{eqnarray*}
  %
  Intercambiando  los roles de  \ $X$  \ e  \ $Y$,  tenemos tambi\'en  $S_W(w) =
  \Esp\left[ \left. S_Y(Y) \right| W =  w \right]$, as\'i que, para cualquier $0
  \le a \le 1$,
  %
  \[
  S_W(w) = \Esp\left[ \left. a S_X(X) + (1-a) S_Y(Y) \right| W = w \right].
  \]
  %
  A    continuaci\'on,    de     la    f\'ormula    de    K\"onig-Huyggens    (ver
  cap\'itulo~\ref{cap:MP:TeoriaProbabilidades},
  subsecci\'on~\ref{Ssec:MP:Momentos}),
  % pagina~\pageref{Ssec:MP:Momentos}),
  %
  \[
  S_W(w) S_W(w)^t \: \le \:  \Esp\left[ \left. \left(a S_X(X) + (1-a) S_Y(Y)
      \right) \left(a S_X(X) + (1-a) S_Y(Y) \right)^t \right| W = w \right],
  \]
  %
  es decir, tomando el promedio en $W$,
  %
  \[
  J(X+Y) \: \le  \: a^2 J(X) + (1-a)^2 J(Y) +  a (1-a) \left( \Esp\left[
      S_X(X) S_Y(Y)^t \right] + \Esp\left[ S_Y(Y) S_X(X)^t \right] \right).
  \]
  %
  Luego, \ $X$ \ e \ $Y$ \  siendo independientes, $S_X(X)$ \ y \ $S_Y(Y)$ \ son
  tambi\'en independientes. Adem\'as son centradas, probando que el t\'ermino en
  $a  (1-a)$ vale  cero, dando  una  versi\'on equivalente  del teorema;  La
  versi\'on dada se recupera re-emplazando \ $X$ \ por \ $\sqrt{a} \, X$ \ e \
  $Y$ \ por \ $\sqrt{1-a} \, Y$.

  Escribiendo la  desigualdad viniendo de  la f\'ormula de  K\"onig-Huyggens, se
  nota que  la igualdad  es satisfecha si  y solamente si  \ \modif{$a  S_X(x) +
  (1-a) S_Y(y) =  S_W(w)$ \ para cualquier \ $y,  w$ \ y \ $x =  w-y$, es decir,
  para cualquier \  $w, y$, \ $a  S_X(w-y) + (1-a) S_Y(y) =  S_W(w)$. Tomando la
  ``primitiva'' en \ $y$ \ se obtiene \  $-a \log p_X(w-y) + (1-a) \log p_Y(y) =
  y^t  S_W(w) +  g(w)$  \ donde  $g(w)$  \ es  una  constante de  integraci\'on.
  Diferenciando  ahora  esta  igualdad  con  respecto a  \  $w$  \  obtenemos  \
  $-a \nabla_w \log p_X(w-y)  = \Hess_w S_W(w) \: y +  \nabla_w g(w)$, es decir,
  en $w = 0$, notando que $\nabla_w \log p_X(w-y) = -\nabla_y \log p_X(w-y)$, se
  nota que \ $\nabla_y \log p_X(-y)$ \ es de la forma \ $\nabla_y \log p_X(-y) =
  R (y  + m)$ \  con \ $R  \in \Pos_d^+(\Rset), \: m  \in \Rset^d$, \ie  \ $\log
  p_X(y)  = \frac12  (y  -  m)^t R  (y  - m)  +  c$ \  con  $c$  \ constante  de
  integrac\'on:}  $X$   es  necesariamente   gausiana.   Similarmente,   $Y$  es
  necesariamente gausiana  tambi\'en \modif{y, por stabilidad  por combinaci\'on
  lineal     Teorema~\ref{Teo:MP:StabilidadGaussiana},     $W$    es     tambi\'en
  Gausiana}.  Adem\'as,  calculando  las  informaciones de  Fisher  en  el  caso
  gausiano, obtenemos  $\left( \Sigma_X + \Sigma_Y  \right)^{-1} = \Sigma_X^{-1}
  +  \Sigma_Y^{-1}$ lo  que  es posible  si  y  solamente si  $\Sigma_X$  \ y  \
  $\Sigma_Y$ \ son proporcionales.
\end{proof}
%
Este teorema  tiene varias consecuencias.  En  particular, interviene justamente
en la prueba de la desigualdad de la potencia entr\'opica.




% ================================= 2nda ley termodinamica

\subseccion{Segunda ley de la termodin\'amica}
\label{Ssec:SZ:SecLeyTermo}

Tratando de procesos  de Markov, aparece el equivalente de la  segunda ley de la
termodin\'amica:  un sistema  aislado evolua  hasta  llegar su  estado lo  m\'as
desorganizado (ver ej.~\cite[y ref.]{CovTho06, Mer10, Mer18}).

\begin{lema}[Versi\'on informacional de la segunda ley de la termodin\'amica]
\label{Lem:SZ:2ndLeyTermodinamica}
%
  Sea $X_1 \mapsto X_2 \mapsto \cdots  \mapsto X_n \mapsto \cdots$ un proceso de
  Markov,  con   probabilidades  de  transici\'on   $r_{X_{n+1}|X_n=x_n}$  dadas
  (independientemente de la condici\'on  inicial).  Estas \'ultimas modelizan el
  sistema, independiente de las  condiciones iniciales.  Sean dos distribuciones
  (condiciones) iniciales  diferentes $p_{X_1}$  y $q_{X_1}$, conduciendo  a las
  distribuciones $p_{X_n}$  y $q_{X_n}$  (con respecto a  una medida  $\mu$ dada)
  para $X_n$. Entonces:
%
\begin{itemize}
\item  Para cualquier  $n \ge  1$,
  %
  \[
  \Dkl[p_{X_{n+1}}]{q_{X_{n+1}}} \le \Dkl[p_{X_n}]{q_{X_n}};
  \]
  %
  las  distribuciones   $p_{X_n}$  y  $q_{X_n}$  no  se   ``alejan''  (tiende  a
  acercarse);
%
\item  Si  $p$  es  una  distribuci\'on  estacionaria,
  %
  \[
  \Dkl[p_{X_{n+1}}]{p} \le \Dkl[p_{X_n}]{p};
  \]
  %
  la distribuci\'on no se aleja de la distribuci\'on estacionaria.
%
\item Adem\'as, si  los $X_n$ tienen $K$ momentos fijos  $m = \Esp[M(X_n)] \quad
  \forall \,  n$ y si \  $p_{\mathrm{me}}$ \ es la  densidad (con respecto a  la medida $\mu$
  dada)  de  entrop\'ia  m\'axima  tiendo  los  mismos  momentos  como  descrito
  subsecci\'on  Sec.~\ref{Ssec:SZ:MaxEnt}, (ej.   $K =  0$, $\X$  de  cardenal o
  volumen finito y ley uniforme, $K = 2$, $M_k(x) = x^k$ y ley gausiana),
  %
  \[
  H(X_{n+1}) \ge H(X_n);
  \]
  %
  el  sistema   tiende  a  desorganizarse (dando los vinculos/momentos).
\end{itemize}
\end{lema}
%
\begin{proof}
  %Escribiendo  \ $P_{(n+1),(n)}$  \  y \  $Q_{(n+1),(n)}$  \ las  distribuciones
  %conjuntas  de  $(X_{n+1},X_n)$  \   para  las  dos  condiciones  iniciales,  \
  %$P_{(n+1)|(n)}$ \ y \ $Q_{(n+1)|(n)}$  \ las distribuciones condicionales de \
  %$X_{n+1}|X_n=x_n$  \  as\'i  que  $P_{(n)|(n+1)}$  \ y  \  $Q_{(n)|(n+1)}$  \  las
  %distribuciones condicionales de \  $X_n|X_{n+1}=x_{n+1}$, s
  Se        muestra       sencillamente        que        \       $\displaystyle
  \Dkl[p_{X_{n+1},X_n}]{q_{X_{n+1},X_n}}   =   \Dkl[p_{X_{n+1}}]{q_{X_{n+1}}}  +
  \int_{\X_{n+1}}    \Dkl[p_{X_n|X_{n+1}=x_{n+1}}]{q_{X_n|X_{n+1}=x_{n+1}}}   \,
  d\mu(x_{n+1})        =       \Dkl[p_{X_n}]{q_{X_n}}        +       \int_{\X_n}
  \Dkl[p_{X_{n+1}|X_n=x_n}]{q_{X_{n+1}|X_n=x_n}}   \,   d\mu(x_n)$.    Adem\'as,
  $p_{X_{n+1}|X_n=x_n}   =   r_{X_{n+1}|X_n=x_n}   =  q_{X_{n+1}|X_n   =   x_n}$
  (transici\'on  independiente   de  la  condici\'on   inicial),  conduciendo  a
  $\Dkl[p_{X_{n+1}|X_n=x_n}]{q_{X_{n+1}|X_n =  x_n}} = 0$ \  con consecuencia de
  que \ $\displaystyle \Dkl[p_{X_n}]{q_{X_n}} = \Dkl[p_{X_{n+1}}]{q_{X_{n+1}}} +
  \int_{\X_{n+1}} \Dkl[p_{X_n|X_{n+1} =  x_{n+1}}]{q_{X_n|X_{n+1} = x_{n+1}}} \,
  d\mu(x_{n+1})$.  \ $p_{X_n|X_{n+1} = x_{n+1}}$  no es necesariamente igual a \
  $q_{X_n|X_{n+1}  =  x_{n+1}}$, pero  la  divergencia  siendo  no negativa,  se
  obtiene la primera  desigualdad.  La segunda desigualdad se  obtiene tomando \
  $q_{X_n} = p$.   Adem\'as, si \ $p$ \ es de  entrop\'ia m\'axima de mismos
  momentos $m  = \Esp[M(X_n)]$  que los $X_n$,  hemos visto  de que \  $p(x) =
  e^{\eta^t M(x)}$ \ ley de la familia exponencial, dando \ $\Dkl[p_{X_n}]{p}
  = -H(X_n) - \eta^t m$, conduciendo a la \'ultima desigualdad.
\end{proof}


\

\SZ{Ver si existe la contraparte con Fisher? \cite{Fri90}}

% ================================= Desigualdad de Cram\'er-Rao

\subseccion{Desigualdad de Cram\'er-Rao}
\label{Ssec:SZ:CramerRao}

Una  otra  interpretaci\'on  de \  $J$  \  como  informaci\'on  es debido  a  la
desigualdad   de   Cram\'er-Rao   que   la   relaciona  a   la   covarianza   de
estimaci\'on~\footnote{De hecho, pareci\'o esta f\'ormula tambi\'en en los papeles
  de  Fr\'echet y  de Darmois~\cite{Fre43,  Dar45}. Como  citado  por Fr\'echet,
  aparece que la primera versi\'on de esta f\'ormula es mucho m\'as vieja y debido
  a K.  Pearson  \& L. N.  G Filon~\cite{PeaFil98} en  1898; luego fue extendida
  por          Edgeworth~\cite{Edg08},          Fisher~\cite{Fis25:07}         o
  Doob~\cite{Doo36}.}~\cite{Rao45,  Rao92,  RaoWis47,  Cra46,  Rio07,  CovTho06,
  Fri04, Kay93,  Bos07}.  Sea \  $X$ \ parametrizada  por $\theta$.  La  meta es
estimar \ $\theta$  \ a partir de \  $X$.  Tal estimador va a  ser una funci\'on
\'unicamente de  \ $X$, lo  que se escribe usualmente~\footnote{Por  ejemplo, si
  $\theta$  es  un promedio  com\'un  a los  componentes  de  $X$, un  estimador
  podr\'ia    ser    \     $\widehat{\theta}    =    \frac1d    \sum_i    X_i$.}
$\widehat{\theta}(X)$ \ (la funci\'on  no depende expl\'icitamente de $\theta$).
Las caracter\'isticas de  la calidad de un estimator es  naturalmente su sesgo \
$b(\theta) = \Esp\left[  \widehat{\theta}(X) \right] - \theta$ \  y su matriz de
covarianza  \  $\Sigma_{\widehat{\theta}}$ \  (la  varianza  da la  dispersi\'on
alrededor de su promedio).  La desigualdad de Cram\'er-Rao acota por debajo esta
covarianza.
%
\begin{teorema}[Desigualdad de Cram\'er-Rao]
\label{Teo:SZ:CramerRao}
%
  Sea \ $X$  \ parametrizada por \ $\theta$, \modif{que  admite una densidad con
  respecto  a una  medida $\mu$  dada independiente  de $\theta$}  de soporte  \
  $\X \subset \Rset^d$  \ independiente de \  $\theta$\modif{, diferenciable con
  respecto  a \  $\theta$}, \  y  \ $\widehat{\theta}(X)$  \ un  estimador de  \
  $\theta$   \modif{(funci\'on   determinista   solamete  de   $X$)}.    Sea   \
  $b(\theta)$  \  su  sesgo  y  \ $\Sigma_{\widehat{\theta}}$  \  su  matriz  de
  covarianza.  Sea  \ $\Jac_b(\theta)$ \  la matriz  Jacobiana del sesgo  \ $b$.
  Entonces,
  %
  \[
  \Sigma_{\widehat{\theta}} - \left( I + \Jac_b(\theta) \right) J_\theta(X)^{-1}
  \left( I + \Jac_b(\theta) \right)^t \ge 0.
  \]
  %
  En particular, en el caso \ $\theta$ \ escalar,
  %
  \[
  \sigma_{\widehat{\theta}}^2 \ge \frac{(1+b'(\theta))^2}{J_\theta(X)},
  \]
  %
  donde  \  $b'$ \  es  la  derivada de  \  $b$.\newline  Tomando  \ $\theta$  \
  par\'ametro de posici\'on y \  $\widehat{\theta} = X$, estimador sin sesgo ($b
  =  0$),  da  lo que  es  conocido  como  la  desigualdad no  param\'etrica  de
  Cram\'er-Rao y toma la expresi\'on
  %
  \[
  \Sigma_X - J(X)^{-1} \ge 0,
  \]
  %
  o, en el caso escalar,
  %
  \[
  \sigma_X^2 \ge \frac{1}{J(X)}.
  \]
  %
  Adem\'as, en el caso no param\'etrico, se  alcanza la cota si y solamente si \
  $X$ \ es un vector gaussiano.
\end{teorema}
%
\noindent Esta desigualdad  acota la varianza de cualquier  estimador, \ie da la
varianza o error m\'inimo(a) que se puede esperar. Esta cota es el inverso de la
informaci\'on de Fisher, \ie \  $J_\theta(X)$ \ caracteriza la informaci\'on que
\ $X$ \ tiene sobre \ $\theta$.
%
\begin{proof}
  Sea  \   $S_\theta  =  \nabla_\theta  \log   p_X$  \  y  \   $\theta_0  =  \Esp\left[
    \widehat{\theta}(X) \right] = \theta +  b(\theta)$.  Fijandose que \ $p_X \,
  \nabla_\theta \log p_X = \nabla_\theta  p_X$, que \ $\widehat{\theta}$ \ no es
  funci\'on de \ $\theta$,  y que el soporte \ $\X$ \  no depende de \ $\theta$,
  se obtiene~\footnote{Se supone que  los integrandes sean \ $\theta$-localmente
    integrables,  tal  que  se  puede  invertir  derivada  en  \  $\theta$  \  e
    integraci\'on;    ver    tambi\'en    teorema~\ref{Teo:MP:IntegralParametro}.}
  % , pagina~\pageref{Teo:MP:IntegralParametro}.}
  %
  \begin{eqnarray*}
  \Esp\left[ S_\theta(X) \left( \widehat{\theta}(X) - \theta_0 \right)^t \right] & = &
  \int_\X \nabla_\theta p_X(x;\theta) \, \widehat{\theta}(x)^t \, d\mu(x) - \left(
  \int_\X \nabla_\theta p_X(x;\theta) \, d\mu(x) \right) \theta_0^t\\[2.5mm]
  %
  & = & \nabla_\theta \int_{\Rset^d} p_X(x;\theta) \, \widehat{\theta}(x)^t \, d\mu(x) -
  \left( \nabla_\theta \int_{\Rset^d} p_X(x;\theta) \, d\mu(x) \right)
  \theta_0^t\\[2.5mm]
  %
  & = & \nabla_\theta \left( \theta + b(\theta) \right)^t  - 
  \left( \nabla_\theta 1 \right) \theta_0^t\\[2.5mm]
  %
  & = & \left( I + \Jac_b(\theta) \right)^t
  \end{eqnarray*}
  %
  Adem\'as, fijandose  que \  $\Esp\left[ S_\theta(X) S_\theta(X)^t  \right] =  J_\theta(X)$ y
  $\Esp\left[   \left(    \widehat{\theta}(X)   -   \theta_0    \right)   \left(
      \widehat{\theta}(X)      -      \theta_0      \right)^t     \right]      =
  \Sigma_{\widehat{\theta}}$, la  desigualdad de Cauchy-Bunyakovsky-Schwarz (ver
  corolario~\ref{Cor:MP:CBS})
  %, pagina~\pageref{Cor:MP:CBS}) 
  conduce a
  %
  \[
  \left( u^t \left( I + \Jac_b(\theta)  \right)^t v \right)^2 \: = \: \Esp\left[
    u^t S_\theta(X) \left( \widehat{\theta}(X) -  \theta_0 \right)^t v \right]^2 \: \le
  \: u^t J_\theta(X) u \: v^t \Sigma_{\widehat{\theta}} \, v.
  \]
  %
  La prueba se termina tomando \ $u = J_\theta(X)^{-1} \left( I + \Jac_b(\theta)
  \right)^t \,  v$ \ (recordandose  que \ $J_\theta$ \  es sim\'etrica).\newline
  Para  $\theta$  parametro  de  posici\'on,  $\widehat{\Theta}  =  X$,  con  la
  elecci\'on  de \  $u$,  en la  desigualdad  de Cauchy-Bunyakovsky-Schwarz,  se
  obtiene la igualdad  cuando \ $v^t J(X)^{-1} S_\theta(x) \propto v^t  (x - \theta)$ \
  para cualquier \ $v$  \ y \ $x$ (c.s.), est decir  \ $\nabla_x p_X (x) \propto
  J(X) (x -  \theta) p_X(x)$, lo que es la  ecuaci\'on diferencial que satisface
  (solamente) la gaussiana: en este caso, se verifica a posteriori que \ $J(X) =
  \Sigma_X^{-1}$,  y  entonces que  se  alcanza la  cota  de  la desigualdad  de
  Cram\'er-Rao no param\'etrica.
\end{proof}
%
\noindent En el caso param\'etrico, no se puede estudiar el caso de igualdad del
hecho  que \  $\widehat{\Theta}$ \  no es  algo dado.   Adem\'as, a\'un  dado un
estimador (expl\'icitamente independiente de $\theta$), no hay garant\'ia de que
existe una  densidad parametrizada por  \ $\theta$ \  que alcanza la cota,  o al
rev\'es, dada  una familia de densidades,  tampoco hay garant\'ia  que existe un
estimador que permite alcanzar la cota~\cite{CovTho06, Kay93, Bos07}.

Fijense  de  que,  nuevamente,  la  gaussiana  juega un  rol  particular  en  la
desigualdad de Cram\'er-Rao no param\'etrica, permitiendo de alcanzar la cota.

Nota: para  dos matrices \  $A \ge 0$  \ y \  $B \ge 0$,  si \ $A  - B \ge  0$ \
entonces $|A| \ge  |B|$, con igualdad si y solamente si  \ $A = B$~\cite[cap.~1,
teorema~25]{MagNeu99}.   Entonces,  de  las  desigualdades  de  Cram\'er-Rao  se
deducen desigualdades de Cram\'er-Rao escalares
%
\[
\left|   \Sigma_{\widehat{\theta}}  \right|   \,   \ge  \,   \frac{\left|  I   +
    \Jac_b(\theta) \right|^2}{\left| J_\theta(X) \right|} \qquad \mbox{y} \qquad
\left| \Sigma_X \right| \, \ge \, \frac{1}{\left| J(X) \right|}.
\]
%
Obviamente, en la segunda,  se alcanza la igualdad si y solamente  si \ $X$ \ es
gaussiano.   Adem\'as, para  una  matriz \  $A  \ge 0$,  existe la  ``relaci\'on
determinente-traza''  \ $|A|^{\frac1d} \le  \frac1d \Tr(A)$,  con igualdad  si y
solamente si  \ $A = I$~\cite[cap.~11, sec.~4]{MagNeu99},  dando otras versiones
escalares de  la desigualdad  de Cram\'er-Rao, por  ejemplo, de la  versi\'on no
param\'etrica,
%
\[
\left| \Sigma_X  \right|^{\frac1d} \,  \ge \, \frac{d}{\Tr\left(  J(X) \right)},
\qquad   \Tr\left(   \Sigma_X   \right)   \,   \ge   \,   \frac{d}{\left|   J(X)
  \right|^{\frac1d}} \qquad \mbox{o} \qquad \Tr\left( \Sigma_X \right) \, \ge \,
\frac{d^2}{\Tr\left( J(X) \right)}.
\]
%
En estos casos,  se obtiene la igualdad si  y solamente si \ $X$  \ es gaussiano
(igualdad  de  la  Cram\'er-Rao  no  param\'etrica)  y  adem\'as  de  covarianza
proporcional a la identidad (igualdad en la relaci\'on determinente-traza).


Se  recordar\'a que,  tratando  de una  secuencia  $X =  \{  X_i \}_{i=1}^n$  de
vectores   aleatorias  independientes   parametrizados  por   $\theta$,  tenemos
$J_\theta(X) = n  J_\theta(X_i)$. Eso significa que estimando  $\theta$ a partir
de la  secuencia, baja de un  factor $\frac{1}{n}$ la cota  de Cram\'er-Rao.  Se
referir\'a  a~\cite{Fis25:07, Sta59,  Kay93, KagSmi99,  Joh04, CovTho06,  Rio07}
entre otros para estas propiedades.


\

Para cerra esta secci\'on, notar que en t\'ermino de estimaci\'on,
%,  al imagen  de las  leyes de  entrop\'ia
%m\'axima,
la informaci\'on  de Fisher
%(sin poner v\'inculos)
juega tambi\'en  un rol  particular en  la inferencia  bayesiana a  trav\'es del
prior  de Jeffrey~\cite{Jef46,  Jef48,  LehCas98,  Rob07}~\footnote{
%~\ref{Foot:SZ:Prior}.
Ver notas de pie~\ref{Foot:MP:BayesPrior}
  y~\ref{Foot:MP:BayesPriorConjugado}.   A  veces, se  toma  como  distribuci\'on a  priori  \
$p_\Theta(\theta)  \propto  |J_\theta(X)|^\frac12$  \   por  su  invarianza  por
reparametrizaci\'on  \ $\eta  =  \eta(\theta)$, \ie  el prior  de  Jeffrey en  \
$\eta$ \ es un\'ivocamente obtenido con la Fisher  en \ $\eta$ \ o por cambio de
variables saliendo de \ $p_\Theta$.}. \SZ{Dejar ac\'a?}



% ================================= Curvatura

\subseccion{Fisher como curvatura de la entrop\'ia relativa}
\label{Ssec:SZ:FisherCurvatura}


Si  la  desigualdad de  Cram\'er-Rao  da  a la  matriz  de  Fisher  un sabor  de
informaci\'on,  aparece  que  \  $J_\theta$  \ es  tambi\'en  relacionada  a  la
entrop\'ia relativa~\cite{CovTho06, Fri04}:
%
\begin{teorema}[Fisher como curvatura de la entrop\'ia relativa]
\label{Teo:SZ:FisherCurvatura}
%
  Sea  \  $X$   \  parametrizado  por  \  \modif{$\theta  \in   \Theta$  \  y  \
  $\theta_0 \in \mathring{\Theta}$ \ interior de  \ $\Theta$ \ dado} ($\Theta$ \
  contiene   un    vecinaje   de   \   $\theta_0$).     \modif{Suponga   que   \
  $P_X(\cdot;\theta)  \ll P_X(\cdot;\theta_0)$  \ sobre  \  $\Theta$ \  y sea  \
  $p_X(\cdot  ; \theta)$  \ densidad  de \  $X$ \  con respecto  a una  medida \
  $\mu$  \ dada  independiente de  \ $\theta$  \  y suponga  que \  $p_X$ \  sea
  diferenciable con  respecto a \  $\theta$ \ en  un vecinaje de  \ $\theta_0$}.
  Siendo  \  $\Dkl[p_X(\cdot;\theta)]{p_X(\cdot;\theta_0)}$  \  funci\'on  de  \
  $\theta \in \Theta$, aparece que
  %
  \[
  \Dkl[p_X(\cdot;\theta)]{p_X(\cdot;\theta_0)}   =  \frac12   \left(   \theta  -
      \theta_0  \right)^t J_{\theta_0}(X)  \left(  \theta -  \theta_0 \right)  +
    o\left( \| \theta - \theta_0 \|\right),
  \]
  %
  donde \  $o(\cdot)$ \ es un resto  peque\~no con respecto a  su argumento.  En
  otros  t\'erminos,  $J_{\theta_0}(X)$  \  es  la curvatura  de  la  entrop\'ia
  relativa en \ $\theta_0$.
\end{teorema}
%
\begin{proof}
  La relaci\'on  es consecuencia  de un desarrollo  de Taylor  al orden 2  de la
  funci\'on \  $\Dkl[p_X(\cdot;\theta)]{p_X(\cdot;\theta_0)}$ \ de  \ $\theta$,
  tomada en \  $\theta = \theta_0$. Por propiedad de  \ $\Dkl{}$, la divergencia
  es positiva  y se cancela cuando  \ $\theta = \theta_0$.   Entonces, el primer
  t\'ermino del desarrollo vale cero y el segundo tambi\'en, \ $\Dkl{}$ \ siendo
  m\'inima en \ $\theta = \theta_0$. Adem\'as,
  %
  \begin{eqnarray*}
  \nabla_\theta \Dkl[p_X(\cdot;\theta)]{p_X(\cdot;\theta_0)} & =
  & \nabla_\theta \int_\X p_X(x;\theta) \log \left(
  \frac{p_X(x;\theta)}{p_X(x;\theta_0)} \right) d\mu(x)\\[2.5mm]
  %
  & = & \int_\X \nabla_\theta p_X(x;\theta) \log \left(
  \frac{p_X(x;\theta)}{p_X(x;\theta_0)} \right) d\mu(x) + \int_\X \nabla_\theta
  p_X(x;\theta) \, d\mu(x)\\[2.5mm]
  %
  & = & \int_\X \nabla_\theta p_X(x;\theta) \log \left(
  \frac{p_X(x;\theta)}{p_X(x;\theta_0)} \right) d\mu(x) + \nabla_\theta \int_\X
  p_X(x;\theta) \, d\mu(x)\\[2.5mm]
  %
  & = & \int_\X \nabla_\theta p_X(x;\theta) \log \left(
  \frac{p_X(x;\theta)}{p_X(x;\theta_0)} \right) d\mu(x)
  \end{eqnarray*}
  %
  la \'ultima ecuaci\'on como consecuencia de que  \ $p_X$ \ suma a 1. \modif{Se
  confirma   de  esta   forma   que   el  gradiente   se   cancela  en   $\theta
  = \theta_0$. Luego},
  %
  \[
  \Hess_\theta     \Dkl[p_X(\cdot;\theta)]{p_X(\cdot;\theta_0)}     =    \int_\X
  \Hess_\theta  p_X(x;\theta) \log  \left( \frac{p_X(x;\theta)}{p_X(x;\theta_0)}
  \right)  d\mu(x)  + \int_\X  \frac{\nabla_\theta  p_X(x;\theta) \,  \nabla^t_\theta
    p_X(x;\theta)}{p_X(x;\theta)} \, d\mu(x).
  \]
  %
  Tomado en \ $\theta = \theta_0$  el primer t\'ermino vale cero.  En el segundo
  se reconoce \ $J_\theta(X)$, lo que termina la prueba.
\end{proof}
%
Este  teorema, ilustrado  en la  figura  Fig.~\ref{Fig:SZ:JCurvatura}, relaciona
claramente  dos objectos  viniendo de  la teor\'ia  de la  estimaci\'on y  de la
teor\'ia de la informaci\'on, mundos a  priori diferentes.  Como se lo puede ver
en  la figura,  cuando \  $J_\theta(X)$ \  tiene peque\~nos  autovalores (figura
(a)), $p_\theta$  \ se \ ``aleja'' \  lentamente de \ $p_{\theta_0}$  \ cuando \
$\theta$  \  se aleja  de  \  $\theta_0$: hay  una  alta  incerteza o  peque\~na
informaci\'on sobre \ $\theta_0$. Y vice-versa (figura (b)).
%
\begin{figure}[h!]
%
\begin{center} \input{TIKZ_SZ/JCurvatura} \end{center}
%
\leyenda{Ilustraci\'on         del        comportamiento         local        de
  $\Dkl[p_X(\cdot;\theta)]{p_X(\cdot;\theta_0)}$ \ en  funci\'on de \ $\theta$ \
  en \ $\theta_0$ \ en el contexto escalar \ $\Theta \subset \Rset$.  (a) Caso
  con \ $J_{\theta_0}(X)$  ``peque\~no'' \ y (b) caso  con \ $J_{\theta_0}(X)$ \
  ``grande''.   En  el caso  (b),  la determinaci\'on  de  \  $\theta$ \  usando
  $\Dkl{}$ \ va a  ser m\'as ``sencillo'' que en el caso  (a) porque el m\'inimo
  es m\'as ``picado''.}
%
\label{Fig:SZ:JCurvatura}
\end{figure}


% ================================= de Bruijn

\subseccion{Identidad de de Bruijn}
\label{Ssec:SZ:DeBruijn}


Un otro  v\'inculo entre el  mundo de la  informaci\'on y el de  la estimaci\'on
aparece a trav\'es de la identidad  de de Bruijn~\footnote{A pesar de que tom\'o
  este nombre, esta identidad en su primera versi\'on fue publicada por Stam. En
  su papel~\cite{Sta59}, menciona que  esta identidad fue comunicada al Profesor
  van  Soest por el  Profesor de  Bruijn.}~\cite{Sta59, CovTho06,  Joh04, Bar84,
  Bar86,  PalVer06, TorZoz18}.  Esta  identidad caracteriza  lo que  es conocido
como  canal gaussiano  de la  figura Fig~\ref{Fig:SZ:deBruijnVerdu}-(a),  \ie la
salida \ $Y$  \ es una versi\'on ruidosa de la  entrada.  La identidad v\'incula
las variaciones  de entrop\'ia  de salida con  respecto al  nivel de ruido,  y la
informaci\'on de Fisher.

\begin{figure}[h!]
%
\begin{center} \input{TIKZ_SZ/CanalDeBruijnVerdu} \end{center}
%
\leyenda{Canal  de  comunicaci\'on  gaussiano  de  entrada  \  $X$.   (a)  Canal
  gaussiano usual, donde \ $T$ \ maneja los par\'ametros (nivel) del ruido.  (b)
  canal gaussiano con un preprocesamiento \ $S$ \ de la entrada.}
%
\label{Fig:SZ:deBruijnVerdu}
\end{figure}

\begin{teorema}[Identidad de de Bruijn]
\label{Teo:SZ:DeBruijn}
%
  Sea \ $X$ \  un vector aleatorio continuo sobre un abierto de  \ $\Rset^d$ \ y
  admitiendo una matriz de covarianza, y sea \ $Y = X + T \N$ \ donde \ $T$ \ es
  determinista, $d \times d'$ \ con \ $ d \le d'$, de rango m\'aximo, y \ $\N$ \
  un vector gaussiano centrado y de covarianza \ $\Sigma_\N$, independiente de \
  $X$   \  (ver   figura  Fig.~\ref{Fig:SZ:deBruijnVerdu}-(a)).    Entonces,  la
  entrop\'ia \modif{(diferencial)}  de Shannon y la  \modif{matriz informaci\'on
  no param\'etrica} de Fisher de \ $Y$ \ satisfacen
  %
  \[
  \nabla_T H(Y) = J(Y) \, T \, \Sigma_\N,
  \]
  %
  donde \ $\nabla_T \, \cdot$ \ es la matriz de componentes \ $\frac{\partial \,
    \cdot}{\partial T_{i,j}}$.  Si \ $T = T(\theta)$ \ depende de un par\'ametro
  escalar~\footnote{Si el  par\'ametro es  multivariado, hace falta  entender la
    desigualdad a trav\'es de derivas parciales con respecto a los componentes de
    $\theta$.}  $\theta$,
  %
  \[
  \frac{\partial}{\partial \theta}  H(Y) = \Tr\left( J(Y) \,  T \, \Sigma_\N
    \, \frac{\partial T^t}{\partial \theta} \right).
  \]
\end{teorema}
%
\begin{proof}
  La clave de este resultado viene del hecho que la densidad \ $p$ \ de \ $T \N$
  \ satisface una ecuaci\'on diferencial  particular.  La distribuci\'on de \ $T
  \N$  \ se  escribe \  $p(x) =  (2 \pi)^{-\frac{d}{2}}  \left| T  \Sigma_\N T^t
  \right|^{-\frac12}   \exp\left(  -   \frac12  x^t   \left(  T   \Sigma_\N  T^t
    \right)^{-1} x  \right)$ \ (el rango  m\'aximo de \  $T$ \ asegura que  \ $T
  \Sigma_\N  T^t$   \  sea  invertible).    Para  una  matriz   invertible  $R$,
  desarollando \  $|R|$ \  con respecto  a su l\'inea  \ $i$,  se obtiene  que \
  $\frac{\partial |R|}{\partial R_{i,j}} = R_{i,j}^*$ \ cofactor de \ $R_{i,j}$,
  dando  por la  regla de  Cram\'er \  $\nabla_R  |R| =  |R| \,  R^{-t}$ \  (ver
  tambi\'en~\cite[cap.~1~\&~9]{MagNeu99}), es decir \ $\nabla_R |R|^{-\frac12} =
  -\frac12 |R|^{-\frac12}  R^{-t}$.  De $\frac{\partial |R|^{-\frac12}}{\partial
    T_{i,j}}  =   \sum_{k,l}  \frac{\partial  |R|^{-\frac12}}{\partial  R_{k,l}}
  \frac{\partial R_{k,l}}{\partial T_{i,j}} = -\frac12 |R|^{-\frac12} \sum_{k,l}
  \left( R^{-1} \right)_{l,k} \frac{\partial R_{k,l}}{\partial T_{i,j}}$ \ con \
  $R  = T  \Sigma_\N  T^t$ \  (sim\'etrica)  y c\'alculos  b\'asicos se  obtiene
  finalmente
  %
  \[
  \nabla_T  \left|   T  \Sigma_\N  T^t  \right|^{-\frac12}  =   -  \left|  T
    \Sigma_\N T^t \right|^{-\frac12} \left( T \Sigma_\N T^t \right)^{-1}
  T \Sigma_\N.
  \]
  %
  Adem\'as, de \ $\left( T  \Sigma_\N T^t \right) \left( T \Sigma_\N T^t
  \right)^{-1}  =  I$ \  viene  \  $\frac{\partial  \left( T  \Sigma_\N  T^t
    \right)^{-1}}{\partial T_{i,j}} = -  \left( T \Sigma_\N T^t \right)^{-1}
  \frac{\partial \left( T \Sigma_\N  T^t \right)}{\partial T_{i,j}} \left( T
    \Sigma_\N T^t \right)^{-1}$.  Usando le vector \ $\uno_i$ \ con 1 en su \
  $i$-\'esima componente, y cero si no (ver notaciones), se obtiene
  %
  \begin{eqnarray*}
  \frac{\partial \left( x^t \left( T \Sigma_\N T^t \right)^{-1} x
  \right)}{\partial T_{i,j}} & = & - x^t \left( T \Sigma_\N T^t \right)^{-1}
  \left( \uno_i \uno_j^t \Sigma_\N T^t + T \Sigma_\N \uno_j \uno_i^t \right) \left( T
  \Sigma_\N T^t \right)^{-1} x\\[2.5mm]
  %
  & = & - 2 \, \uno_i^t \left( T \Sigma_\N T^t \right)^{-1} x x^t \left( T
  \Sigma_\N T^t \right)^{-1} T \Sigma_\N \uno_j
  \end{eqnarray*}
  %
  usando la relaci\'on  \ $x^t A \uno_k \uno_l^t B  x = \uno_l^t B x  x^t A \uno_k =
  \uno_k^t A^t x x^t  B^t \uno_l$ \ (escalares comutan y un  escalar es igual a su
  traspuesta) y usando la simetr\'ia de \ $T \Sigma_\N T^t$.  Eso significa
  que
  %
  \[
  \nabla_T \left( x^t \left( T \Sigma_\N T^t \right)^{-1} x \right) = - 2 \,
  \left(  T \Sigma_\N  T^t \right)^{-1}  x  x^t \left(  T \Sigma_\N  T^t
  \right)^{-1} T \Sigma_\N,
  \]
  %
  dando
  %
  \[
  \nabla_T p(x)  = \left( - \left(  T \Sigma_\N T^t \right)^{-1}  + \left( T
      \Sigma_\N  T^t   \right)^{-1}  x   x^t  \left(  T   \Sigma_\N  T^t
    \right)^{-1} \right) T \Sigma_\N \, p(x).
  \]
  %
  Tomando la Hessiana de \ $p$ \  con respecto a \ $x$ \ se obtiene sencillamente
  que \ $p$ \ satisface la ecuaci\'on diferencial
  %
  \[
  \nabla_T \, p = \Hess_x \, p \: T \, \Sigma_\N.
  \]
  %
  Suponiende que  se puede intervertir derivadas  y integrales (ver~\cite{Bar84,
    Bar86}     donde     se      dan     condiciones     rigurosas,     y     el
  teorema~\ref{Teo:MP:IntegralParametro},
  %pagina~\pageref{Teo:MP:IntegralParametro}),     
  $\displaystyle  p_Y(y)  =  \int_{\Rset^d}  p_X(x)  \, p(y-x)  \,  dx$  \  (ver
  ejemplo~\ref{Ej:MP:Suma})
  %,  pagina~\pageref{Ej:MP:Suma})
  \ satisface tambi\'en esta ecuaci\'on diferencial, y adem\'as
  %
  \begin{eqnarray*}
  \nabla_T H(Y) & = & - \int_{\Rset^d} \nabla_T \, p_Y(y) \log p_Y(y)
  \, dy - \int_{\Rset^d} \nabla_T \, p_Y(y) \, dy\\[2.5mm]
  %
  & = & - \left( \int_{\Rset^d} \Hess_y \, p_Y(y) \log p_Y(y) \, dy \right) T \,
  \Sigma_\N - \nabla_T \int_{\Rset^d} p_Y(y) \, dy\\[2.5mm]
  %
  & = & - \left( \int_{\Rset^d} \left( \Hess_y \Big( p_Y(y) \log p_Y(y) \Big) -
  \Hess_y \, p_Y(y) - \frac{\nabla_y \, p_Y(y) \, \nabla_y \, p_Y(y)^t}{p_Y(y)}
  \right) \, dy \right) T \, \Sigma_\N\\[2.5mm]
  %
  & = & - \left( \int_{\Rset^d} \Hess_y \Big( p_Y(y) \log p_Y(y) \Big) \, dy -
  \int_{\Rset^d} \Hess_y p_Y(y) \, dy \right) T \, \Sigma_\N \: + \: J(Y) \, T
  \, \Sigma_\N
  \end{eqnarray*}
  %
  usando la ecuaci\'on diferencial en la segunda l\'inea, el hecho que \ $p_Y$ \
  suma  a  1 en  la  tercera  l\'inea (su  gradiente  es  cero  entonces), y  la
  definici\'on de la matriz de Fisher  en la \'ultima l\'inea. Usando el teorema
  de la  divergencia (intergraci\'on por partes) aplicada  respectivamente a los
  componentes de \ $\nabla_y p_Y \log  p_Y$ \ y \ $\nabla_y p_Y$, suponiendo que
  estos gradientes se cancelan en el borde del dominio de integraci\'on, los dos
  t\'erminos integrales  valen cero, lo que  cierra la prueba  de la desigualdad
  general.  Adem\'as,  si \ $T =  T(\theta)$, la segunda desigualdad  sigue de \
  $\frac{\partial   \cdot}{\partial    \theta}   =   \sum_{i,j}   \frac{\partial
    \cdot}{\partial   T_{i,j}}   \frac{\partial   T_{i,j}}{\partial  \theta}   =
  \Tr\left( \nabla_T \cdot \, \frac{\partial T^t}{\partial \theta} \right)$.
\end{proof}

La versi\'on  inicial de la identidad de  de Bruijn, con \  $\Sigma_\N = I$,
que se escribe
%
\[
\frac{d}{d\theta}     H(X+\sqrt{\theta}    \N)    =     \frac12    \Tr\left(
  J(X+\sqrt{\theta} \N) \right),
\]
%
se recupera  en el caso particular  \ $T =  \sqrt{\theta} I$.  En este  caso, la
ecuac\'ion diferencial satisfecha por la densidad  de probabilidad \ $p$ \ es la
{\it  ecuaci\'on del  calor}.  Esta  desigualdad cuantifica  las  variaciones de
entrop\'ias   bajo  variaciones   de  ``niveles''   del  ruido   del   canal  de
comunicaci\'on. De una  forma, caracteriza la robustez del  canal con respecto al
nivel de ruido gaussiano (la gaussiana juega de nuevo un rol central ac\'a).

\

Existe una  otra forma  muy similar  de esta desigualdad  debido a  Guo, Shamai,
Verd\'u,  Palomar~\cite{GuoSha05, PalVer06,  TorZoz18}.  Esta  versi\'on vincula
a\'un m\'as el mundo  de la informaci\'on y el de la  estimaci\'on.  Del lado de
la  comunicaci\'on, consiste  a  caracterizar la  informaci\'on  mutua entre  la
entrada \ $X$ \ de un canal ruidoso y su  salida, \ $Y = S X + \N$ \ donde \
$S$ \ coresponde a un pre-tratamiento antes de la salida. Eso es ilustrado en la
figura  Fig.~\ref{Fig:SZ:deBruijnVerdu}-(b).  Del lado  de la  estimaci\'on, uno
puede querer  estimar \ $X$  \ observando solamente  \ $Y$.  Es conocido  que el
estimador  que minimiza  el error  cuadr\'atico promedio  \  $\Esp\left[ \left|
    \widehat{X}(Y)  - X  \right|^2 \right]$  \  es la  esperanza condicional  \
$\widehat{X}(Y)   =    \Esp[X|Y]$~\cite{Kay93,   Rob07,   LehCas98}.     \   Una
caracter\'istica de un  estimador siendo su matriz de  covarianza, se notar\'a \
$\E(X|Y)  =  \Esp\left[  \left( X  -  \Esp[X|Y]  \right)  \left( X  -  \Esp[X|Y]
  \right)^t  \right]$ \  esta  matriz.  Sorpredentemente,  existe tambi\'en  una
identidad entre \ $I(X;Y)$ \ y \ $\E(X|Y)$:
%
\begin{teorema}[Identidad de Guo--Shamai--Verd\'u]
\label{Teo:SZ:GuoShamaiVerdu}
%
  Sea \ $X$ \ un vector aleatorio  continuo sobre un abierto de \ $\Rset^{d'}$ \
  y admitiendo una  matriz de covarianza, y sea \  $Y = S X +  \N$ \ donde \
  $S$  es determinista,  $d  \times d'$,  y  \ $\N$  \  un vector  gaussiano
  centrado  y de covarianza  $\Sigma_\N$, independiente  de $X$  (ver figura
  Fig.~\ref{Fig:SZ:deBruijnVerdu}-(b)). Entonces, la informaci\'on mutua entre \
  $X$ \ e \ $Y$ \ y  la matriz de covarianza del estimador de error cuadr\'atico
  m\'inimo satisfacen
  %
  \[
  \nabla_S \, I(X;Y) = \Sigma_\N^{-1} S \, \E(X|Y).
  \]
  %
  Si \ $S = S(\snr)$ \ depende de un par\'ametro escalar $\snr$,
  %
  \[
  \frac{\partial}{\partial \snr} I(X;Y) = \Tr\left( \Sigma_\N^{-1} \, S \,
    \E(X|Y) \, \frac{\partial S^t}{\partial \snr} \right).
  \]
\end{teorema}
%
\begin{proof}
  Notando  que \ $p_{Y|X=x}  (y) =  (2 \pi)^{-\frac{d}{2}}  \left| \Sigma_\N
  \right|^{-\frac12}  \exp\left( -\frac12  (y-S  x)^t \Sigma_\N^{-1}  (y-Sx)
  \right)$   \   viene  \   $\nabla_S   \,   p_{Y|X=x}(y)   =  p_{Y|X=x}(y)   \,
  \Sigma_\N^{-1}  (y -  S x)  x^t$ \  (ver  unos pasos  de la  prueba de  la
  identidad de de  Bruijn) as\'i que \ $\nabla_y  \, p_{Y|X=x}(y) = p_{Y|X=x}(y)
  \, \Sigma_\N^{-1} (y - S x)$, dando
  %
  \[
  \nabla_S \, p_{Y|X=x}(y) = \nabla_y \, p_{Y|X=x}(y) x^t \qquad \mbox{y} \qquad
  \nabla_S \, p_{X,Y}(x,y) = \nabla_y \, p_{X,Y}(x,y) x^t
  \]
  %
  (multiplicando ambos lados por $p_X$). Ahora, \ $I(X;Y) = H(Y) - H(Y|X) = H(Y)
  - H(\N)$ \ (de la independencia, cuando \ $X  = x$, \ $Y = S x + \N$ \
  gaussiana de  misma convarianza que \  $\N$ \ y  de promedio \ $S  x$ (ver
  ejemplo~\ref{Ej:MP:SumaCond},
  % pagina~\pageref{Ej:MP:SumaCond}), 
  as\'i que
  %
  \begin{eqnarray*}
  \nabla_S I(X;Y) & = & \nabla_S H(Y)\\[2.5mm]
  %
  & = & - \int_{\Rset^d \times \Rset^{d'}} \nabla_S \Big( p_{X,Y}(x,y) \, \log
  p_Y(y) \Big) \, d\mu(x,y)\\[2.5mm]
  %
  & = & - \int_{\Rset^d \times \Rset^{d'}} \nabla_S \, p_{X,Y}(x,y) \, \log p_Y(y) \,
  d\mu(x,y) - \int_{\Rset^d \times \Rset^{d'}} p_{X|Y=y}(x) \, \nabla_S \, p_Y(y) \, d\mu(x,y)\\[2.5mm]
  %
  & = & \int_{\Rset^d \times \Rset^{d'}} \nabla_y \, p_{X,Y}(x,y) \, x^t \log p_Y(y)
  \, d\mu(x,y) - \int_{\Rset^d} \nabla_S \, p_Y(y) \, d\mu(y)\\[2.5mm]
  %
  & = & - \int_{\Rset^d \times \Rset^{d'}} \nabla_y \, p_Y(y) x^t p_{X|Y=y}(x) \, d\mu(x,y)\\[2.5mm]
  %
  & = & - \int_{\Rset^d} \nabla_y \, p_Y(y) \, \Esp\left[X^t | Y = y \right] \, d\mu(y)
  \end{eqnarray*}
  %
  La  segunda l\'inea  viene  de la  escritura  de \  $H(Y)$  usando $p_Y$  como
  marginale de $p_{X,Y}$ en $x$ y intercambiando gradiente e integral (ver pasos
  de   la   prueba  de   la   desigualdad  de   de   Bruijn);   la  tercera   de
  $\frac{p_{X,Y}(x,y)}{p_Y(y)}  =   p_{X|Y=y}(x)$;  en  la  cuarta   se  usa  la
  ecuaci\'on  diferencial satisfecha  por  $p_{X,Y}$ en  la  primera integral  y
  integrando en $x$ en la segunda  integral; la quinta l\'inea se obtiene usando
  el teorema de  la divergencia (intergraci\'on por partes)  en la integraci\'on
  en $y$  de la primera  integral, e intercambiando  gradiente e integral  el la
  segunda ($p_Y$ sumando a 1, el t\'ermino se cancela). Adem\'as,
  %
  \begin{eqnarray*}
  \nabla_y p_Y(y) & = & \int_{\Rset^{d'}} \nabla_y \, p_{Y|X=x}(y) \, p_X(x) \,
  d\mu(x)\\[2.5mm]
  %
  & = & - \Sigma_\N^{-1} \int_{\Rset^{d'}} (y - S x) \, p_{Y|X=x}(y) \, p_X(x) \,
  d\mu(x)\\[2.5mm]
  %
  & = & - \Sigma_\N^{-1} \left( y - S \int_{\Rset^{d'}} x \, p_{X|Y=y}(x) \,d\mu(x)
  \right) p_Y(y)\\[2.5mm]
  %
  & = & - \Sigma_\N^{-1} \Big( y - S \Esp\left[X | Y=y \right] \Big) \, p_Y(y)
  \end{eqnarray*}
  %
  escribiendo $p_{Y|X=x}(y)  \, p_X(x) =  p_{X|Y=y}(x) \, p_Y(y)$ en  la tercera
  l\'inea. Esta ecuaci\'on permite escribir
  %
  \begin{eqnarray*}
  \nabla_S I(X;Y) & = & \Sigma_\N^{-1} \int_{\Rset^d} \Big( y - S \Esp\left[X |
  Y=y \right] \Big) \Esp\left[X^t | Y=y \right] \, p_Y(y) \, d\mu(y)\\[2.5mm]
  %
  & = & \Sigma_\N^{-1} \left( \Esp\left[ Y \Esp\left[ X^t|Y \right] \right] - S
  \Esp\left[ \Esp\left[X | Y \right] \Esp\left[X | Y \right]^t \right]
  \right)\\[2.5mm]
  %
  & = & \Sigma_\N^{-1} \left( \Esp\left[ Y X^t \right] - S \Esp\left[
  \Esp\left[X | Y \right] \Esp\left[X | Y \right]^t \right] \right)\\[2.5mm]
  %
  & = & \Sigma_\N^{-1} S \left( \Esp\left[ X X^t \right] - \Esp\left[
  \Esp\left[X | Y \right] \Esp\left[X | Y \right]^t \right] \right)
  \end{eqnarray*}
  %
  la \'ultima l\'inea viniendo de $Y  = S X + \N$ con $\N$ independiente
  de  $X$ y  de  promedio 0.   La prueba  se  cierra notando  que \  $\Esp\left[
    \Esp[X|Y] \right]  = \Esp[X]$  \ y por  la f\'ormula de  K\"onig-Huyggens (ver
  cap\'itulo~\ref{cap:MP:TeoriaProbabilidades},
  subsecci\'on~\ref{Ssec:MP:Momentos}).
  % , pagina~\pageref{Ssec:MP:Momentos}).

  La  segunda identidad  viene de  \ $\frac{\partial  \cdot}{\partial  \snr} =
  \Tr\left(  \nabla_S \,  \frac{\partial S^t}{\partial  \snr} \right)$  \ (ver
  prueba de la identidad de de Bruijn).
\end{proof}
%
\noindent  La  primera versi\'on  de  esta  identidad se  recupera  con  \ $S  =
\sqrt{\snr}$, \ $\Sigma_\N = I$ \ y \ $X$ \ de covarianza la identidad; $\snr$ \
es conocido como relaci\'on se\~nale/ruido en este caso.

Existen versiones a\'un m\'as completas  (con gradientes con respecto a la matriz
\  $\Sigma_\N$  \  por  ejemplo)  que se  pueden  consultar  en~\cite{Joh04,
  PalVer06, PayPal09}.



% ================================= Stam

\subseccion{Desigualdad de Stam}
\label{Ssec:SZ:Stam}


De la  desigualdad de  la potencia entr\'opica  y de  la identidad de  de Bruijn
surge  una otra  desigualdad implicando  la potencia  entr\'opica \  $N$ \  y la
informaci\'on de Fisher \ $J$.  Esta desigualdad es conocida como desigualdad de
Stam~\footnote{Como para  la identidad  de de Bruijn,  Stam mencion\'o  que esta
  desigualdad fue  comunicada al  Profesor van Soest  por el Profesor  de Bruijn
  quien  da una  prueba variacional  de la  desigualdad.}~\cite{CovTho06, Rio07,
  Sta59},     o    a    veces     ``desigualdad    isoperimetrica     para    la
entrop\'ia''~\cite{WanMad04}.
%
\begin{teorema}[Desigualdad de Stam]
\label{Teo:SZ:Stam}
%
  Sea  \  $X$   \  una  variable  aleatoria  continua   sobre  \  $\X  \subset
  \Rset^d$. Entonces,
  %
  \[
  N(X) \Tr\left( J(X) \right) \, \ge \, d,
  \]
  %
  con igualdad si y solamente si \ $X$ \ es gaussiano de covarianza proporcional
  a la identidad.
\end{teorema}
%
\begin{proof}
  De la desigualdad de la potencia entr\'opica se obtiene \ $N(X + \sqrt{\theta}
  \N)  \ge N(X)  + \theta  \left| \Sigma_\N  \right|^{\frac1d}$. Tomando
  $\Sigma_\N =  I$, se  obtiene \ $\forall  \, \theta  > 0,$ \  $\frac{N(X +
    \sqrt{\theta} \N) - N(X)}{\theta}  \ge 1$.  Entonces, tomando el l\'imite
  \ $\theta \to 0$, aparece  que \ $\left. \frac{d}{d\theta} N(X + \sqrt{\theta}
    \N)   \right|_{\theta  =   0}  \ge   1$.   La   prueba  se   cierra  con
  $\frac{d}{d\theta}  N(X   +  \sqrt{\theta}   \N)  =  \frac{1}{2   \pi  e}
  \frac{d}{d\theta}  \exp\left( \frac2d  H(X +  \sqrt{\theta} \N)  \right) =
  \frac2d  N(X +  \sqrt{\theta}  \N) \frac{d}{d\theta}  H(X +  \sqrt{\theta}
  \N) = d N(X +  \sqrt{\theta} \N) \Tr\left( J(X + \sqrt{\theta} \N)
  \right)$ \ (por la identidad de  de Bruijn).  Adem\'as, la igualdad se obtiene
  cuando se  alcanza la cota  de la desigualdad  de la potencia  entr\'opica, es
  decir cuando \ $X$ \ es gaussiano de varianza proporcional a la del ruido, que
  es la identidad en este caso.
\end{proof}
%
Se  puede  ver  de  nuevo  el  rol  central  que  juega  la  gaussiana  en  esta
desigualdad. Adem\'as, de la desigualdad  de Stam se puede deducir tamb\'ien las
versiones escalares de la desigualdad de Cram\'er-Rao. Viene del hecho que, dada
una matriz de covarianza, la entrop\'ia \ $H(X)$ \ es m\'axima cuando \ $X$ \ es
gaussiano.  Entonces, para cualquier \ $X$ \ de covarianza \ $\Sigma_X$, \ $N(X)
\le  \left| \Sigma_X  \right|^{\frac1d}$,  dando  de la  desiguldad  de Stam,  \
$\left| \Sigma_X \right|^{\frac1d} \Tr\left( J(X)  \right) \ge d$ \ (y las otras
versiones  escalares de  la relaci\'on  determinente-traza).  Como  se  lo puede
esperar, se  obtiene la igualdad si y  solamente \ $X$ \  es gaussiano (potencia
entr\'opica alcanzando su  cota superior) y de matriz  la identidad (desiguladad
de Stam se saturando).

Varias   otras  pruebas   de  la   desigualdad  de   Stam  pueden   provenir  de
generalizaciones~\cite{Ber12:06_1, Ber13, LutYan05, LutLv12, ZozPue17}.
%, por ejemplo debido a Lutwak o Bercher~\cite{Lut, Ber}.
  \SZ{La
  secci\'on ZZZ lo va a rapidamente evocar. Ver caso discreto Kagan~\cite{Kag01}.}
