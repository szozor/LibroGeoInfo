% MACROS
%
\def\A{\mathcal{A}}
\def\B{\mathcal{B}}
\def\D{\mathcal{D}}
\def\E{\mathcal{E}}
\def\H{\mathcal{H}}
\def\P{\mathcal{P}}
\def\X{\mathcal{X}}
%
\def\Cset{\mathbb{C}}
\def\Nset{\mathbb{N}}
\def\Rset{\mathbb{R}}
\def\Zset{\mathbb{Z}}
%
\def\Esp{\operatorname{E}}
\def\e{\operatorname{e}}
\def\Jac{\operatorname{J}}
\def\Hess{\mathcal{H}}
\def\Tr{\operatorname{Tr}}
\def\Gauss{\mathcal{N}}
%
\def\un{\mathbbm{1}}
\def\opt{\mathrm{opt}}
%
\def\ie{{\it i. e.,}\xspace}
%
\newcounter{propiedad}
\newcounter{PropPermutacion}
\newcounter{PropBiyeccion}
\newcounter{PropCotamaxima}
%
% propiedades generales
\newenvironment{propiedades}
{\begin{enumerate}[label={[P\arabic*]}]\setcounter{enumi}{\value{propiedad}}}
{\setcounter{propiedad}{\value{enumi}}\end{enumerate}}
%
% propiedades cambiadas en el caso continuo
\newenvironment{propiedadesC}
{\begin{enumerate}[label={[P'\arabic*]}]}
{\end{enumerate}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\capitulo{Nociones de teor\'ia de la informaci\'on}{Steeve Zozor}

% Epigrafe de capitulo
\begin{epigrafe}
  ``You should call it entropy, for two reasons.\\
  In the first place your  uncertainty function\\
  has been used  in statistical mechanics\\
  under that  name, so it already  has a name.\\
  In the  second place, and more  important,\\
  no one knows what entropy really is,\\
  so in a  debate you will always have  the advantage''
  %
  \autortituloepigrafe{von Neumann to Shannon~\cite{TriMcI71}}
\end{epigrafe}


% =============================== Introduccion =============================== %

\seccion{Introducci\'on}
\label{s:SZ:Introduccion}

La  noci\'on  de  informaci\'on encuentra  su  origen  con  el desarollo  de  la
comunicaci\'on  moderna,  por ejemplo  a  trav\'es  del  telegrafo siguiendo  el
patente de Moorse en 1840. La idea  de asignar un c\'odigo (punto o barra) a las
letras del alfabeto es la semilla de la codificaci\'on entropica, la que se basa
precisamente  sobre  la asignaci\'on  de  un codigo  a  simbolos  de una  fuente
(codificaci\'on  de   fuente)  seg\'un   las  frequencias  (o   probabilidad  de
aparici\'on) de cada simbolo en una cadena.  De hecho, el principio de codificar
un mensage  y mandar la  versi\'on codificada por  un canal de  transmisi\'on es
mucho mas antiguo, a pesar de que no habia ninguna formalizaci\'on matematica ni
siquiera explicitamente  una noci\'on de  informaci\'on.  Entre otros,  se puede
mencionar el  telegrafo optico de  Claude Chappe (1794), experimentos  con luces
por Guillaume Amontons  (en los a\~nos 1690 en Paris),  o a\'un mas antiguamente
la transmisi\'on de mensaje con antorchas en la Grecia antigua, con humo por los
indios o chiflando en la  prehistoria~\cite{Mon08}.  Cada forma es una instancia
practica  del esquema  de comunicaci\'on  de Shannon~\cite{Sha48,  ShaWea64}, es
decir la  codificaci\'on de  la informaci\'on, potencialmente  de manera  la mas
economica  que  se puede,  su  transmisi\'on a  un  ``receptor''  (por un  canal
ruidoso)  que  la  interpreta/lee/decodifica.   Implicitamente, la  noci\'on  de
informaci\'on a lo menos tan antigua que la humanidad.

A  pesar  de  que  la  idea  de codificar  y  transmitir  ``informaci\'on''  sea
tremendamente antigua, la formalizaci\'on matematica de la noci\'on de incerteza
o falta de informaci\'on, intimamente  vinculado a la noci\'on de informaci\'on,
naci\'o bajo el  impulso de C.  Shannon y la publicaci\'on  de su papel seminal,
``A  mathematical theory  of communication''  en 1948~\cite{Sha48},  o  un a\~no
depues  en su  libre re-titulado  ``The mathematical  theory  of communication''
reamplanzando el ``A'' por un ``The''. Desde esto a\~nos, las herramientas de la
dicha teoria de  la informaci\'on dio lugar a  muchas aplicaciones especialmente
en communicaci\'on (ver por  ejemplo~\cite[y ref.]{CovTho06, Ver98, Gal01}, pero
tambi\'en en  otros campos muy  diversos tal como {\color{red}\bf  completar con
  ref, Boltzman, von Neumann, Gibbs, Maxwell, etc. cf. GdR}.

{\color{red} La  meta de este cap\'itulo es  de describir las ideas  y los pasos
  dando lugar a la definicion de  la entropia, como medida de incerteza o (falta
  de)  informaci\'on.   En este  cap\'itulo,  se  empieza  con la  descripci\'on
  intuitiva que subtiende a la noci\'on de informaci\'on contenida en una cadena
  de simobolo, lo que condujo a  la definici\'on de la entropia. Esta defici\'on
  puede  ser deducida  tambi\'en  de  una seria  de  propiedades razonables  que
  deber\'ia cumplir  una medida de incerteza (enfoque  axiomatico).  Se continua
  con la descripci\'on  de tal noci\'on de entropia,  pasando del mundo discreto
  (simbolos,  alfabeta) al  mundo continuo,  lo que  no es  trivial  ni siquiera
  intuitivo.  Se  adelanto presentando  el concepto de  informaci\'on compartida
  entre dos sistemas o variables aleatorias, concepto fundamental en el marco de
  la transmisi\'on de informaci\'on o de mensajes. \bf Seguir. }

% ================================= Entropia ================================= %

\seccion{Entrop\'ia como medida de incerteza}
\label{s:SZ:Entropia}

% ================================= Axiomas

\subseccion{Entrop\'ia de Shannon, propiedades}
\label{ss:SZ:Axiomas}

Un de los primeros trabajos  tratando de formalizar la noci\'on de informaci\'on
de una cadena  de simbolos es debido a Raph  Hartley~\cite{Har28}.  En su papel,
Hartley defin\'o la informaci\'on de una secuencia como siendo proporcional a su
longitud. Mas precisamente,  para simbolos de un alfabeto  de cardinal $\alpha$,
exiten  $\alpha^n$   cadenas  diferentes  de   longitud  $n$;  Se   defin\'o  la
informaci\'on de  tales cadenas  como siendo $K  n$.  Para ser  consistente, dos
ensembles de mismo  tamanio $\alpha_1^{n_1} = \alpha_2^{n_2}$ deben  llegar a la
misma informaci\'on, as\'i que la informaci\'on  de Hatley es definida como $H =
\log\left( \alpha^n \right)$  donde la base del logaritmo  es arbitraria.  Dicho
de otra manera,  tomando un logaritmo de base 2, esta  informaci\'on es nada mas
que los  numeros de bits  (0-1) necesarios para  codificar todas las  cadenas de
longitud $n$  de simbolos de  un alfabeto de cardinal  $\alpha$.

{\color{red}\bf Hartley, equiv de Gibbs de la termostat}.

Una debilidad del  enfoque de Hartley es que considera  implicitamente que en un
mensage, cada cadena de longitud dada  puede aparecer con la misma frecuencia, o
probabilidad $1/\alpha^n$,  siendo la informaci\'on menos el  logaritmo de estas
probabilidades.  A contrario, parece  mas l\'ogico considerar que secuencias muy
frecuentes no  llevan mucha informaci\'on  (se sabe que aparecen),  mientras que
las  que aparencen  raramente llevan  mas informaci\'on  (hay mas  sorpresa, mas
incerteza  en observarlas).  Volviendo  a los  simbolos elementales  $x$, vistos
como aleatorios (o valores o estados que puede tomar una variable aleatoria), la
(falta  de)  informaci\'on o  incerteza  va a  ser  intimamente  vinculada a  la
probabilidad de aparici\'on de estos simbolos $x$. Siguiendo la idea de Hartley,
la informaci\'on elemental  asociado al estado $x$ va a ser  $- \log p(x)$ donde
$p(x)$  es la  probabilidad de  apararici\'on de  $x$.  Se  define  la incerteza
asociada a  la variable aleatoria como  el promedio estadistico  sobre todos los
estados  posibles $x$~\cite{Sha48, ShaWea64}~\footnote{En  la misma  \'epoca que
  Shannon,  independientemente,   la  noci\'on  de   informaci\'on  o  medidades
  equivalentes apareciendo por ejemplo en calculo de capacidad de canal aparecen
  en  varios  trabajo  como  el de  Calvier~\cite{Cla48},  Laplume~\cite{Lap48},
  Wiener~\cite[Cap.~III]{Wie48}  entre  varios  otros  (ver~\cite{Ver98,  Lun02,
    RioMag14, FlaRio16, RioFla17, Che17}).}.
%
% \cite{Sha48, ShaWea64}
\begin{definicion}[Entropia de Shannon]\label{def:SZ:Shannon}
  Sea $X$ una  variable aleatoria definida sobre una  alpfabeto discreto $\A$ de
  cardinal $\alpha = |\A| < +  \infty$ finito. Sea $p_X(x)$ la distribuci\'on de
  probabilidad de $X$, \ie $ \forall \, x \in \A, \quad p_X(x) = \Pr[X = x]$. La
  entropia de Shannon de la variable $X$ es definida por
  %
  \begin{equation}
    H(p_X) = H(X) = - \sum_{x \in \A} p_X(x) \, \log p_X(x)
  \end{equation}
  %
  con la convenci\'on $0 \log 0 = 0$.
\end{definicion}

La base del logaritmo es arbitrario; si  es $\log_2$ el logaritmo de base 2, $H$
es en bits  y si se usa el logaritmo  natural $\ln$, $H$ es en  nats.  En lo que
sigue, a\'un que,  rigorosamente, $H$ sea una funci\'on  de la distribuci\'on de
probabilidad $p_X$ y no de la  variable $X$, se usara indistamente la notaci\'on
$H(p_X)$  tal como  $H(X)$ seg\'un  lo mas  conveniente.  Ademas,  $p_X$ podr\'a
denotar  indistamente  la  distribuci\'on   de  probabilidad,  o  el  vector  de
probabilidad $p_X \equiv [p(x_1) \quad \ldots \quad p(x_\alpha)]^t$.

%En  su papel,  C. Shannon  defin\'o esta  entropia a  partir de  en  ensemble de
%axiomas  m\'inimos   que  deberia   cumplir  una  medida   de  incerteza   o  de
%informaci\'on.   Se va  a ver  estos  axiomas como  unas de  las 
$H$ tiene propiedades notables, que coresponden a las que se puede exigir de una
medida de incerteza~\cite{Sha48, ShaWea64, CovTho06, Rio07, DemCov91, Joh04}:
%
\begin{propiedades}
\item\label{prop:SZ:continuidad} {\it Continuidad:}  Vista como una funci\'on de
  $\alpha$ variables $u_i = p_X(x_i)$, $H$ es continua con respeto a los $u_i$.
%
\setcounter{PropPermutacion}{\value{enumi}}
\item\label{prop:SZ:permutacion}   {\it  Invariance  bajo   una  permutaci\'on:}
  Obviosamente,  la  entropia  es  invariante  bajo  una  permutaci\'on  de  las
  probabilidades, \ie, $$\mbox{para cualquier permutaci\'on } \sigma: \A \to \A,
  \quad H(p_\sigma)  = H(p) \quad \mbox{con} \quad  p_\sigma(x) = p(\sigma(x))$$
  lo que se  escribe tambi\'en $H(\sigma(X)) = H(X)$.   En particular, denotando
  $p^\downarrow$ la  distribuci\'on de probabilidades obtenida a  partir de $p$,
  clasificando las  probabilidades en orden  decreciente, $p^\downarrow(x_1) \ge
  p^\downarrow(x_2) \ge \cdots  \ge p^\downarrow(x_\alpha)$, $$H(p^\downarrow) =
  H(p)$$
%
\setcounter{PropBiyeccion}{\value{enumi}}
\item\label{prop:SZ:biyeccion}   {\it  Invariance   bajo   una  transformaci\'on
    biyectiva:}  La  entropia  es  invariante  bajo  cualquier  transformaci\'on
  biyectiva, \ie $$\mbox{para  cualquier funci\'on biyectiva } g:  \A \to g(\A),
  \quad H(g(X))  = H(X)$$ A  trav\'es tal transformaci\'on los  estados cambian,
  pero  no  cambia  la  distribuci\'on  de probabilidad  vinculada  al  alfabeto
  transformado.  Tomando el  ejemplo de un dado, la  incerteza vinculada al dado
  no debe  depender de  los simbolos  escritos sobre las  caras, sean  enteras o
  cualquier letras.
%
\item\label{prop:SZ:positividad} {\it  Positividad:} La entropia  es acotada por
  debajo, $$H(X) \ge 0$$  con iguladad si y solamente si existe  un $x_0 \in \A$
  tal que $p_X(x_0)  = 1$ \ y  \ $p_X(x) = 0$ \  para \ $x \ne x_0$,  $$H(X) = 0
  \quad \mbox{ssi} \quad X \mbox{ es deterministico}$$ En otras palabras, cuando
  $X$ no es  aleatoria, \ie $X =  x_0$, no hay incerteza, o  la observaci\'on no
  lleva  informaci\'on (se  sabe lo  que va  a salir,  sin duda):  $H =  0$.  La
  positividad  es  consecuencia de  $p_i  \le  1$, dando  $-  p_i  \log p_i  \ge
  0$. Ademas, la suma de terminos positive  es cero si y solo si cada termino es
  cero, dando $p_i = 0$ o $p_i = 1$.
%
\setcounter{PropCotamaxima}{\value{enumi}}
\item\label{prop:SZ:cotamaxima}  {\it Maximalidad:} La  entropia es  acotada por
  arriba, $$H(X) \le \log \alpha$$ con  iguladad si y solamente si existe $X$ es
  uniforme sobre $\A$,  \ie $$H(X) = \log \alpha  \quad \mbox{ssi} \quad \forall
  \, x \in \A, \: p_X(x) = \frac{1}{\alpha}$$ En otras palabras, la incerteza es
  maxima cuando cualquier  estado $x$ puede aparecer con  la misma probabilidad;
  cada  observaci\'on lleva una  informaci\'on importante  sobre el  sistema que
  genera $X$.   La cota m\'axima resuelta  de la maximisaci\'on de  $H$ sujeto a
  $\sum_i  p_i  =  1$,  es  decir,   con  la  tecnica  del  Lagrangiano,  de  la
  minimizaci\'on  de  $\sum_i (-  p_i  \log p_i  +  \lambda  p_i)$.  Se  obtiene
  sencillamente   que   $\log  p_i   =   -\lambda$,   dando  la   distribuci\'on
  uniforma.\newline   La   figura~\ref{fig:SZ:EntropiaBinaria}   representa   la
  entropia de  un sistema  a dos estados,  de probabilidades  \ $\lambda$ \  y \
  $1-\lambda$  \ (lei  de Bernoulli  de parametro  $\lambda$), entropia  a veces
  dicha {\it entropia binaria}, en  funci\'on de $\lambda$.  Esta figura ilustra
  ambas cotas ($\lambda = 1$ o  1, $\lambda = \frac12$) as\'i que la invariancia
  bajo    una    permutaci\'on    ($h(\lambda)    =    H(\lambda,1-\lambda)    =
  H(1-\lambda,\lambda) = h(1-\lambda)$).
  %
  \begin{figure}[h!]
  \begin{center}\input{TEX/EntropiaBinaria}\end{center}
  %
  \leyenda{Entropia  binaria  (de  una  variable  de  Bernoulli)  $h(\lambda)  =
    H(\lambda,1-\lambda)$ en funci\'on de $\lambda \in [0 , 1]$.}
  \label{fig:SZ:EntropiaBinaria}
  \end{figure}
%
\item\label{prop:SZ:expansabilidad} {\it Expansibilidad:}  A\~nadir un estado de
  probabilidad  0 no  cambia  la entropia,  \ie  $$\widetilde{\A} =  \A \cup  \{
  \widetilde{x}_0  \}  \quad  \mbox{con}  \quad \widetilde{p}(x)  =  p(x)  \quad
  \mbox{si}  \quad x \in  \A, \quad  \widetilde{p}(\widetilde{x}_0) =  0, \qquad
  \mbox{entonces}  \quad  H(\widetilde{p}) =  H(p)$$  Esta  propiedad es  obvia,
  consecuencia de $\lim_{p \to 0} p \log p = 0$.
%
\item\label{prop:SZ:recursividad} {\it Recursividad:} Juntar dos estados baja la
  entropia de una cantidad igual a la entropia interna de los dos estados por la
  probabilidad    de    ocurencia   de    este    conjunto    de   estados,    y
  vice-versa,  $$\left\{  \begin{array}{l}\overline{\A}  =  \{ x_1  ,  \ldots  ,
      x_{\alpha-2}  ,   \overline{x}_{\alpha-1}\}  \quad  \mbox{con   el  estado
        interno}  \quad  \overline{x}_{\alpha-1} =  \{  x_{\alpha-1} ,  x_\alpha
      \},\\[2.5mm] \overline{p}(x_i) = p(x_i), \quad  1 \le i \le \alpha-1 \quad
      \mbox{y} \quad \overline{p}(\overline{x}_{\alpha-1}) = p(x_{\alpha-1}) +
      p(x_\alpha)  \quad  \mbox{distribuci\'on  sobre  }  \overline{\A}\\[2.5mm]
      \displaystyle   \overline{q}(x_j)    =   \frac{p(x_j)}{p(x_{\alpha-1})   +
        p(x_\alpha)}, \quad j =  \alpha-1, \alpha \quad \mbox{distribuci\'on del
        estado   interno}\end{array}\right.$$   $$H(p)   =   H(\overline{p})   +
  \overline{p}(\overline{x}_{\alpha-1})  \,  H(\overline{q})$$  Esta  relaci\'on
  viene de $p_{n-1} \log  p_{n-1} + p_n \log p_n = \left(  p_{n-1} + p_n \right)
  \left( \frac{p_{n-1}}{p_{n-1} +  p_n} \log\left( \frac{p_{n-1}}{p_{n-1} + p_n}
    \right)  + \frac{p_n}{p_{n-1}  + p_n}  \log\left( \frac{p_n}{p_{n-1}  + p_n}
    \right) -  \log\left( p_{n-1}  + p_n \right)\right)  $ esta ilustrada  en la
  figura~\ref{fig:SZ:Recursividad} siguiente.\newline
  %
  \begin{figure}[h!]
  \begin{center} \input{TEX/Recursividad} \end{center}
  %
  \leyenda{Ilustraci\'on de  la propiedad  de recursividad, que  cuantifica como
    decrece la entropia en un  ensemble cuando se juntan dos estados, vincluando
    la entropia  total, la  entropia despues del  la agrupaci\'on y  la entropia
    interna a los dos estados juntados.}
  \label{fig:SZ:Recursividad}
  \end{figure}
%
\item\label{prop:SZ:concavidad} {\it Concavidad:} La  entropia es concava, en el
  sentido  de que  la entropia  de una  combinaci\'on convexa  de distribuciones
  (mezcla) de probabilidades es siempre major o igual a la combinaci\'on convexa
  de entropias: $$\forall \: \{  \lambda_i \}_{i=1}^n, \quad 0 \le \lambda_i \le
  1,  \quad  \sum_i  \lambda_i  =   1  \quad  \mbox{and  cualquier  conjunto  de
    distribuciones} \quad  \{ p_i  \}_{i=1}^n,$$ $$H\left( \sum_i  \lambda_i p_i
  \right)  \ge  \sum_i \lambda_i  H(p_i)$$  Esta  desigualdad  es conocida  como
  desigualdad de  Jensen.  Es  una consequencia directa  de la convexidad  de la
  funci\'on   $\phi:   u   \mapsto   u   \log   u$,   como   ilustrado   en   la
  figura~\ref{fig:SZ:Concavidad}-(a).    La   figura~\ref{fig:SZ:Concavidad}-(b)
  ilustra como se puede obtener una mezcla de distribuciones de dos probabilidad
  $p_1$  (dado  izquierda)  y  $p_2$  (dado  derecho)  haciendo  una  elecci\'on
  aleatoria a  partir de una moneda  en este ejemplo  (probabilidad $\lambda$ de
  elegir   el   dado   izquierda).\newline
  %
  \begin{figure}[h!]
  \begin{center} \input{TEX/Concavidad} \end{center}
  %
  \leyenda{(a) $\phi(u)  = u \log u$ es  concava: la curva es  siempre debajo de
    sus cuerdas; entonces, cada promedio de $\phi(u_1)$ y $\phi(u_2)$ estando en
    la cuerda  juntando estos punto, queda  arriba de la funci\'on  tomada en el
    promedio de $u_1$  y $u_2$.  Escribiendo eso para (mas  de dos puntos) sobre
    los $\sum_i \lambda_i  p_i(x)$ y sumando sobre los $x$  da la desigualdad de
    Jensen.  (b) Ilustraci\'on de  una distribuci\'on de mezcla, ac\'a mezclando
    $p_1$  y  $p_2$  a  partir  de  una tercera  variable  aleatoria  (ac\'a  de
    Bernoulli).}
  \label{fig:SZ:Concavidad}
  \end{figure}
%
\item\label{prop:SZ:Schurconcavidad}  {\it Schur-concavidad:}  Como se  lo puede
  querrer,  lo mas  ``concentrado'' es  una distribuci\'on  de  probabilidad, lo
  menos hay  incerteza, y entonces lo  mas peque\~no debe ser  la entropia. Esta
  propiedad intuitiva se resuma a partir de la noci\'on de mayorizaci\'on:
  %
  \begin{definicion}[Mayorizaci\'on]\label{def:SZ:Mayorizacion}
    Una distribuci\'on  discreta finita de probabilidad $p$  es dicha mayorizada
    por una distribuci\'on $q$, $$p  \prec q \quad \mbox{ssi} \quad \sum_{i=1}^k
    p^\downarrow(x_i) \le \sum_{i=1}^k q^\downarrow(x_i), \quad 1 \le k < \alpha
    \quad \mbox{y} \quad \sum_{i=1}^\alpha p^\downarrow(x_i) = \sum_{i=1}^\alpha
    q^\downarrow(x_i)$$  (las  \'ultimas  sumas  siendo  igual  a  1).   Si  los
    alfabetos de definici\'on de $p$ y $q$ son de tama\~nos diferentes, $\alpha$
    es el  tama\~no lo mas grande y  la distribuci\'on sobre el  alfabeto lo mas
    corto es completada por estados de  probabilidad 0 (recuerdense de que no va
    a cambiar la entropia).
  \end{definicion}
  %
  La  Schur-concavidad  se  traduce  por  la  relaci\'on  $$  p  \prec  q  \quad
  \Rightarrow \quad  H(p) \ge H(q)$$ Fijense  de que las cotas  sobre $H$ pueden
  ser  vistas como  consecuencia de  esta desiguldad:  la  distribuci\'on cierta
  mayoriza  cualquier  distribuci\'on  y  cualquier distribuci\'on  mayoriza  la
  distribuci\'on uniforme.
\end{propiedades}

En muchos  casos, uno tiene que  trabajar con varias  variables aleatorias. Para
simplificar les  notaciones, considera una par  de variables $X$  y $Y$ definida
respectivamente sobre  los alfabetos $\A$ y  $\B$ de cardinal $\alpha  = |\A|$ y
$\beta = |\B|$.   Tal par de variable puede ser vista  como una variable $(X,Y)$
definida sobre el alfabeto $\A \times  \B$ de cardinal $\alpha \beta$ tal que se
definie naturalmente  la entropia  para esta variable;  tal entropia  es llamada
{\it entropia conjunta} de $X$ y $Y$:
%
%~\cite{Sha48, ShaWea64}
\begin{definicion}[Entropia conjunta]\label{def:SZ:EntropiaConjunta}
  Sean $X$ y $Y$ dos variable aleatorias definidas sobre los alpfabetos discretos
  $\A$  y $\B$,  de  cardinal  $\alpha =  |\A|  < +\infty$  y  $\beta  = |\B|  <
  +\infty$. Sea $p_{X,Y}(x,y)$ la distribuci\'on de probabilidad conjunta de $X$
  e $Y$, \ie $  \forall \, (x,y) \in \A \times \B,  \quad p_{X,Y}(x,y) = \Pr[X =
  x, Y  = y]$. La  entropia conjunta de  Shannon de las  variables $X$ y  $Y$ es
  definida por
  %
  \begin{equation}
  H(p_{X,Y}) = H(X,Y) = - \sum_{(x,y) \in \A \times \B} p_{X,Y}(x,y) \, \log p_{X,Y}(x,y)
  \end{equation}
  %
  con la convenci\'on $0 \log 0 = 0$.
\end{definicion}

A partir de esta definici\'on,  aparecen otras propiedades importantes, sino que
fundamentales, de la entropia de Shannon.
%
\begin{propiedades}
\item\label{prop:SZ:aditividad}  {\it Aditividad:} La  entropia conjunta  de dos
  variables  aleatorias   $X$  e  $Y$  \underline{independientes}   se  suma,  y
  reciprocamente:  $$X   \:  \mbox{e}   \:  Y  \:   \mbox{independientes}  \quad
  \Leftrightarrow \quad  H(X,Y) = H(X) +  H(Y)$$ Dicho de otra  manera, para dos
  variables aleatorias, la incerteza global es la suma de las incertezas de cada
  variable individual.   La propiedad ``$\Rightarrow$''  es consecuencia directa
  de $p_{X,Y}(x,y) = p_X(x) p_Y(y)$. Se va a probar en la secci\'on siguiente la
  reciproca. Se  generaliza sencillamente a un conjunto  de variables aleatorias
  $\{ X_i \}$.
%
\item\label{prop:SZ:subaditividad} {\it Sub-aditividad:} La entropia conjunta de
  dos variables aleatorias  $\{ X_i \}_{i=1}^n$ es siempre menor  que la suma de
  cada entropia individual: $$ H(X_1,\ldots,X_n) \, \le \, \sum_{i=1}^n H(X_i)$$
  Dicho de otra manera, variables  pueden compartir informaci\'on, de tal manera
  de que le entropia global sea menor que la suma.  De la propiedad anterior, se
  obtiene la igualdad ssi los $X_i$ son indepedientes.
%
\item\label{prop:SZ:superaditividad}   {\it   Super-aditividad:}   La   entropia
  conjunta de dos variables aleatorias  $\{ X_i \}_{i=1}^n$ es siempre major que
  cualquiera  de  las entropias  individuales  $$  H(X_1,\ldots,X_n)  \, \ge  \,
  \max_{1 \le i \le n} H(X_i)$$.
\end{propiedades}

Es importante notar  de que existen varios enfoques basados  sobre una series de
axiomas, dando lugar  a la definici\'on de la entropia  tal como definido. Estos
axiomas  son conocidos  como axiomas  de Shannon-Khinchin  y son  la continuidad
(propiedad~\ref{prop:SZ:continuidad}),               la              maximalidad
(propiedad~\ref{prop:SZ:cotamaxima}),              la             expansabilidad
(propiedad~\ref{prop:SZ:expansabilidad})         y         la         aditividad
(propiedad~\ref{prop:SZ:aditividad}).  Existen varios otros conjunto de axiomas,
conduciendo tambi\'en a la entropia de Shannon (ver Shannon~\cite[Sec.~6]{Sha48}
or  \cite{ShaWea64},  R\'enyi~\cite{Ren61}  o Fadeev~\cite{Fad56,  Fad58}  entre
otros).

Para  una  serie de  variables  aleatorias,  $X_1,  X_2, \ldots$,  representando
simbolos, se puede  definir una entropia por simbolo  como una entropia conjunta
divido por numero de simbolos, $\frac{H(X_1,\ldots,x_n)}{n}$, as\'a que una taza
de entropia cuando $n$ va al inifinito.
%
\begin{definicion}[Taza de entropia]\label{def:SZ:TazaDeEntropia}
  Sea $\X = \{  X_i \}_{i \in \Nset^*}$ una  serie de variable aleatoria.  La taza de
  entropia de esta serie es definida por
  %
  \begin{equation}
  \H(\X) = \lim_{n \to \infty} \frac{H(X_1,\ldots,X_n)}{n}
  \end{equation}
  %
\end{definicion}
%
\noindent Esta cantidad siempre existe, porque $H(X_1 , \ldots , X_n) \le \sum_i
H(X_i) \le \sum_i \log \alpha_i \le  n \max_i \alpha_i$ donde los $\alpha_i$ son
los cardinales de los alfabetos de definici\'on de los $X_i$.

\

Se termina esta sub-secci\'on con el caso de variables discretas definidas sobre
un  alfabeto $\A$ de  cardinal infinito  $|\A| =  + \infty$,  por ejemplo  $\A :
\Nset$.   Por  analogia,  se  puede  siempre  definir la  entropia  como  en  la
definici\'on Def.~\ref{def:SZ:Shannon}. Esta extensi\'on resuelta delicada dando
de que unas  propiedades se perdien.  Por ejemplo, la  entropia no queda acotada
por  arriba como  se  lo puede  probar  para la  distribuci\'on de  probabilidad
$\displaystyle p(x)  \propto \frac{1}{(x+2) \left(  \log (x+2) \right)^2},  \: x
\in \Nset$,  corectamente normalizada ($\propto$  significa ``proporcional a''):
$\displaystyle \frac{\log \log(x+2)}{(x+2) \left( \log (x+2) \right)^2} \ge 0$ y
la serie $\displaystyle \sum_x  \frac{1}{(x+2) \log (x+2)}$ es divergente, as\'i
que la serie $\displaystyle - \sum_x p(x) \log p(x)$ diverge.

% ================================= Axiomas

\subseccion{Entrop\'ia diferencial}
\label{ss:SZ:Diferencial}

Volviendo a la definici\'on Def.~\ref{def:SZ:Shannon} de la entropia de Shannon,
usando el operador $\Esp$ promedio  estadistica o esperanza matematica, se puede
rescribir la entropia de Shannon como $H(X) = \Esp\left[ - \log p_X(X) \right]$.
Con este punto  de vista, es facil extender la definici\'on  de la entropia para
variables aleatorias continuas admitiendo  una densidad de probabilidad.  Eso da
lugar a lo que es conocido como la {\it entropia diferencial}:

\begin{definicion}[Entropia diferencial]\label{def:SZ:EntropiaDiferencial}
  Sea $X$ una  variable aleatoria definida sobre un  espacio $d$-dimensional $\D
  \subseteq \Rset^d$ y sea $p_X(x)$ la densidad (distribuci\'on) de probabilidad
  de $X$, La entropia diferencial de la variable $X$ es definida por
  %
  \begin{equation}
    H(p_X) = H(X) = - \int_{\D} p_X(x) \, \log p_X(x) \, dx
  \end{equation}
  %
  (con la  convenci\'on $0 \log  0 = 0$,  se puede escribir la  integraci\'on en
  $\Rset^d$).
\end{definicion}
%
Como en el  caso discreto, para $X = (X_1,\ldots,X_d)$, esta  entropia de $X$ es
dicha entropia conjunta de los componentes $X_i$.

Como se lo  va a ver, la entropia diferencial no  tiene la misma significaci\'on
de  incerteza,  siendo de  que  depende no  solamente  de  la distribuci\'on  de
probabilidad, sino  que de los estados tambi\'en.  Mas alla, no se  la puede ver
como l\'imite continua de un caso discreto:  a trav\'es de tal l\'imite, se va a
ver que se  llama diferencial, a causa del efecto de  la diferencial $dx$.  Para
ilustrar  eso,  considera una  variable  aleatoria  escalar  $X$ viviendo  sobre
$\Rset$ y $p_X$ su densidad de probabilidad.  Sea $\delta > 0$ y sea el alfabeto
$\A^\delta  = \{  x_k  \}_{k \in  \Zset}$ donde  los  $x_k$ se  definen tal  que
$\displaystyle p_X(x_k)  \delta = \int_{k \delta}^{(k+1) \delta}  p_X(x) \, dx$,
como ilustrado en la figure~\ref{fig:SZ:CuantificacionX}.  Se define la variable
aleatoria discreta $X^\delta$  sobre $\A^\delta$ tal que $\Pr[X^\delta  = x_k] =
p_{X^\delta}(x_k) = p_X(x_k) \delta$. Se  puede ver $X^\delta$ como la versi\'on
cuantificada de $X$, con $X^\delta = x_k$ cuando $X \in [k \delta , (k+1) \delta
)$.   Al rev\'es,  a\'un que  sea  delicado, se  puede interpretar  $X$ como  el
``l\'imite'' de $X^\delta$ cuando $\delta$ tiende a 0. Ahora, es claro de que
%
\begin{eqnarray*}
H(X^\delta) & = & - \sum_k p_{X^\delta}(x_k) \log p_{X^\delta}(x_k)\\[2.5mm]
%
& = & - \log \delta - \sum_k \Big( p_X(x_k) \log p_X(x_k) \Big) \, \delta
\end{eqnarray*}
%
lo que se escribe tambien $$H(X^\delta)  + \log \delta = - \sum_k \Big( p_X(x_k)
\log p_X(x_k)  \Big) \, \delta$$ Entonces,  de la intergraci\'on  de Rieman sale
que
%
\begin{equation*}
\lim_{\delta \to 0} \left( H(X^\delta) + \log \delta \right) = H(X)
\end{equation*}
%
Dicho de  otra manera,  la entropia  diferencial de $X$  no es  el limite  de la
entropia  de su  versi\'on  cuantificada:  aparece con  la  entropia el  termino
``diferencial'' $\log \delta$.
%
\begin{figure}[h!]
\begin{center} \input{TEX/CuantificacionX} \end{center}
%
\leyenda{Densidad de probabilidad $p_X$ de $X$, construcci\'on del alfabeto $\A$
  donde  se  define   la  versi\'on  cuantificada  $X^\delta$  de   $X$  con  su
  distribuci\'on discreta de probabilidad $p_{X^\delta}$. La superficia en grise
  oscuro es igual a la superficia definida por el rectangulo en grise claro.}
\label{fig:SZ:CuantificacionX}
\end{figure}
%

Mas  all\'a  de  esta  notable  diferencia  entre  la  entropia  y  la  entropia
diferencial, la \'ultima depende de los estados,  es decir que si $Y = g(X)$ con
$g$  biyectiva,  no  se  conserva  la  entropia,  \ie  \underline{se  pierde  la
  propiedad~\ref{prop:SZ:biyeccion}}  del  caso discreto:
%
\begin{eqnarray*}
H(Y) & = & - \int_{\Rset^d} p_Y(y) \log p_Y(y) \, dy\\[2.5mm]
%
& = &  - \int_{\Rset^d} p_Y(g(x)) \log p_Y(g(x)) \, |\Jac_g(x)| \, dx\\[2.5mm]
%
& = & - \int_{\Rset^d} p_Y(g(x)) \Big( \log \big( p_Y(g(x)) \, |\Jac_g(x)| \big) -
\log |\nabla^t g(x)| \Big) \, |\Jac_g(x)| \, dx
\end{eqnarray*}
%
donde $\Jac_g$ es la  matriz de componentes $\frac{\partial g_i}{\partial x_j}$,
Jacobiano  de  la transformaci\'on  $g:  \Rset^d  \mapsto  \Rset^d$ y  $|\cdot|$
representa el valor absoluto del determinente de la matriz.  Recordandose de que
$p_X(x) = p_Y(g(x)) |\Jac_g(x)|$, se obtiene
%
\begin{propiedadesC}\setcounter{enumi}{\value{PropBiyeccion}}
%
\item\label{prop:SZ:biyeccionC}
Para cualquier biyecci\'on $g: \Rset^d \mapsto \Rset^d$
  \begin{equation*}
    H(g(X)) = H(X) +  \int_{\Rset^d} p_X(x) \log |\Jac_g(x)| \, dx
  \end{equation*}
  %
  donde el \'ultimo termino, $\Esp\left[  \log |\Jac_g(X)| \right]$ no vale cero
  en    general.   En    particular,   si    $H$   es    invariante    bajo   un
  deplazamiento,  $$H(X+\mu) = H(X)  \quad \forall  \: \mu  \in \Rset^d$$  no es
  invariante por cambio de escala, $$H(a X) = H(X) + \log |a| \quad \forall \: a
  \in \Rset^*$$
\end{propiedadesC}
%
Esta  \'ultima relaci\'on  queda valid  para  $a$ matriz  invertible.  Por  esta
\'ultima  relaci\'on, se puede  ver que,  dado $X$,  cuando $a$  tiende a  0, la
entropia de  $a X$ tiende a  $-\infty$.  Es decir que,  para $a$ suficientemente
peque\~no,  se  puede  tener $H(a  X)  <  0$,  as\'i que  \underline{se  pierde}
tambi\'en \underline{la positividad, propiedad~\ref{prop:SZ:positividad}}.  Esta
perdida definitivamente quita la interpretaci\'on de incerteza/informaci\'on que
hubiera podido tener la entropia diferencial.  A veces, se usa lo que es llamado
potencia entropica:
%
\begin{definicion}[Potencia entropica]
  Sea $X$ una  variable aleatoria $d$-dimensional. La potencia  entropica de $X$
  es definida por $$N(X) = \frac{2 \pi \e} \exp\left( \frac2d H(X)
  \right)$$
\end{definicion}
%
\noindent Por construcci\'on,  $N(X) \ge 0$.  Ademas, en  el caso continuo, $N(a
X+b) = |a|^2 N(X)$ (queda valida para una matriz $a$ invertible): esta propiedad
puede justificar la idea de  ``potencia''; ademas $N(a X+b)$ tiende naturalmente
a cero cuando $a$ tiende a  cero.  Se recupera as\'i la noci\'on informacional a
trav\'es  de  $N$ en  este  contexto  ($a X  +  b$  ``tiende''  a $b$,  variable
deterministica).

Si se pierde  la propiedad de invarianza bajo  una biyecci\'on, sopredentemente,
se conserva la entropia bajo el equivalente continuo del rearreglo.
%
\begin{definicion}[Rearreglo simetrico]
  Sea $\P \subset \Rset^d$ abierto de  volumen finito $|\P| < +\infty$.  El {\it
    rearreglo simetrico}  $\P^\downarrow$ de  $\P$ es la  bola centrada en  0 de
  mismo volumen  que $\P$, \ie $$\P^\downarrow =  \left\{ x \in \Rset^d  \, : \:
    \frac{2  \pi^{\frac{d}{2}}  |x|^d}{\Gamma\left(\frac{d}{2}\right)} \le  |\P|
  \right\}$$  donde $|\cdot|$  denota  la norma  euclideana.   Eso es  ilustrado
  figure~\ref{fig:SZ:ensemblerearreglado}-a.\newline  Sea $p_X$ una  densidad de
  probabilidad y sea $\P_t = \{ y \, : \: p_X(y) > t \}$ para cualquier $t > 0$,
  sus ensembles  de niveles.  La densidad de  probabilidad~\footnote{Se proba de
    que  esta funci\'on,  positiva  por  definici\'on, suma  a  1.  Ademas,  por
    construcci\'on,  depende   unicamente  de   $|x|$  y  decrece   con  $|x|$.}
  rearreglada    simetrica    $p^\downarrow_X$     de    $p_X$    es    definida
  por  $$p^\downarrow_X(x) =  \int_0^{+\infty} \un_{\P_u^\downarrow}(x)  \, du$$
  con $\un_A$ el indicator  del ensemble $A$, \ie $\un_A(x) = 1$  si $x \in A$ y
  cero sino.
\end{definicion}
%
Del hecho  de que $\forall \, t  < \tau \: \Leftrightarrow  \: \P_\tau \subseteq
\P_t  \:  \Leftrightarrow \:  \P_\tau^\downarrow  \subseteq \P_t^\downarrow$  es
sencillo   ver   que   si   $x   \in  \P_\tau^\downarrow$,   entonces   $x   \in
\P_t^\downarrow$, lo que conduce a  $p_X^\downarrow(x) > \tau$ y vice-versa. Mas
alla,  sobre $\P_{\tau+d\tau}  \backslash \P_\tau$  la funci\'on  $p_X$ ``vale''
$\tau$ \ y \ sobre $\P_{\tau+d\tau}^\downarrow \backslash \P_\tau^\downarrow$ la
funci\'on $p_X^\downarrow$  ``vale'' tambien $\tau$, lo que  da \ $\displaystyle
\int_{\P_\tau^\downarrow} p_X^\downarrow(x) \, dx = \int_{\P_\tau} p_X(x) \, dx$
(ver~\cite{LieLos01, WanMad04} para une prueba mas rigorosa).  La representaci\'on de la
definici\'on es conocida como representaci\'on en capas de pastel (layer cake en
ingles). Eso es ilustrado en la figura~\ref{fig:SZ:ensemblerearreglado}-b
  %
  \begin{figure}[h!]
  \begin{center} \input{TEX/Rearreglo} \end{center}
  %
  \leyenda{(a):  Ilustraci\'on  del rearreglo  simetrico  $\P^\downarrow$ de  un
    ensemble  $\P$,  siendo  la  bola  centrada  en  0  de  mismo  volumen.  (b)
    Construcci\'on  del rearreglo  $p_X^\downarrow$:  dado un  $\tau$, se  busca
    $\P_\tau$ y  se deduce $P_\tau^\downarrow$; dado  un $x$, se  busca el mayor
    $t$  tal  que  $x  \in  P_t^\downarrow$, este  $t$  maximo  siendo  entonces
    $p_X^\downarrow(x)$;  ademas, por construcci\'on,  las superficias  en grise
    son iguales.}
  \label{fig:SZ:ensemblerearreglado}
  \end{figure}
% =  \B  \left( 0  , r_\P  \right)$ con  $\frac{2
%    \pi^{d/2} r_\P^d}{\Gamma(d/2)} = |\P|$.

\begin{propiedadesC}\setcounter{enumi}{\value{PropPermutacion}}
\item\label{prop:SZ:permutacionC} {\it invarianza  bajo un rearreglo:} Sea $p_X$
  densidad   de  probabilidad   sobre   un  abierto   de  $\Rset^d$,   $$H\left(
    p_X^\downarrow \right) = H(p_X)$$
\end{propiedadesC}
%
\noindent Esta propiedad es probada por ejemplo en~\cite{LieLos01, WanMad04}.

{\color{red}\bf      a      ver       que      pasa      en      termino      de
  mayorizacion~\ref{prop:SZ:mayorizacion}?}

Como  se lo  ha visto,  la  entropia diferencial  no es  siempre positiva,  como
consecuencia  de~\ref{prop:SZ:biyeccionC}.   Tambi\'en,  la  propiedad  de  cota
superior,  \underline{propiedad~\ref{prop:SZ:cotamaxima} se pierde}  en general,
\underline{salvo si se pone vinculos}:
%
\begin{propiedadesC}\setcounter{enumi}{\value{PropCotamaxima}}
\item
\begin{enumerate}
\item\label{prop:SZ:cotamaximauniforme} Si  $\D$ es de volumen finito  $|\D| < +
  \infty$,  la  entropia es  acotada  por arriba,  $$H(X)  \le  \log |\D|$$  con
  igualdad ssi $X$ es \underline{uniforme}.
%
\item\label{prop:SZ:cotamaximagaussiana} Si $\D =\Rset^d$ y $X$ tiene una matriz
  de  covarianza dada  $\Sigma_X =  \Esp\left[  X X^t  \right]$ donde  $\cdot^t$
  denota la transpuesta, la entropia es tambi\'en acotada por arriba, $$H(X) \le
  \frac{d}{2} \log(2 \pi \e) + \frac12 \log |\Sigma_X|$$ con igualdad ssi $X$ es
  \underline{gaussiana}.  En  particular, la potencia entropica  de la gaussiana
  vale $N(X) = |\Sigma_X|^{\frac1d}$, dando  de nuevo un ``sabor'' de potencia a
  $N$. Como se o va a ver  en este cap\'itulo, la gaussiana juega un rol central
  en la teoria de la informaci\'on.
\end{enumerate}
\end{propiedadesC}
%
{\bf\color{red} Poner MaxEnt mas general? Con vinculo tipo desigualdad de Gibbs,
  o momentos y leyes de la familia exponencial?}

Al      final,     \underline{se      conservan      las     propiedades      de
  concavidad~\ref{prop:SZ:concavidad},  de aditividad~\ref{prop:SZ:aditividad} y
  de sub-aditividad~\ref{prop:SZ:subaditividad}}.   Es interesante de  notar que
de  la desigualdad~\ref{prop:SZ:subaditividad},  puramente  entropica, se  puede
deducir la  desigualdad de Hadamard,  desigualdad puramente matricial:  $|R| \le
\prod_i  R_{i,i}$  para  cualquier  matriz simetrica  definida  positiva  (viene
de~\ref{prop:SZ:subaditividad} escrita  para una  gaussiana de covarianza  $R$ y
tomando una exponencial de la desigualdad).


% ================================== Mutua =================================== %

\seccion{Entropia condicional, informaci\'on mutua, entropia relativa}
\label{s:SZ:Mutua}

Tratando de un par de variable  aleatorias $X$ e $Y$, una cuesti\'on natural que
occure es de cuantificar la incerteza que queda sobre una de las variable cuando
se  observa  la  otra.  Dicho  de  otra  manera, si  se  mide  $Y  =  y$,  ?`que
informaci\'on lleva sobre  $X$? La respuesta a esta  interogaci\'on se encuentra
en la  noci\'on de entropia condicional. Si  uno mide $Y =  y$, la descripci\'on
estadistica de $X$ conociendo este $Y$ se resuma a la distribuci\'on condicional
de  probabilidad $p_{X|Y}  = \frac{p_{X,Y}}{p_Y}$.   Con esta  restricci\'on, se
puede  evaluar una  incerteza sobre  $X$, sabiendo  de que  $Y=y$,  $$H(X|Y=y) =
H\left(  p_{X|Y}(\cdot,y)  \right)$$ Entonces,  condicionalmente  a la  variable
aleatoria $Y$,  la incerteza va  a ser el  promedio estadistico sobre  todos los
estados $Y$ es decir $H(X|Y) = \sum_y p_Y(y) H(X|Y=y)$:
%
\begin{definicion}[Entropia condicional]\label{def:SZ:entropiacondicional}
  Sean $X$ e $Y$ dos  variables aleatorias discretas, la entropia condicional de
  $X$ sabiendo  $Y$ es  definida por $$H(X|Y)  = - \sum_{x,y}  p_{X,Y}(x,y) \log
  p_{X|Y}(x,y)$$
\end{definicion}
%
Esta definici\'on se transpone naturalmente a la entropia diferencial:
%
\begin{definicion}[Entropia diferencial condicional]\label{def:SZ:entropiadiferencialcondicional}
  Sean $X$ e $Y$ dos  variables aleatorias continuas, la entropia condicional de
  $X$ sabiendo $Y$ es definida por $$H(X|Y) = - \int_{\Rset^d} p_{X,Y}(x,y) \log
  p_{X|Y}(x,y) \, dx \, dy$$
\end{definicion}

Si $X$ e $Y$ son indepedientes, $p_{X|Y}$ se reduce a $p_X$, as\'i que vale cero
la entropia condicional:
%
\begin{propiedades}
\item\label{prop:SZ:independenciacondicional}   $$X    \:   \mbox{e}   \:    Y   \:
  \mbox{independientes} \quad \Leftrightarrow \quad H(X|Y) = H(X)$$
\end{propiedades}
%
Esta  propiedad  vale  en ambos  casos,  discreto  como  continuo.  En  el  caso
discreto, se interpreta como el hecho  de que $Y$ no lleva ninguna informaci\'on
sobre $X$, y intonces ninguna medici\'on  de $Y$ va a cambiar la incerteza sobre
$X$.

Siendo $H(X|Y=y)$  una entropia,  va a  heredir de todas  las propiedades  de la
entropia  (diferencial).  Ademas,  de  $p_{X,Y}  = p_{X|Y}  p_Y$  de  deduce  la
propiedad siguiente (valida para la entropia como su extensi\'on diferencial)
%
\begin{propiedades}
\item\label{prop:SZ:cadena}  {\it Regla de  cadena} $$H(X,Y)  = H(X|Y)  + H(Y)$$
  Esta  regla, valida  en ambos  casos,  discreto como  continuo, se  generaliza
  sencillamente a $$H(X_1 , \ldots  , X_n) = H(X_1) + \sum_{i=2}^n H(X_i|X_{i-1}
  ,   \ldots   ,   X_1)$$   De   esta   regla   de   cadena   se   recupera   la
  propiedad~\ref{prop:SZ:independenciacondicional}     a     partir    de     la
  propiedad~\ref{prop:SZ:aditividad}.
\end{propiedades}
%
Siendo  $H(X|Y=y)$  una   entropia,  en  el  caso  discreto   esta  cantidad  es
positiva. Entonces, en  el caso discreto, $H(X|Y)$ es positiva,  lo que proba la
la super-aditividad~\ref{prop:SZ:superaditividad}.

De la regla de  cadena $H(X,Y) = H(X|Y) + H(Y) = H(Y|X)  + H(X)$ aparece que las
cantidades  $H(X|Y)-H(X)$, $H(Y|X)-H(Y)$  y $H(X,Y)  -  H(X) -  H(Y)$ son  todas
iguales. Estas canditades definen lo que se llama la informaci\'on mutua entre \
$X$ \ e \ $Y$:

%
\begin{definicion}[Informaci\'on mutua]\label{def:SZ:mutua}
  Sean $X$ e $Y$ dos variables  aleatorias, la informaci\'on mutua entre \ $X$ \
  e  \ $Y$ \  es la  cantida simetrica  $$I(X;Y) =  H(X|Y)-H(X) =  H(Y|X)-H(Y) =
  H(X,Y) -  H(X) - H(Y)$$ En el  caso discreto se expresa  $$I(X;Y) = \sum_{x,y}
  p_{X,Y}(x,y)  \log \left(  \frac{p_{X,Y}(x,y)}{p_X(x) p_Y(y)}  \right)$$  y su
  forma  diferencial, se  escribe  $$I(X;Y) =  \int_{\Rset^d} p_{X,Y}(x,y)  \log
  \left( \frac{p_{X,Y}(x,y)}{p_X(x) p_Y(y)} \right) \, dx \, dy$$
\end{definicion}

Las diferentes cantitades  peden ser vista a trav\'es  una visi\'on ensemblista,
como  descrita el la  figura~\ref{fig:SZ:Venn}. Este  diagrama es  conocido como
diagrama de Venn.
%
\begin{figure}[h!]
\begin{center} \input{TEX/Venn} \end{center}
\leyenda{Diagrama  de Venn:  Ilustraci\'on  de la  definici\'on  de la  entropia
  condicional,  informaci\'on  mutua, y  los  vinculos  entre  cada medida.   La
  superficia  del elipse en  linea llena  (parte grise)  representa $H(X)$  y el
  interior  de la en  linea punteada  representa $H(Y)$.   La parte  grise clara
  representa  $H(X|Y)$ superficia  del  ensemble $H(X)$  quitando  la parte  que
  partenece  a  $H(Y)$.  La  parte  blanca  representa  $H(Y|X)$ superficia  del
  ensemble $H(Y)$  quitando la parte que  partenece a $H(X)$. La  parte en grise
  oscuro es entonces lo que $X$ e $Y$ comparten, es decir $I(X;Y)$.}
\label{fig:SZ:Venn}
\end{figure}

Como se lo va a probar, $I$ es positiva; representa realmente una informaci\'on,
la compartida  entre \ $X$  \ e  \ $Y$. Si  de la incerteza  de $X$ se  quita la
incerteza  de  $X$   una  vez  que  $Y$  es  medida,  lo   que  queda  tiene  la
significaci\'on de la informaci\'on que  estas variables tienen en com\'un. Para
probar la positividad de $I$, se  introduce de manera mas general la noci\'on de
entropia     relativa,     conocida     tambi\'en    como     divergencia     de
Kullback-Leibler~\cite{CovTho06, Rio07, Kul}:
%
{\bf\color{red} Buscar ref de Kullback and so on}
%
\begin{definicion}[Entropia relativa]\label{def:SZ:entropiarelativa}
  La entropia relativa, o divergencia de una distribuci\'on de probabilidad $q$,
  con  respeco a  una distribuci\'on  de referencia  $p$, donde  el  alfabeto de
  definici\'on de $p$ inluye lo de $q$, es definida como $$D(q\|p) = \sum_x q(x)
  \log \left( \frac{q(x)}{p(x)} \right)$$ o, en su forma diferencial $$D(q\|p) =
  \int_{\Rset^d} q(x) \log \left( \frac{q(x)}{p(x)} \right) \, dx$$
\end{definicion}
%
Esta  medida se  puede ser  vista como  una entropia  de la  distribuci\'on $q$,
relativamente a  una distribuci\'on de referencia  $p$. Por ejemplo,  en el caso
discreto  finito, si  $p$ es  la distribuci\'on  uniforme sobre  un  alfabeto de
cardinal  $\alpha$,  $D(q\|p) =  \log  \alpha -  H(q)$,  lo  que representa  una
desviaci\'on  de la  entropia con  su valor  maximal. La  misma interpretaci\'on
queda en  el caso continuo  con la  lei uniforme ($p$  y $q$ definidas  sobre el
mismo espacio  de volumen finito) o  con la gaussiana  ($p$ y $q$ dando  la misma
matriz de covarianza).

\begin{lema}[Positividad de la entropia relativa]
  $$D(q\|p) \ge 0 \quad \mbox{con igualdad ssi} \quad p = q \: (c.s.)$$ donde
  $(c.s.)$ significa ``casi siempre''.
\end{lema}
%
\begin{proof}
  Existen varias pruebas,  pero la mas linda puede ser  la usando la desigualdad
  de   Jensen:   para   $\phi$   estrictamente   convexa,   $\Esp[\phi(X)]   \ge
  \phi(\Esp[X])$ con igualdad ssi $X$  es deterministica (casi siempre). Sea $X$
  de distribuci\'on  o densidad  de probabilidad $p$.  En el caso  discreto como
  diferencial,   se  escribe   la  entropia   relativa  $D(q\|p)   =  \Esp\left[
    \frac{q(X)}{p(X)} \log  \left( \frac{q(X)}{p(X)} \right) \right]$.  Sea $Y =
  \frac{q(X)}{p(X)}$   y  $\phi(u)   =  u   \log  u$,   funci\'on  estrictamente
  convexa. Entonces $D(q\|p) =  \Esp[\phi(Y)] \ge \phi(\Esp[Y])$. Con $\Esp[Y] =
  \Esp\left[ \frac{q(X)}{p(X)} \right] = \sum_x q(x) = 1$ (y con una integral en
  el caso diferencial) y $\phi(1) = 0$ sz termina la prueba. El caso de igualdad
  apareciendo   ssi  $Y$   es  deterministica,   es   decir  $\frac{p(X)}{q(X)}$
  deterministica, es equivalente a $p(x) \propto  q(x) \: (c.s.)$, \ie $p = q \:
  (c.s.)$ porque ambas suman a uno.
\end{proof}


{\bf\color{red} Eso es la desigualdad de Gibbs}

Esta propiedad, valide  en el caso discreto como  continuo, tiene consecuencias,
cuando  se fije  de que  $$I(X;Y) =  D \left(  \left. p_{X,Y}  \right\|  p_X p_Y
\right)$$ \ie la informaci\'on mutua es la divergencia de Kullback-Leibler de la
distribuci\'on conjunta relativa al producto de las marginales.
%
\begin{propiedades}
\item\label{prop:SZ:Ipositive}   {\it   $I$   es   positiva,  como   medida   de
    independencia:} $$I(X;Y)  \ge 0 \quad \mbox{con  igualdad ssi $X$  e $Y$ son
    independientes}$$
%
\item\label{prop:SZ:condicionar} {\it  Condicionar reduce la  entropia} $$H(X|Y)
  \le H(X)  \quad \mbox{con  igualdad ssi $X$  e $Y$ son  independientes}$$ Esta
  desigualdad,      con     la      regla     de      cadena,      prueba     la
  sub-aditividad~\ref{prop:SZ:subaditividad}.    {\color{red}  Esta  reducci\'on
    vale en promedio, pero el conocimiento  de un valor particular puede ser tal
    que  $H(X|Y  =  y)  >   H(X)$,  \ie  aumentar  la  entropia!  (ver  ejemplos
    en~\cite[p.~59]{Rio07}}
\end{propiedades}

Fijense  que  si  $D$ es  positiva,  no  es  simetrica  y tampoco  satisface  la
desigualdad triangular. Por  eso, no es una distancia y tiene  el nombre de {\it
  divergencia}. La distribuci\'on de referencia $p$ juega un rol fundamental.


% ================================== Mutua =================================== %

\seccion{Unas identidades y desigualdades}
\label{s:SZ:Desigualdades}

% Libro Loss Ruskai [Lieb]

{\bf\color{red} Desigualdades de Fano? Rioul p. 78, Cover P.~663}

% ================================= EPI

\subseccion{Desigualdad de la potencia entropica}

Sean $X$  e $Y$ dos variables indepedientes.  Si se sabe las  relaciones entre \
$H(X,Y)$, \ $H(X)$,  \ $H(Y)$, una pregunta natural  concierna la relaci\'on que
podrian tener $X+Y$ con cada variable en termino de entropia. La respuesta no es
trivial, y el  resultado general concierna el caso  de variables continuas sobre
$\Rset^d$.   Es conocido  como desigualdad  de la  potencia entropica  (EPI para
entropy power  inequality en ingl\'es). No  vincula las entropias,  sino que las
potencias entropicas.
%
\begin{teorema}[Desigualdad de la potencia entropica]
  Sean  $X$  e  $Y$  dos variables  $d$-dimensionales  continuas  indepedientes,
  entonces  $$N(X +  Y)  \ge  N(X) +  N(Y)$$  con igualdad  sii  $X$  e $Y$  son
  gaussianas  con  matrices  de  covarianza  proporcionales,  $\Sigma_Y  \propto
  \Sigma_X$ (siempre verdad en el contexto escalar).
\end{teorema}
%
\noindent     Existen    varias     formulaciones     alternativas    a     esta
desiguladad~\cite{Sha48, Lie78, CovTho06, DemCov91, Rio07}:
%
\begin{enumerate}
\item\label{EPI:SZ:EquivGauss} Sean $\widetilde{X}$ y $\widetilde{Y}$ gaussianas
  independientes   de   matriz   de   covarianza  proporcionales   y   tal   que
  $H(\widetilde{X}) =  H(X)$ y $H(\widetilde{Y}) = H(Y)$.  Entonces $$N(X+Y) \ge
  N\left( \widetilde{X} + \widetilde{Y} \right)$$ con igualdad sii $X$ y $Y$ son
  gaussianas.
%
\item\label{EPI:SZ:PresCov}    {\it    Desigualdad    de    preservaci\'on    de
    covarianza:} $$ \forall \, 0 \le \lambda \le 1, \quad H\left( \sqrt{\lambda}
    X +  \sqrt{1-\lambda} Y  \right) \ge \lambda  H(X) + (1-\lambda)  H(Y)$$ con
  igualdad el el caso gaussiano con matrices de covarianza proporcionales.
\end{enumerate}
%
%\noindent La  forma del teorema implica la  forma~\ref{EPI:SZ:PresCov} tomando \
%$\sqrt{\lambda} X$ \ y  \ $\sqrt{1-\lambda} Y$ \ en lugar de \ $X$  \ e \ $Y$, y
%recordandose que \ $N(a X) =  a^2 N(X)$. Reciprocamente, la forma del teorema se
%recupera de la forma~\ref{EPI:SZ:PresCov} tomando \ $\lambda = \frac12$.

La prueba  de esta(s) desigualdad(es) no es  trivial.  Numeras versiones
existen,  dadas por  ejemplo  en las  referencias~\cite{Bla65, Sta59,  ShaWea64,
  Rio07,  Rio11,  Rio17,  CovTho06,  DemCov91,  Lie78,  VerGuo06}  (ver  tambien
teorema~6  de~\cite{Lie75}) entre  otros.  Como  se lo  puede ver,  la gaussiana
juega un rol particular en esta desigualdad, saturandola.


{\color{red}\bf  ver  si  es  corto   probar  la  equivalencia  entre  las  tres
  formas. Existe una forma, de Madiman, a traver rearreglo}

Esta desigualdad se usa para  probar otras desigualdades, como por ejemplo la
desigualdad de  Minkowsky $|R_1 +  R_2|^{\frac1d} \ge $ para  cualquier matrices
$R_1,  R_2$ simetricas  definidas positivas  (viene de  $X$ e  $Y$  gaussianas de
covarianza $R_1$  y $R_2$).  Aparece  tambi\'en para acotar  informaci\'on mutua
entre variables y  calcular la capacidad de un canal  de communicaci\'on como se
le va a ver~\cite{CovTho06, DemCov91, Rio07, Joh04}.

{\color{red}  En  el  caso  discreto,  no hay  un  resultado  general.  Existent
  solamento resultados para variables particulares~\cite{toto, titi}.}


% ================================= Data Processing Theorem

\subseccion{Desigualdad de procesamiento de datos}

Esta  desigualdad  traduce  que  procesando  datos,  no  se  puede  aumentar  la
informaci\'on disponible sobre  una variable. Se basa sobre  una desigualdad que
satisface la informaci\'on mutua aplicada a un proceso de Markov.

\begin{definicion}[Proceso de Markov]
  Una secuencia  $X_1 \mapsto X_2 \mapsto  \ldots \mapsto X_n$ es  dicha {\it de
    Markov}   si   para  cualquier   $i   >   1$,  $$p_{X_{i-1},X_{i+1}|X_i}   =
  p_{X_{i-1}|X_i}  p_{x_{i+1}|X_i}$$ Dicho  de otra  manera,  condicionalmente a
  $X_i$,  las  variables  $X_{i-1}$  y  $X_{i+1}$ son  independientes.   Eso  es
  equivalente  a  $$p_{X_{i+1}|X_i,X_{i-1},\ldots}  = p_{X_{i+1}|X_i}$$  Si  $i$
  representa  un tiempo, significa  que la  esdadistica de  $X_{i+1}$ conociendo
  todo  el  pasado  se  reduce   a  esa  conociendo  el  pasado  inmediato  (las
  probabilidades   dichas   de   transici\'on   $p_{X_{i+1}|X_i}$   caracterisan
  completamente  el proceso).  Es sencillo  fijar  de que  $X_n \mapsto  X_{n-1}
  \mapsto \ldots \mapsto X_1$ es tambien un proceso de Markov.
\end{definicion}

\begin{teorema}[Desigualdad de procesamiento de datos]
  Sea  $X \mapsto  Y \mapsto  Z$ un  proceso de  Markov. Entonces,  $$I(X;Y) \ge
  I(X;Z)$$ con igualdad  sii $X \mapsto Z \mapsto Y$ es  tambi\'en un proceso de
  Markov. En  particular, es sencillo ver  que para cualquier  funci\'on $g$, $X
  \mapsto Y  \mapsto g(Y)$ es un  proceso de Markov,  lo que da $$\forall  \, g,
  \quad  I(X;Y) \ge  I(X;g(Y))$$ La  \'ultima desigualdad  se  escribe tambi\'en
  $H(X|g(Y))  \ge   H(X|Y)$  y  significa   que  procesar  $Y$  no   aumenta  la
  informaci\'on  que  $Y$  da  sobre   $X$  (la  incerteza  condicional  es  mas
  importante).
\end{teorema}
%
\begin{proof}
  Por definici\'on  de la  informaci\'on mutua, considerando  $X$ y  la variable
  conjunta $(Y,Z)$,
%
\begin{eqnarray*}
I(X ; Y,Z) & = & H(X) - H(X|Y,Z)\\[2.5mm]
%
& = & H(X) - H(X|Y) + H(X|Y) - H(X|Y,Z)
\end{eqnarray*}
%
\noindent Por la propiedad que $Z  \mapsto Y \mapsto X$ sea tambi\'en un proceso
de Markov, es sencillo probar que $H(X|Y,Z) = H(X|Y)$ (conciendo $Y$ sufice para
caracterisar completamente $X$), lo que da $$I(X;Y,Z) = I(X;Y)$$ Tambi\'en,
%
\begin{eqnarray*}
I(X ; Y,Z) & = & H(X) - H(X|Z) + H(X|Z) - H(X|Y,Z)\\[2.5mm]
%
& = & I(X;Y) + H(X|Z) - H(X|Y,Z)
\end{eqnarray*}
%
\noindent  Ademas,   escribiendo  $\frac{p_{X|Y,Z}}{p_{X|Z}}  =  \frac{p_{X|Y,Z}
  p_{Y|Z}}{p_{X|Z} p_{Y|Z}} = \frac{p_{X,Y|Z}}{p_{X|Z} p_{Y|Z}}$
% $H(X|Z) -  H(X|Y,Z) \Esp\left[ \log \left(
% \frac{p_{X|Y,Z}(X,Y,Z)}{p_{X|Z}(X,Z)}\right)\right] = $
%& = & \int p_{X,Y,Z} \log \left(
%  \frac{p_{X|Y,Z}}{p_{X|Z}} \right)\\[2.5mm]
%%
%& = & \int p_{X,Y,Z} \log \left( \frac{p_{X|Y,Z}
%    p_{Y|Z}}{p_{X|Z}   p_{Y|Z}}   \right)\\[2.5mm]
%%
%& = &   \int   p_{X,Y,Z}   \log   \left(
%  \frac{p_{X,Y|Z}}{p_{X|Z}  p_{Y|Z}}  \right)
%\end{eqnarray*}
%
%\noindent  (y  similaramente  en  el   caso  discreto)  
%Calculos sencillos,
se muestra  que $H(X|Z)  - H(X|Y,Z)$ es  una divergencia de  Kullback-Leibler de
$p_{X,Y|Z}$ relativamente a $p_{X|Z}  p_{Y|Z}$, o informaci\'on mutua $I(X;Y|Z)$
entre \ $X$ \ e \ $Y$,  condicionalmente a \ $Z$).  Entonces $$I(X;Y) = I(X;Z) +
I(X;Y|Z)$$ lo que  proba la desigualdad del hecho de que  una divergencia sea no
negativa. Ademas, se obtiene la igualdad sii  $I(X;Y|Z) = 0$, es decir $X$ e $Y$
independientes  condicionalmente a  $Z$, lo  que es  la definici\'on  de  que $X
\mapsto Z \mapsto Y$ sea un proceso de Markov.
\end{proof}

% ================================= Data Processing Theorem

\subseccion{Secunda lei de la termodinamica}

Tratando de procesos  de Markov, aparece el equivalente de la  secunda lei de la
termodinamica:  un  sistema  aislado  evolua  hasta  llegar  su  estado  lo  mas
desorganizado.

\begin{lema}[ver~\cite{CovTho06}]
  Sea $X_1 \mapsto X_2 \mapsto \cdots  \mapsto X_n \mapsto \cdots$ un proceso de
  Markov,  con probabilidades  de transici\'on  $p_{X_{n+1}|X_n}$  dadas.  Estas
  modelan  el sistema,  independiente de  las condiciones  iniciales.   Sean dos
  distribuciones (condiciones) iniciales diferentes $p_1$ y $q_1$, conduciendo a
  las distribuciones $p_n$ y $q_n$ para $X_n$. Entonces:
%
\begin{itemize}
\item  Para cualquier  $n \ge  1$, $$D(p_{n+1}\|q_{n+1})  \le  D(q_n\|p_n)$$ las
  distribuciones $p_n$ y $q_n$ no se ``alejan'' (tiende a acercarse);
%
\item  Si  $p^*$  es  una  distribuci\'on  estacionaria,  $$D(p_{n+1}\|p^*)  \le
  D(p_n\|p^*)$$ la distribuci\'on no se aleja de la distribuci\'on estacionaria.
%
\item Ademas, si  los $X_n$ viven sobre  $\D$ de cardinal o volumen  finito y si
  $p^*$ es  uniforme sobre $\D$, $$H(X_{n+1})  \ge H(X_n)$$ el  sistema tiende a
  desorganizarse  (y recuerdese  de  que  la distribuci\'on  uniforme  es la  de
  entropia m\'axima).
\end{itemize}
\end{lema}
%
\begin{proof}
  Escribiendo   $p_{n+1,n}$  y  $q_{n+1,n}$   las  distribuciones   conjunta  de
  $(X_{n+1},X_n)$  para   las  dos   condiciones  iniciales,  ,   $p_{n+1|n}$  y
  $q_{n+1|n}$  las  distribuciones  condicionales  de $X_{n+1}|X_n$  as\'i  que,
  $p_{n|n+1}$ y  $q_{n|n+1}$ las distribuciones  condicionales de $X_n|X_{n+1}$,
  se muestra sencillamente  que $D(p_{n+1,n}\|q_{n+1,n}) = D(p_{n+1}\|q_{n+1}) +
  D(p_{n+1|n}\|q_{n+1|n})  =  D(p_n\|q_n)  +  D(p_{n|n+1}\|q_{n|n+1})$.  Ademas,
  $p_{n+1|n}     =    p_{X_{n+1}|X_n}     =     q_{n+1|n}$,    conduciendo     a
  $D(p_{n+1|n}\|q_{n+1|n}) = 0$ y  entonces $D(p_{n+1}\|q_{n+1}) = D(p_n\|q_n) +
  D(p_{n|n+1}\|q_{n|n+1})$.   $p_{n|n+1}$   no   es   necesariamente   igual   a
  $q_{n|n+1}$,  pero la divergencia  siendo nonnegativa,  se obtiene  la primera
  desigualdad.  La secunda desigualdad se  obtiene tomando $q_n = p^*$.  Ademas,
  si $p^*$  es uniforme  $p^*(x) = \frac{1}{|\D|}$  da $D(p_n\|p^*) =  -H(X_n) +
  \log |\D|$, dando la \'ultima desigualdad.
\end{proof}


% ================================= Principop de incerteza

\subseccion{Principio de incerteza entropico}


% ================================= Fisher

\subseccion{Un Foco sobre la informaci\'on de Fisher}

Si  la entropia  y las  heramientas relacionadas  son naturales  como  medida de
informaci\'on, no se  puede resumir una distribuci\'on a  una medida escalar. En
el marco  de la teoria de la  estimaci\'on, R. Fisher introdujo  una noci\'on de
informaci\'on intimamente relacionada al  error cuadratico en la estimaci\'on de
un   parametro    a   partir   de   una   variable    parametrizado   por   este
parametro~\cite{Fis22, Fis25:07, Kay93, Bos07, CovTho06}.
%
\begin{definicion}[Matriz informaci\'on de Fisher parametrica]
  Sea $X$ una variable aleatoria parametrizada por un parametro $m$-dimensional,
  $\theta  \in  \Theta \subseteq  \Rset^m$,  de  distribuci\'on de  probabilidad
  $p_X(\cdot;\theta)$ continua  sobre $\D  \subseteq \Rset^d$ su  soporte. Asume
  que $p_X$ sea diferenciable en  $\theta$ sobre $\Theta$.  La matriz de Fisher,
  de  tamanio $m \times  m$ es  definida por  $$J_\theta(X) =  \Esp\left[ \left(
      \nabla_\theta  \log   p_X(X;\theta)  \right)  \left(   \nabla_\theta  \log
      p_X(X;\theta) \right)^t \right]$$ donde  $\nabla_\theta = \left[ \cdots \:
    \frac{\partial}{\partial \theta_i}  \: \cdots \right]^t$ es  el gradiente en
  $\theta$.   Es la matriz  de covarianza  del {\it  score parametrico}  $S(X) =
  \nabla_\theta \log p_X(X;\theta)$  (se proba que su promedio  es cero), siendo
  $\log p_X$  la {\it log-verosimilitud}.   Bajo condiciones de  regularidad, se
  puede  mostrar~\footnote{Es una  consecuencia del  teorema de  la divergencia,
    suponiendo que los bordes del dominio de $X$ no depende de $\theta$ y que la
    funci\'on score se cancela en estos bordes.}  que $J_\theta(X)$ es tambi\'en
  menos  el promedio de  la Hessiana~\footnote{Para  $f: \Rset^m  \mapsto \Rset,
    \quad  \Hess_\theta  f$  est  la  matriz  de  componentes  $\frac{\partial^2
      f}{\partial  x_i \partial x_j}$}  $\Hess_\theta$ de  $\log p_X(X;\theta)$.
  Nota: a veces se define la  informaci\'on de Fisher como $\Tr(J)$, traza de la
  matriz informaci\'on de Fisher.
\end{definicion}
%
Como  para   la  entropia,   la  matriz  de   Fisher  se   escribe  generalmente
$J_\theta(X)$, a  pesar de que no  sea funci\'on de  $X$ pero de la  densidad de
probabilidad. Se  la notara tambi\'en  $J_\theta(p_X)$ seg\'un la  escritura la
mas conveniente.

Tomando el gradiente  en $x$ en lugar de $\theta$ da  la matriz de informaci\'on
de Fisher no parametrica,
%
\begin{definicion}[Matriz informaci\'on de Fisher no parametrica]
  Sea  $X$  una  variable  aleatoria  de distribuci\'on  de  probabilidad  $p_X$
  definida  sobre  $\D \subseteq  \Rset^d$  su  soporte.   Asuma que  $p_X$  sea
  diferenciable (en $x$).   La matriz de Fisher no parametrica,  $d \times d$ es
  definida por  $$J(X) = \Esp\left[  \left( \nabla_x \log p_X(X)  \right) \left(
      \nabla_x \log p_X(X) \right)^t \right]$$  Es la matriz de covarianza de la
  {\it  funci\'on  score} $\nabla_x  \log  p_X(X)$  (se  proba que  su  promedio
  tambi\'en es cero) o, bajo condiciones de regularidad, menos el promedio de la
  Hessiana en $x$ de la log-verosimilitud.
\end{definicion}
%
Es intersante notar que:
%
\begin{itemize}
\item Cuando $\theta$ es un  parametro de posici\'on, $p_X(x;\theta) = p(x -
\theta)$,  $\nabla_\theta \log p_X  = -  \nabla_x \log  p_X$ y  la informaci\'on
parametrica se reduce a la informaci\'on no parametrica.
%
\item  Si $X$  es  gaussiano de  matriz  de covarianza  $\Sigma_X$, entonces  se
  muestra sencillamente de que $J(X)  = \Sigma_X^{-1}$ (o, de una forma, inversa
  de la dispersi\'on o incerteza en termino de estadisticas de orden 2).
%
\item  Es  sencillo  ver  que,  por  definici\'on  $J_\theta(X)$  y  $J(X)$  son
  simetricas y  que $J_\theta(X)  > 0$  y $J(X) >  0$ donde  estas desiguladades
  significan  que las  matrices  son definidas  positivas  (los autovalores  son
  positivos).  Ademas, $$\forall \ a \ne 0, \quad J(aX) = \frac{1}{|a|^2} J(X)$$
  (queda valide  para $a$  matriz invertible).  Esta  relaci\'on da a  $J(X)$ un
  sabor de  informaci\'on en el sentido  de que, cuando $a$  tiende al infinito,
  $J(aX)$  tiende a  0;  $a X$  tiende  a ser  muy diepersas  as\'i  que no  hay
  informaci\'on sobre su posici\'on.
\end{itemize}


Una otra interpretaci\'on  de $J$ como informaci\'on es  debido a la desigualdad
de  Cram\'er-Rao que  la vincula  a la  covarianza  de estimaci\'on~\footnote{De
  hecho,  pareci\'o esta  formula tambi\'en  en los  papeles de  Fr\'echet  y de
  Darmois~\cite{Fre43, Dar45}. Como citado por Fr\'echet, aparece que la primera
  versi\'on  de esta  formula  es mucho  mas vieja  y  debido a  K.  Pearson  \&
  Filon~\cite{PeaFil98} en 1898;  luego fue extendido por Edgworth~\cite{Edg08},
  Fisher~\cite{Fis25:07}  o   Doob~\cite{Doo36}}~\cite{Rao45,  Rao92,  RaoWis47,
  Cra46, Rio07,  CovTho06, Kay93, Bos07}.   Sea $X$ parametrizada  por $\theta$.
La  meta es  estimar $\theta$  a partir  de  $X$.  Tal  estimador va  a ser  una
funci\'on unicamente de $X$, lo que se escribe usualmente~\footnote{Por ejemplo,
  si $\theta$  es un  promedio com\'un  a los componentes  de $X$,  un estimador
  podr\'ia  ser $\widehat{\theta} =  \frac1d \sum_i  X_i$} $\widehat{\theta}(X)$
(la funci\'on  no depende explicitamente  de $\theta$).  Las caractericas  de la
calidad  de  un  estimator  es  naturalmente su  bias  $b(\theta)  =  \Esp\left[
  \widehat{\theta}(X)    \right]   -    \theta$   y    su    matriz   covarianza
$\Sigma_{\widehat{\theta}}$  (la varianza  da  la dispersi\'on  alrededor de  su
promedio). La desigualdad de Cram\'er-Rao acota por debajo esta covarianza.
%
\begin{teorema}[Desigualdad de Cram\'er-Rao]
  Sea  $X$ parametrizada  por $\theta$,  de  densidad de  soporte $\D  \subseteq
  \Rset^d$  indendiente  de $\theta$  y  $\widehat{\theta}(X)$  un estimador  de
  $\theta$.  Sea $b(\theta)$ su  bias y $\Sigma_{\widehat{\theta}}$ su matriz de
  covarianza.   Sea   $\Jac_b(\theta)$  la   matriz  Jacobiana  del   bias  $b$.
  Entonces,  $$\Sigma_{\widehat{\theta}}  - \left(  I  + \Jac_b(\theta)  \right)
  J_\theta(X)^{-1} \left( I + \Jac_b(\theta) \right)^t \ge 0$$ En particular, en
  el     caso     $\theta$     escalar,    $$\sigma_{\widehat{\theta}}^2     \ge
  \frac{(1+b'(\theta))^2}{J_\theta(X)}$$   donde   $b'$   es  la   derivada   de
  $b$.\newline Tomando  $\theta$ parametro  de posici\'on y  $\widehat{\theta} =
  X$,  estimador  sin bias  ($b  =  0$),  eso da  lo  que  es conocido  como  la
  desigualdad no parametrica de Cram\'er-Rao  y toma la expresi\'on $$\Sigma_X -
  J(X)^{-1} \ge  0$$ o,  en el caso  escalar, $$\sigma_X^2  \ge \frac{1}{J(X)}$$
  Ademas, en el caso no parametrico, se alcanza la cota si y solamente si $X$ es
  un vector gaussiano.
\end{teorema}
%
\noindent Esta  desigualdad acota la variaza  de cualquier estimador,  \ie da la
varianza o error  m\'inima que se puede  esperar. Esta cota es el  inverso de la
informaci\'on de Fisher, \ie  $J_\theta(X)$ caracteriza la informaci\'on que $X$
tiene sobre $\theta$.
%
\begin{proof}
  Sea $S = \nabla_\theta \log  p_X$ y $\theta_0 = \Esp\left[ \widehat{\theta}(X)
  \right] = \theta + b(\theta)$. Fijandose  que $\nabla_\theta \log p_X \, p_X =
  \nabla_\theta p_X$, que $\widehat{\theta}$ no  es funci\'on de $\theta$, y que
  el soporte $\D$ no depende de $\theta$, se obtiene~\footnote{Se supone que los
    integrandes sean $\theta$-localmente integrables,  tal que se puede invertir
    derivada en $\theta$ e integraci\'on}
  %
  \begin{eqnarray*}
  \Esp\left[ S(X) \left( \widehat{\theta}(X) - \theta_0 \right)^t \right] & = &
  \int_\D \nabla_\theta p_X(x;\theta) \widehat{\theta}(x)^t \, dx - \left(
  \int_\D \nabla_\theta p_X(x;\theta) \, dx \right) \theta_0^t\\[2.5mm]
  %
  & = & \nabla_\theta \int_{\Rset^d} p_X(x;\theta) \widehat{\theta}(x)^t \, dx -
  \left( \nabla_\theta \int_{\Rset^d} p_X(x;\theta) \, dx \right)
  \theta_0^t\\[2.5mm]
  %
  & = & \nabla_\theta \left( \theta + b(\theta) \right)  - 
  \left( \nabla_\theta 1 \right) \theta_0^t\\[2.5mm]
  %
  & = & \left( I + \Jac_b(\theta) \right)^t
  \end{eqnarray*}
  %
  Ademas,  fijandose  que  $\Esp\left[  S(X)  S(X)^t \right]  =  J_\theta(X)$  y
  $\Esp\left[   \left(    \widehat{\theta}(X)   -   \theta_0    \right)   \left(
      \widehat{\theta}(X)      -      \theta_0      \right)^t     \right]      =
  \Sigma_{\widehat{\theta}}$,            la            desigualdad            de
  Cauchy-Bunyakovsky-Schwarz~\footnote{De  hecho, fue  probada  por Cauchy  para
    sumas en 1821, para integrales por  Bunyakovsky en 1859 y mas elegamente por
    Schwarz  en  1888~\cite{Ste04}.}   conduce  a  $$  \left(  u^t  \left(  I  +
      \Jac_b(\theta) \right)^t  v \right)^2 \:  = \: \Esp\left[ u^t  S(X) \left(
      \widehat{\theta}(X)  -  \theta_0  \right)^t  v  \right]^2 \:  \le  \:  u^t
  J_\theta(X) u  \: v^t  \Sigma_{\widehat{\theta}} \, v$$  La prueba  se termina
  tomando  \ $u =  J_\theta(X)^{-1} \left(  I +  \Jac_b(\theta) \right)^t  \, v$
  (recordandose que $J$  es simetrica).\newline Con la elecci\'on  de $u$, en la
  desigualdad de Cauchy-Bunyakovsky-Schwarz, se obtiene la igualdad cuando, $v^t
  J(X)^{-1} S(x) \propto  v^t (x - \theta)$ para cualquier $v$  y $x$, est decir
  $\nabla_x p_X (x)  \propto J(X) (x - \theta) p_X(x)$, lo  que es la ecuaci\'on
  diferencial que satisface (solamente) la  gaussiana: en este caso, se verifica
  a posteriori que $J(X) = \Sigma_X^{-1}$,  y entonces que se alcanza la cota de
  la Cram\'er-Rao no parametrica.
\end{proof}
%
\noindent En el  caso parametrico, no se puede estudiar el  caso de igualdad del
hecho  de  que  $\widehat{\theta}$ no  es  algo  dado.   Ademas, a\'un  dado  un
estimador  (independiente explicitamente de  $\theta$), no  hay garantia  de que
existe  una  densidad parametrizado  por  $\theta$ que  alcanza  la  cota, o  al
rev\'es, dado una  familia de densidades, tampoco no hay  garantia que existe un
estimador que permite alcanzar la cota.

Fijense de que, de nuevo, la gaussiana juega un rol particular en la desigualdad
de Cram\'er-Rao no parametrica , permitiendo alcanzar la cota.

Nota: para dos  matrices $A \ge 0$  \ y \ $B \ge  0$, si $A - B  \ge 0$ entonces
$|A|  \ge  \B|$,   con  igualdad  si  y  solamente   si  $A  =  B$~\cite[cap.~1,
teorema~25]{MagNeu99}.   Entonces,  de  las  desigualdades  de  Carm\'er-Rao  se
deducen     desigualdades     de     Cram\'er-Rao    escalares     $$     \left|
  \Sigma_{\widehat{\theta}} \right|  \, \ge  \, \frac{\left| I  + \Jac_b(\theta)
  \right|^2}{\left| J_\theta(X) \right|}  \qquad \mbox{y} \qquad \left| \Sigma_X
\right| \, \ge \, \frac{1}{\left|  J(X) \right|}$$ Obviamente, en la secunda, se
alcanza la igualdad si y solamente  si $X$ es gaussiano. Ademas, para una matriz
$A  \ge 0$,  existe la  ``relaci\'on determinente-traza''  \  $|A|^{\frac1d} \le
\frac1d  \Tr(A)$,  con  igualdad  si  y  solamente  si  $A  =  I$~\cite[cap.~11,
sec.~4]{MagNeu99},  dando  otras  versiones   escalares  de  la  desigualdad  de
Cram\'er-Rao, por ejemplo
% \Tr\left(\Sigma_{\widehat{\theta}}  \right)  \,  \ge  \, \frac{d^2  \,  \left(
%     \Tr\left(I   +  \Jac_b(\theta)  \right)   \right)^2}{\Trleft|  J_\theta(X)
% \right|} \qquad \mbox{y} \qquad
$$\left| \Sigma_X \right|^{\frac1d} \,  \ge   \, \frac{d}{\Tr\left( J(X) \right)},
\qquad   \Tr\left(   \Sigma_X   \right)   \,   \ge   \,   \frac{d}{\left|   J(X)
  \right|^{\frac1d}} \qquad \mbox{o} \qquad \Tr\left( \Sigma_X \right) \, \ge \,
\frac{d^2}{\Tr\left( J(X) \right)}$$ En estos casos, se obtiene la igualdad si y
solamente si  $X$ es  gaussiana (igualdad de  la Cram\'er-Rao no  parametrica) y
ademas  de covarianza  proporcional a  la identidad  (igualdad en  la relaci\'on
determinente-traza).

\

Si  la  desigualdad de  Cram\'er-Rao  da  a la  matriz  de  Fisher  un sabor  de
informaci\'on, aparece que $J$ es tambi\'en relacionado a la entropia relativa:
%
\begin{teorema}[Fisher como curvatura de la entropia relativa]
  Sea $X$  parametrizado por $\theta_0  \in \Theta$ con $\Theta$  conteniendo un
  vecinaje  de   $\theta_0$.   Siendo   $D\left(  p_X(\cdot;\theta)  \,   \|  \,
    p_X(\cdot;\theta_0)  \right)$  funci\'on  de  $\theta \in  \Theta$,  aparece
  que  $$D  \left( p_X(\cdot;\theta)  \,  \|  \,  p_X(\cdot;\theta_0) \right)  =
  \frac12  \left( \theta -  \theta_0 \right)^t  J_{\theta_0}(X) \left(  \theta -
    \theta_0 \right) + o\left( \| \theta - \theta_0 \|\right)$$ donde $o(\cdot)$
  es  un  resto peque\~no  con  respecto a  su  argumento.   En otros  terminos,
  $J_{\theta_0}(X)$ es la curvatura de la entropia relativa en $\theta_0$.
\end{teorema}
%
\begin{proof}
  La relaci\'on  es consecuencia  de un desarrollo  de Taylor  al orden 2  de la
  funci\'on $D\left( p_X(\cdot;\theta) \,  \| \, p_X(\cdot;\theta_0) \right)$ de
  $\theta$, tomada en $\theta =  \theta_0$. Por propiedad de $D$, la divergencia
  es  positiva y  se cancela  cuando $\theta  = \theta_0$.  Entonces,  el primer
  termino del desarrollo  vale cero y el secundo  tambi\'en, $D$ siendo m\'inima
  en $\theta = \theta_0$. Ademas,
%
\begin{eqnarray*}
\nabla_\theta D\left( p_X(\cdot;\theta) \, \| \, p_X(\cdot;\theta_0) \right) & =
& \nabla_\theta \int_\D p_X(x;\theta) \log \left(
\frac{p_X(x;\theta)}{p_X(x;\theta_0)} \right) dx\\[2.5mm]
%
& = & \int_\D \nabla_\theta p_X(x;\theta) \log \left(
\frac{p_X(x;\theta)}{p_X(x;\theta_0)} \right) dx + \int_\D \nabla_\theta
p_X(x;\theta) \, dx\\[2.5mm]
%
& = & \int_\D \nabla_\theta p_X(x;\theta) \log \left(
\frac{p_X(x;\theta)}{p_X(x;\theta_0)} \right) dx + \nabla_\theta \int_\D
p_X(x;\theta) \, dx\\[2.5mm]
%
& = & \int_\D \nabla_\theta p_X(x;\theta) \log \left(
\frac{p_X(x;\theta)}{p_X(x;\theta_0)} \right) dx
\end{eqnarray*}
%
la   \'ultimo  ecuaci\'on   como   consecuencia   de  que   $p_X$   suma  a   1.
Entonces, $$\Hess_\theta D\left(  p_X(\cdot;\theta) \, \| \, p_X(\cdot;\theta_0)
\right)     =     \int_\D     \Hess_\theta     p_X(x;\theta)     \log     \left(
  \frac{p_X(x;\theta)}{p_X(x;\theta_0)} \right) dx + \int_\D \frac{\nabla_\theta
  p_X(x;\theta) \, \nabla^t_\theta  p_X(x;\theta)}{p_X(x;\theta)} \, dx$$ Tomado
en  $\theta  =  \theta_0$  el  primer  vale cero.  En  el  secundo  se  reconoce
$J_\theta(X)$, lo que termina la prueba.
\end{proof}
%
Este teorema, ilsutrado en la figura~\ref{fig:SZ:JCurvatura}, vincula claramente
dos  objectos viniendo  de  la  teoria de  la  estimaci\'on y  la  teoria de  la
informaci\'on, mundos  a priori diferentes.  Como  se lo puede ver  en la figura
cuando $J_\theta(X)$  tiene peque\~nas  autovalores (figura (a)),  $p_\theta$ se
``aleja'' lentamente de  $\theta_0$ cuando $\theta$ se aleja  de $\theta_0$: hay
una  alta incerteza  o  peque\~a informaci\'on  sobre  $\theta_0$. Y  vice-versa
(figura (b)).
%
\begin{figure}[h!]
  \begin{center}   \input{TEX/JCurvatura}  \end{center}   \leyenda{Caso  escalar
    $\Theta \subseteq \Rset$  (para la representaci\'on) de $D$  en funci\'on de
    $\theta$.   (a) Caso  con  $J_{\theta_0}(X)$ ``peque\~no''  y  (b) caso  con
    $J_{\theta_0}(X)$ ``grande''. En el  caso (b) la determinaci\'on de $\theta$
    usando $D$ va a ser mas ``sencillo'' porque el m\'inimo es mas ``picado''.}
\label{fig:SZ:JCurvatura}
\end{figure}

\

Un otro vinculo  entre el mundo de la informaci\'on y  la estimaci\'on aparece a
trav\'es  de la  identidad de  de Bruijn~\footnote{A  pesar de  que  tom\'o este
  nombre, esta identidad  en su primera versi\'on fue publicada  por Stam. En su
  papel~\cite{Sta59}, menciona que esta identidad fue comunicada al Profesor van
  Soest por el Profesor  de Bruijn.}~\cite{Sta59, CovTho06, Joh04, Bar84, Bar86,
  PalVer06}. Esta identidad caracterisa lo  que es conocido como canal gaussiano
figura~\ref{fig:SZ:deBruijnVerdu}-(a),  \ie  la  salida  $Y$  es  una  versi\'on
ruidosa  de la  entrada. La  identidad vincula  las variaciones  de  entropia de
salida con respeto al nivel de ruido, y la informaci\'on de Fisher.

\begin{teorema}[Identidad de de Bruijn]
  Sea $X$ un  vector aleatorio continuo sobre un  abierto $\Rset^d$ y admitiendo
  una  matriz  de  covarianza,  y  sea  $Y   =  X  +  C  \Gauss$  donde  $C$  es
  deterministica, $d \times  d'$ con $ d \le d'$, de  rango m\'aximo, y $\Gauss$
  un vector gaussiano centrado y de covarianza $\Sigma_\Gauss$, independiente de
  $X$  (ver  figura~\ref{fig:SZ:deBruijnVerdu}-(a)).  Entonces, la  entropia  de
  Shannon y la informaci\'on de Fisher  de $Y$ satisfacen $$\nabla_C H(Y) = J(Y)
  \, C \, \Sigma_\Gauss$$ donde $\nabla_C  \, \cdot$ es la matriz de componentes
  $\frac{\partial \,  \cdot}{\partial C_{i,j}}$.  Si $C =  C(\theta)$ depende de
  un  parametro escalar~\footnote{Si  el parametro  es multivariado,  hace falta
    entender la  desigualdad a  trav\'es de deriva  parciales con respeto  a los
    componentes de $\theta$.}  $\theta$, $$\frac{\partial}{\partial \theta} H(Y)
  = \Tr\left( J(Y) \, C \, \Sigma_\Gauss \, \frac{\partial C^t}{\partial \theta}
  \right)$$
\end{teorema}
%
\begin{proof}
  La  clave de  este resultado  viene del  hecho de  que la  densidad $p$  de $C
  \Gauss$ satisface una ecuaci\'on diferencial particular.
  %  $$\nabla_C p_{C \Gauss}(x) =  \Hess_x p_{C
  %    \Gauss}(x) \, C \, \Sigma_\Gauss$$ Esta ecuaci\'on viene de
  La  distribuci\'on de  $C \Gauss$  se escribe  $p(x) =  (2 \pi)^{-\frac{d}{2}}
  \left| C \Sigma_\Gauss C^t  \right|^{-\frac12} \exp\left( - \frac12 x^t \left(
      C  \Sigma_\Gauss C^t  \right)^{-1} x  \right)$ (el  rango m\'aximo  de $C$
  asegura que $C \Sigma_\Gauss C^t$ sea invertible).  Para una matriz invertible
  $R$,  desarollando  $|R|$  con  respecto  a  su  linea  $i$,  se  obtiene  que
  $\frac{\partial  |R|}{\partial R_{i,j}}  = R_{i,j}^*$  cofactor  de $R_{i,j}$,
  dando por la regla de Cram\'er $\nabla_R |R| = |R| \, \left( R^{-1} \right)^t$
  (ver     tambi\'en~\cite[cap.~1~\&~9]{MagNeu99}),    es     decir    $\nabla_R
  |R|^{-\frac12}  =  -\frac12   |R|^{-\frac12}  \left(  R^{-1}  \right)^t$.   De
  $\frac{\partial |R|^{-\frac12}}{\partial  C_{i,j}} = \sum_{k,l} \frac{\partial
    |R|^{-\frac12}}{\partial R_{k,l}}  \frac{\partial R_{k,l}}{\partial C_{i,j}}
  =   -\frac12    |R|^{-\frac12}   \sum_{k,l}   \left(    R^{-1}   \right)_{l,k}
  \frac{\partial  R_{k,l}}{\partial  C_{i,j}}$ con  $R  =  C \Sigma_\Gauss  C^t$
  (simetrica)  y calculos  basicos  se obtiene  finalmente  $$\nabla_C \left|  C
    \Sigma_\Gauss  C^t  \right|^{-\frac12}  =   -  \left|  C  \Sigma_\Gauss  C^t
  \right|^{-\frac12} \left(  C \Sigma_\Gauss C^t  \right)^{-1} C \Sigma_\Gauss$$
  Ademas,  de $\left(  C \Sigma_\Gauss  C^t \right)  \left( C  \Sigma_\Gauss C^t
  \right)^{-1}   =  I$   viene  $\frac{\partial   \left(  C   \Sigma_\Gauss  C^t
    \right)^{-1}}{\partial C_{i,j}} = -  \left( C \Sigma_\Gauss C^t \right)^{-1}
  \frac{\partial \left( C \Sigma_\Gauss  C^t \right)}{\partial C_{i,j}} \left( C
    \Sigma_\Gauss  C^t  \right)^{-1}$ donde  $e_i$  es el  vector  con  1 en  su
  componente $i$ y cero si no, dando
  %
  \begin{eqnarray*}
  \frac{\partial \left( x^t \left( C \Sigma_\Gauss C^t \right)^{-1} x
  \right)}{\partial C_{i,j}} & = & - x^t \left( C \Sigma_\Gauss C^t \right)^{-1}
  \left( e_i e_j^t \Sigma_\Gauss C^t + C \Sigma_\Gauss e_j e_i^t \right) \left( C
  \Sigma_\Gauss C^t \right)^{-1} x\\[2.5mm]
  %
  & = & - 2 \, e_i^t \left( C \Sigma_\Gauss C^t \right)^{-1} x x^t \left( C
  \Sigma_\Gauss C^t \right)^{-1} C \Sigma_\Gauss e_j
  \end{eqnarray*}
  %
  usando $x^t A e_k e_l^t  B x = e_l^t B x x^t A e_k =  e_k^t A^t x x^t B^t e_l$
  (escalares  comutan y  un  escalar es  igual  a su  transpuesta)  y usando  la
  simetria de  $C \Sigma_\Gauss C^t$.   Eso significa que $$\nabla_C  \left( x^t
    \left(  C  \Sigma_\Gauss C^t  \right)^{-1}  x  \right) =  -  2  \, \left(  C
    \Sigma_\Gauss C^t \right)^{-1} x x^t \left( C \Sigma_\Gauss C^t \right)^{-1}
  C \Sigma_\Gauss,$$ dando $$\nabla_C p(x) = \left( - \left( C \Sigma_\Gauss C^t
    \right)^{-1}  + \left(  C  \Sigma_\Gauss  C^t \right)^{-1}  x  x^t \left(  C
      \Sigma_\Gauss C^t  \right)^{-1} \right) C \Sigma_\Gauss  \, p(x)$$ Tomando
  la  Hessiana  de $p$  con  respeto  a $x$  se  obtiene  sencillamente que  $p$
  satisface  la  ecuaci\'on  diferencial  $$\nabla_C  p  = \Hess_x  p  \,  C  \,
  \Sigma_\Gauss$$  Suponiende que  se puede  intervertir derivadas  y integrales
  (ver~\cite{Bar84, Bar86}  donde se dan  condiciones rigorosas), $\displaystyle
  p_Y(y) = \int_{\Rset^d} p_X(x) p(y-x) \, dx$ satisface tambi\'en la ecuaci\'on
  diferencial, y ademas
  %
  \begin{eqnarray*}
  \nabla_C H(Y) & = & - \int_{\Rset^d} \nabla_C \, p_Y(y) \log p_Y(y)
  \, dy - \int_{\Rset^d} \nabla_C \, p_Y(y) \, dy\\[2.5mm]
  %
  & = & - \left( \int_{\Rset^d} \Hess_y \, p_Y(y) \log p_Y(y) \, dy \right) C \,
  \Sigma_\Gauss - \nabla_C \int_{\Rset^d} p_Y(y) \, dy\\[2.5mm]
  %
  & = & - \left( \int_{\Rset^d} \left( \Hess_y \Big( p_Y(y) \log p_Y(y) \Big) -
  \Hess_y \, p_Y(y) - \frac{\nabla_y \, p_Y(y) \, \nabla_y \, p_Y(y)^t}{p_Y(y)}
  \right) \, dy \right) C \, \Sigma_\Gauss\\[2.5mm]
  %
  & = & - \left( \int_{\Rset^d} \Hess_y \Big( p_Y(y) \log p_Y(y) \Big) \, dy -
  \int_{\Rset^d} \Hess_y p_Y(y) \, dy \right) C \, \Sigma_\Gauss \: + \: J(Y) \, C
  \, \Sigma_\Gauss
  \end{eqnarray*}
  %
  usando la  ecuaci\'on diferencial en la  secunda linea, el hecho  de que $p_Y$
  suma  a  1  en  la  tercera  linea  (su gradiente  es  cero  entonces),  y  la
  definici\'on de la matriz de Fisher en la \'ultima linea. Usando el teorema de
  la  divergencia (intergraci\'on  por  partes) aplicada  respectivamente a  los
  componentes de $\nabla_y p_Y \log  p_Y$ y $\nabla_y p_Y$, suponiendo que estos
  gradientes  se cancelan  en el  borde del  dominio de  integraci\'on,  los dos
  terminos  integrales valen cero,  lo que  cierra la  prueba de  la desigualdad
  general.   Ademas,  si  $C  =  C(\theta)$, la  secunda  desigualdad  sigue  de
  $\frac{\partial   \cdot}{\partial    \theta}   =   \sum_{i,j}   \frac{\partial
    \cdot}{\partial   C_{i,j}}   \frac{\partial   C_{i,j}}{\partial  \theta}   =
  \Tr\left( \nabla_C \, \frac{\partial C^t}{\partial \theta} \right)$.
\end{proof}

La  versi\'on  inicial  de la  identidad  de  de  Bruijn, con  $\Sigma_\Gauss  =
I$,   $$\frac{d}{d\theta}   H(X+\sqrt{\theta}   \Gauss)  =   \frac12   \Tr\left(
  J(X+\sqrt{\theta} \Gauss)  \right)$$ se  recupera en el  caso particular  $C =
\sqrt{\theta}  I$.  En  este caso,  la ecuac\'ion  diferencial satisfada  por la
densidad de probabilidad $p$ es la {\it ecuaci\'on del calor}.  Esta desigualdad
cuantifica las variaciones de entropias bajo varaciones de ``niveles'' del ruido
del canal de comunicaci\'on. De una forma, caracteriza la robustez del canal con
respeto al nivel de ruido gaussiano  (la gaussiana juega de nuevo un rol central
ac\'a).

\

Existe una  otra forma  muy similar  de esta desigualdad  debido a  Guo, Shamai,
Verd\'u, Palomar~\cite{GuoSha05, PalVer06}. Esta versi\'on vincula a\'un mas los
mundo  de  la   informaci\'on  y  el  de  la  estimaci\'on.    Del  lado  de  la
comunicaci\'on, consiste a caracterisar  la informaci\'on mutua entre la entrada
$X$ de un canal  ruidoso y su salida, $Y = B X +  \Gauss$ donde $B$ coresponde a
un  pre-tratamiento antes  de la  salida, figura~\ref{fig:SZ:deBruijnVerdu}-(b).
Del lado de  la estimaci\'on, uno puede querer  estimar $X$ observando solamente
$Y$.  Es  conocido que el estimador  que minimiza el  error cuandratico promedio
$\Esp\left[  \left\| \widehat{X}(Y)  -  X \right\|^2  \right]$  es la  esperanza
condicional  $\widehat{X}(Y) =  \Esp[X|Y]$. Una  caracteristica de  un estimador
siendo su  matriz de covarianza,  se notar\'a $\E(X|Y)  = \Esp\left[ \left(  X -
    \Esp[X|Y]  \right) \left(  X  - \Esp[X|Y]  \right)^2  \right]$ esta  matriz.
Sorpredentemente,  existe  tambi\'en  una  identidad  entre \  $I(X;Y)$  \  y  \
$\E(X|Y)$:
%
\begin{teorema}[Identidad de Guo--Shamai--Verd\'u]
  Sea  $X$  un  vector  aleatorio  continuo  sobre  un  abierto  $\Rset^{d'}$  y
  admitiendo una  matriz de covarianza, y  sea $Y = B  X + \Gauss$  donde $B$ es
  deterministica, $d  \times d'$, y $\Gauss$  un vector gaussiano  centrado y de
  covarianza      $\Sigma_\Gauss$,      independiente      de      $X$      (ver
  figura~\ref{fig:SZ:deBruijnVerdu}-(b)). Entonces, la informaci\'on mutua entre
  \ $X$ \  e \ $Y$ y la  matriz de covarianza del estimador  de error cuadratico
  m\'inimo satisfacen  $$\nabla_B I(X;Y) = \Sigma_\Gauss^{-1} B  \, \E(X|Y)$$ Si
  $B      =     B(\theta)$     depende      de     un      parametro     escalar
  $\theta$,    $$\frac{\partial}{\partial    \theta}    I(X;Y)    =    \Tr\left(
    \Sigma_\Gauss^{-1} \,  B \, \E(X|Y) \,  \frac{\partial B^t}{\partial \theta}
  \right)$$
\end{teorema}
%
\begin{proof}
  Notando  que  $p_{Y|X} (y,x)  =  (2  \pi)^{-\frac{d}{2}} \left|  \Sigma_\Gauss
  \right|^{-\frac12}  \exp\left( -\frac12  (y-B  x)^t \Sigma_\Gauss^{-1}  (y-Bx)
  \right)$ viene $\nabla_B p_{Y|X}(x,y)  = p_{Y|X}(x,y) \, \Sigma_\Gauss^{-1} (y
  - B x) x^t$  (ver unos pasos de la prueba de la  identidad de de Bruijn) as\'i
  que $\nabla_y  p_{Y|X}(y,x) = p_{Y|X}(y,x)  \, \Sigma_\Gauss^{-1} (y -  B x)$,
  dando  $$\nabla_B p_{Y|X}(y,x)  =  \nabla_y p_{Y|X}(y,x)  x^t \qquad  \mbox{y}
  \qquad  \nabla_B  p_{X,Y}(x,y) =  \nabla_y  p_{X,Y}(x,y) x^t$$  (multiplicando
  ambos lados por $p_X$. Ahora, $I(X;Y) =  H(Y) - H(Y|X) = H(Y) - H(\Gauss)$ (de
  la  independencia, cuando  $X =  x$, $Y  = B  x +  \Gauss$ gaussiana  de misma
  convarianza que $\Gauss$ y de promedio $B x$), as\'i que
  %
  \begin{eqnarray*}
  \nabla_B I(X;Y) & = & \nabla_B H(Y)\\[2.5mm]
  %
  & = & - \int_{\Rset^d \times \Rset^{d'}} \nabla_B \Big( p_{X,Y}(x,y) \, \log
  p_Y(y) \Big) \, dx \, dy\\[2.5mm]
  %
  & = & - \int_{\Rset^d \times \Rset^{d'}} \nabla_B \, p_{X,Y}(x,y) \, \log p_Y(y) \,
  dx \, dy - \int_{\Rset \times \Rset} p_{X|Y}(x,y) \, \nabla_B \, p_Y(y) \, dx \,
  dy\\[2.5mm]
  %
  & = & \int_{\Rset^d \times \Rset^{d'}} \nabla_y p_{X,Y}(x,y) \, x^t \log p_Y(y)
  \, dx \, dy - \int_{\Rset^d} \nabla_B p_Y(y) \, dy\\[2.5mm]
  %
  & = & - \int_{\Rset^d \times \Rset^{d'}} \nabla_y p_Y(y) x^t p_{X|Y}(x,y) \, dx
  \, dy\\[2.5mm]
  %
  & = & - \int_{\Rset^d} \nabla_y p_Y(y) \, \Esp\left[X^t | Y = y \right] \, dy
  \end{eqnarray*}
  %
  La secunda linea  viene de la escritura de $H(Y)$  usando $p_Y$ como marginale
  de $p_{X,Y}$  en $x$ y  intercambiando gradiente e  integral (ver pasos  de la
  prueba de la desigualdad de de Bruijn); la tercera de $p_{X,Y}/p_Y = p_{X|Y}$;
  en la cuarta  se usa la ecuaci\'on diferencial satisfecha  por $p_{X,Y}$ en la
  primera integral y  integrando en $x$ en la secunda  integral; la quinta linea
  se obtiene usando el teorema  de la divergencia (intergraci\'on por partes) en
  la integraci\'on en  $y$ de la primera integral,  e intercambiando gradiente e
  integral el la secunda ($p_Y$ sumando a 1, el termino se cancela). Ademas,
  %
  \begin{eqnarray*}
  \nabla_y p_Y(y) & = & \int_{\Rset^{d'}} \nabla_Y p_{Y|X}(y,x) \, p_X(x) \,
  dx\\[2.5mm]
  %
  & = & - \Sigma_\Gauss^{-1} \int_{\Rset^{d'}} (y - B x) \, p_{Y|X}(y,x) \, p_X(x) \,
  dx\\[2.5mm]
  %
  & = & - \Sigma_\Gauss^{-1} \left( y - B \int_{\Rset^{d'}} x \, p_{X|Y}(x,y) \,dx
  \right) p_Y(y)\\[2.5mm]
  %
  & = & - \Sigma_\Gauss^{-1} \Big( y - B \Esp\left[X | Y=y \right] \Big) \, p_Y(y)
  \end{eqnarray*}
  %
  escribiendo $p_{Y|X}(y,x)  \, p_X(x) =  p_{X|Y}(x,y) \, p_Y(y)$ en  la tercera
  linea. Esta ecuaci\'on permite escribir
  %
  \begin{eqnarray*}
  \nabla_B I(X;Y) & = & \Sigma_\Gauss^{-1} \int_{\Rset^d} \Big( y - B \Esp\left[X |
  Y=y \right] \Big) \Esp\left[X^t | Y=y \right] \, p_Y(y) \, dy\\[2.5mm]
  %
  & = & \Sigma_\Gauss^{-1} \left( \Esp\left[ Y \Esp\left[ X^t|Y \right] \right] - B
  \Esp\left[ \Esp\left[X | Y \right] \Esp\left[X | Y \right]^t \right]
  \right)\\[2.5mm]
  %
  & = & \Sigma_\Gauss^{-1} \left( \Esp\left[ Y X^t \right] - B \Esp\left[ \Esp\left[X
  | Y \right] \Esp\left[X | Y \right]^t \right] \right)\\[2.5mm]
  %
  & = & \Sigma_\Gauss^{-1} B \left( \Esp\left[ X X^t \right] - \Esp\left[
  \Esp\left[X | Y \right] \Esp\left[X | Y \right]^t \right] \right)
  \end{eqnarray*}
  %
  la \'ultima linea viniendo de $Y = B X + \Gauss$ con $\Gauss$ independiente de
  $X$ y  de promedio 0.  La  prueba se cierra notando  que $\Esp\left[ \Esp[X|Y]
  \right] = \Esp[X]$ y por  la formula de K\"onig-Huyggens. La secunda identidad
  viene  de  $\frac{\partial \cdot}{\partial  \theta}  =  \Tr\left( \nabla_B  \,
    \frac{\partial B^t}{\partial \theta} \right)$ (ver prueba de la identidad de
  de Bruijn).
\end{proof}
%
\noindent La primera versi\'on de esta identidad se recupera con $B = \sqrt{s}$,
$\Sigma_\Gauss  =  I$  y $X$  de  covarianza  identidad;  $s$ es  conocido  como
relaci\'on se\~nale/ruido es este caso.

Existen versiones  a\'un mas completas (con  gradientes con respeto  a la matriz
$\Sigma_\Gauss$ por  ejemplo) que se pueden  consultar en~\cite{Joh04, PalVer06,
  PayPal09}.

\begin{figure}[h!]
  \begin{center}  \input{TEX/CanalDeBruijnVerdu} \end{center}  \leyenda{Canal de
    comunicaci\'on gaussiano  de entrada $X$.  (a) Canal  gaussiano usual, donde
    $C$ maneja los parametros (nivel) del  ruido. (b) canal con gaussiano con un
    preprocesamiento $B$ de la entrada.}
\label{fig:SZ:deBruijnVerdu}
\end{figure}
\

De la desigualdad de la potencia entropica  y de la identidad de de Bruijn surge
una otra desigualdad implicando la  potencia entropica $N$ y la informaci\'on de
Fisher $J$. Esta desigualdad es conocida como desigualdad de Stam~\footnote{Como
  por  la identidad  de  de Bruijn,  stam  mencion\'o que  esta desigualdad  fue
  comunicada al Profesor van Soest por el Profesor de Bruijn quien da una prueba
  variacional  de  la desigualdad.}~\cite{CovTho06,  Rio07,  Sta59},  o a  veces
``desigualdad isoperimetrica para la entropia''~\cite{WanMad04}.
%
\begin{teorema}[Desigualdad de Stam]
  Sea   $X$    una   variable    aleatoria   continua   sobre    $\D   \subseteq
  \Rset^d$. Entonces, $$N(X)  \Tr\left( J(X) \right) \, \ge  \, d$$ con igualdad
  si y solamente si $X$ es gaussiano de covarianza proporcional a la identidad.
\end{teorema}
%
\begin{proof}
  De la  desigualdad de  la potencia entropica  se obtiene $N(X  + \sqrt{\theta}
  \Gauss)  \ge N(X)  + \theta  \left| \Sigma_\Gauss  \right|^{\frac1d}$. Tomando
  $\Sigma_\Gauss  = I$,  se obtiene  $\forall  \, \theta  > 0,$  \ $\frac{N(X  +
    \sqrt{\theta} \Gauss)  - N(X)}{\theta} \ge  $.  Entonces, tomando  el limite
  $\theta  \to 0$,  aparece que  $\left. \frac{d}{d\theta}  N(X  + \sqrt{\theta}
    \Gauss)   \right|_{\theta  =   0}  \ge   1$.   La   prueba  se   cierra  con
  $\frac{d}{d\theta}  N(X   +  \sqrt{\theta}   \Gauss)  =  \frac{1}{2   \pi  \e}
  \frac{d}{d\theta}  \exp\left( \frac2d  H(X +  \sqrt{\theta} \Gauss)  \right) =
  \frac2d  N(X +  \sqrt{\theta}  \Gauss) \frac{d}{d\theta}  H(X +  \sqrt{\theta}
  \Gauss) = d N(X +  \sqrt{\theta} \Gauss) \Tr\left( J(X + \sqrt{\theta} \Gauss)
  \right)$  (por la identitad  de de  Bruijn).  Ademas,  la igualdad  se obtiene
  cuando se obtiene  la igualdad en la desigualdad de  la potencia entropica, es
  decir cuando $X$ es gaussiano de  varianza proporcional a la del ruido, que es
  la identidad en este caso.
\end{proof}
%
Se  puede  ver  de  nuevo  el  rol  central  que  juega  la  gaussiana  en  esta
desigualdad. Ademas,  de la desigualdad de  Stam se puede  deducir tamb\'ien las
versiones escalares de la desigualdad Cram\'er-Rao. Viene del hecho de que, dado
una matriz de covarianza, le entropia  $H(X)$ es maxima cuando $X$ es gaussiano.
Entonces, para cualquier $X$ de covarianza $\Sigma_X$, $N(X) \le \left| \Sigma_X
\right|^{\frac1d}$,   dando  de   la  desiguldad   de  Stam,   $\left|  \Sigma_X
\right|^{\frac1d} \Tr\left( J(X) \right) \ge d$ (y las otras versiones escalares
de la relaci\'on  determinente-traza).  Como se lo puede  esperar, se obtiene la
igualdad si y solamente $X$  es gaussiana (potencia entropica alcanzando su cota
superior) y de matriz la identidad (desiguladad de Stam se saturando).

Varias otras pruebas de la desigualdad de Stam pueden venir de generalizaciones,
por ejemplo debido a Lutwak o Bercher~\cite{Lut, Ber}. {\color{red} La secci\'on
  ZZZ lo va a rapidamente evocar.}

\

{\color{red}\bf   Existe un data proc ineq con Fisher, cf Rioul 07 ou Stam 59}



% ================================== Mutua =================================== %

\seccion{Unos ejemplos y aplicaciones}
\label{s:SZ:Ejemplos}


% ================================= canal

\subseccion{Canal de transmisi\'on y su capacidad}
\label{ss:SZ:CanalCapacidad}

Siguiendo el  esquema de comunicaci\'on de  Shannon, un mensaje  que se modelisa
como  un vector  aleatorio~\footnote{De  punto  de vista  de  un receptor,  este
  mensaje es  desconocido. Ademas,  se lo  puede ver como  una instancia  de una
  clase  importante   de  posibles  mensajes,   justificando  la  modelisaci\'on
  aleatoria.} $X$  pasa por un  canal de comunicaci\'on  y se recibe  un mensaje
$Y$, vector  aleatorio. En el trabajo de  Shannon, el canal es  supuesto a ruido
additiva,  es decir que  se a\~nade  un ruido  a $X$.   De manera  general, para
conocer la informaci\'on de $X$ que se recibe, se calcula la informaci\'on mutua
$I(X;Y)$, es decir  la cuantidad de informaci\'on que comparten  la entrada y la
salida del canal.   Lo mas $I$ es grande, lo mas  de informaci\'on se transmite.
Dado el canal,  se puede arreglar $X$ (su distribuci\'on)  de manera a maximizar
$I(X;Y)$, es decir la cantidad maxima  que se puede transmitir en este canal. Es
lo que  es conocido como capacidad  del canal~\cite[part.~II~\&~III]{Sha48} (ver
tambi\'en~\cite{CovTho06, Rio07} entre otros):
%
\begin{definicion}[Capacidad de canal]
  Sea un canal de transmici\'on, $X$  su entrada e $Y$ su salida, como ilustrado
  figura~\ref{fig:SZ:CanalComunicacion}.   Sea   $p_X$   la  distribuci\'on   de
  probabilidad  de  $X$. La  capacidad  $C$  del canal  es  definida  por $$C  =
  \max_{p_X} \, I(X;Y)$$
\end{definicion}

\begin{figure}[h!]
  \begin{center}  \input{TEX/ComunicacionShannon}  \end{center} \leyenda{Esquema
    de  comunicaci\'on de  Shannon.   En una  primera  etapa, un  mensaje $M$  a
    transmitir  es  c\'odificado  (ej.  c\'odigo  binario)  o  puesto  en  forma
    (ej.  simbolos modulando una  funci\'on para  que sea  anal\'ogica y  en una
    banda  frecuencial  dada). Sea  $X$  este  mensaje  codificado o  puesto  en
    forma. A la recepci\'on, se mide  $Y$ (ej. versio\'on ruidosa de $X$), antes
    de ser decodificado o usado  para tomar una decisi\'on, $\widehat{M}$ siendo
    la estimaci\'on de  $M$ (ej. simbolos estimados a partir  de $Y$). Una etapa
    importante es el vinculo entre la entrada  $X$ y la salida $Y$ del canal, es
    decir la cantidad de informaci\'on  que tienen en com\'un.  La capacidad del
    canal es la informaci\'on $I(X;Y)$ m\'axima con respeto a su entrada.}
%~\cite{Sha48}.}
\label{fig:SZ:CanalComunicacion}
\end{figure}


% ================================= canal binario

\subsubseccion{Canal binario}

Suponiendo  que el  mensaje  mandado en  un  canal es  una  cadena de  simbolos,
variables   aleatorias  independientes,   se  puede   concentrarse   sobre  cada
simbolo. En  este marco, un  canal de comunicaci\'on  lo mas simple  es conocido
como  {\it canal  binario}~\cite[Sec.~15]{Sha48}: $X$  es una  variable definida
sobre  $\A =  \{ 0  , 1  \}$;  tal tipo  de entrada  es natural,  pensando a  la
c\'odificaci\'on binaria.   La salida $Y$  es tambi\'en definida sobre  $\A$; se
puede imaginar medir y tomar una decisi\'on binaria usando la medida.  Tal canal
es   definido  por  sus   probabilidad  de   transici\'on  $p_{Y|X}$,   \ie  las
probabilidades que un 0 (resp. un 1)  se transmite corectamente o cambia en un 1
(resp. 0),  \ie \ $$p  = \Pr[Y = 1  | X =  0] = 1  - \Pr[Y =  0 | X =  0] \qquad
\mbox{y} \qquad q  = \Pr[Y = 0  | X = 1] =  1 - \Pr[Y =  1 | X = 1]$$  $p$ y $q$
representan    errores    de   comunicaci\'on.     Tal    canal   es    descrito
figura~\ref{fig:SZ:CanalBinario}-(a).   La  figura~\ref{fig:SZ:CanalBinario}-(b)
da un esquema ``f\'isico'' que puede se  al origen de tal canal. Cuando $p = q$,
el canal es conocido como {\it canal binario simetrico}. Cuando $p = 0$ y $q \in
(0 \, ; \, 1)$, el canal es conocido como {\it canal binario en Z}.

\begin{figure}[h!]
\begin{center}\input{TEX/CanalBinario}\end{center}
\leyenda{(a): canal  binario. La entrada  $X$ definida sobre  $\A = \{ 0  , 1\}$
  pasa  por este canal  e $Y$  definida sobre  $\A$ es  recibido. Este  canal es
  caracterisado por  las probabilidades  de transition $p_{Y|X}$.   (b): Esquema
  que puede conducir  al canal binario; una variable puede ser  la salida de una
  puerta  l\'ogica, con  niveles $v_0$  (nivel  bajo, c\'odificando  0) y  $v_1$
  (nivel  alto,  c\'odificando  1).   Se  puede imaginar  que  este  voltaje  es
  transmitido por  un canal  a\~nandido un ruido  $\xi$.  En la  recepci\'on, se
  toma una  decisi\'on, por ejemplo  0 (resp.  1)  si la medida es  mayor (resp.
  menor) que $\eta = \frac{v_0 + v_1}{2}+\Esp[\xi]$.  En este ejemplo, $p$ y $q$
  van a  ser caracterisada completamente por  la distribuci\'on del  ruido (y de
  los deos niveles posibles de la entrada), pero no de la distribuci\'on $p_X$.}
%~\cite{Sha48}.}
\label{fig:SZ:CanalBinario}
\end{figure}

{\color{red}\bf mettre le canal a effacement?}

En este caso, trabajando con bits, el logaritmo lo mas l\'ogico es el de base 2,
dando una capacidad en bits. Sea $$\alpha = \Pr[X = 0]$$ dando la distribuci\'on
de la entrada. La distribuci\'on de la salida va a ser dada a partir de $\beta =
\Pr[Y = 0] = \Pr[Y =  0 | X = 0] \Pr[X = 0] + \Pr[Y = 0 |  X = 1] \Pr[X = 1]$ es
decir $$\beta  = \Pr[Y =  0] =  q + \alpha  (1-p-q)$$ La informaci\'on  mutua se
escribe $I(X;Y) = H(Y) - H(Y|X) = H(Y)  - H(Y|X=0) \Pr[X = 0] + H(Y|X=1) \Pr[X =
1]$, lo que  toma la expresi\'on $$I(X;Y) = h(\beta) -  \alpha h(p) - (1-\alpha)
h(q)$$  donde  $h(\lambda) =  -  \lambda  \log_2  \lambda -  (1-\lambda)  \log_2
(1-\lambda)$ es  la entropia  binaria en bits.  Para calcular la  capacidad $C$,
hace  falta  m\'aximizar $I$  con  respeto  a  $\alpha$.  Diferenciando  $I$  en
$\alpha$,   \ie  $\frac{\partial   I(X;Y)}{\partial  \alpha}   =  \frac{\partial
  h(\beta)}{\partial  \beta}  \frac{\partial \beta}{\partial  \alpha}  - h(p)  +
h(q)$,  es  decir $$\frac{\partial  I(X;Y)}{\partial  \alpha}  = (1-p-q)  \log_2
\left( \frac{1-\beta}{\beta} \right) - h(p) + h(q)$$
%
\begin{itemize}
\item Claramente, $$q =  1-p \quad \Rightarrow \quad C = 0$$  Viene del hecho de
  que  para  $q  =  1-p$,  de  $h(p)  = h(1-p)$  se  deduce  que  $I(X;Y)  =0  $
  constante. De hecho, en  este caso, un 0 en la salida puede venir  de un 0 o 1
  con probabilida igual, y  lo mismo para un 1 en la  salida; en otros terminos,
  la salida  aparece independiente de  la entrada.  Eso se  verifica formalmente
  con $\beta = q$, dando $p_{Y|X}  = p_Y$, dando una informaci\'on mutua cero, y
  entonces una capacidad cero.
%
\item Si $q  \ne 1-p$, la derivada de  $I$ con respeto a $\alpha$  se anula para
  $\beta    =   \beta^\opt$   ($\alpha    =   \alpha^\opt$),    $$\beta^\opt   =
  \frac{1}{1+2^{\frac{h(p)-h(q)}{1-p-q}}}     \qquad     \mbox{siendo}    \qquad
  \alpha^\opt = \frac{\beta^\opt  - q}{1-p-q}$$ y dando un  extremo para $I$.  A
  continuaci\'on,      $\frac{\partial^2       I}{\partial      \alpha^2}      =
  \frac{(1-p-q)^2}{\beta  (1-\beta)}  >  0$   (en  particular  para  le  $\beta$
  ``\'optimo''),  probando  de que  el  extremo  es  un m\'aximo.   Poniendo  el
  $\alpha^\opt$ en la  formula de $I(X;Y)$, luego de  muchos calculos (basicos),
  se  obtiene  $$C =  \log_2\left(  1  +  2^{\frac{h(p)-h(q)}{1-p-q}} \right)  -
  \frac{1-q}{1-p-q} h(p)  + \frac{p}{1-p-q} h(q)$$  Cuando $q \to  1-p$, notando
  que $h(p) =  h(1-p)$ y tomando el  limite de esta formula, se  recupera que $C
  \to 0$.
  %
  % ---
  %
%  \newline{\color{red}\bf   Interpretacion?    no   hay   combinacion   convexa
%    $\frac{p}{1-p-q}$ siendo del mismo signo que $\frac{1-q}{1-p-q}$; nota: $C =
%    \log_2\left(  1  + 2^{-  \frac{h(1-q)-h(p)}{(1-q)-p}}  \right) +  \frac{1}{p
%      (1-q)} \frac{(1-q) h(1-q) - p h(p)}{q - (1-p)}$}
\end{itemize}
%
\noindent De \ $I(X;Y) =  H(Y) - H(Y|X) \le H(Y) \le 1$ bit  \ ($Y$ es binario, de
entropia maxima en  el caso uniforme), aparece sin calculos que  $$C \le 1$$ \ie
la capacidad es menor que 1  bit~\footnote{De manera general, de la escritura de
  $I$  con entropias condicionales,  para $X$  definido sobre  $\A$ e  $Y$ sobre
  $\B$, da $0 \le C \le \min( \log  |\A| \, , \, \log |\B| )$. Ademas, $p_{Y|X}$
  depende solo  del canal  y no  de la entrada,  as\'i que  para $p_X  = \lambda
  p_X^{(1)}  + (1-\lambda)  p_X^{(2)}$ se  obtiene  $p_Y =  \lambda p_Y^{(1)}  +
  (1-\lambda) p_Y^{(2)}$ con $p_Y^{(i)}$  salida corespondiente a $p_X^{(i)}$ de
  $I(X;Y) = H(Y)-H(Y|X)$,  el secundo termino dependiente solo  del canal, de la
  concavidad de  $H$ se obtiene  que $I$ es  concava con respeto a  $p_X$. $p_X$
  parteciendo a  un convexo, $I$  tiene un m\'aximo, \'unico.}:  para transmitir
informaci\'on en  este canal, hace  falta introducir redundancia en  el mensaje.
Se alcanza  $C = 1$ si, (i)  por un lado $H(Y|X)  = 0$, es decir  $\alpha h(p) +
(1-\alpha) h(q)  = 0$ y (ii) ademas  $h(\beta) = 1$.  Estudiando  cada caso (ej.
con $\alpha = 0$ y $q = 0$ se satisface (i) pero no (ii) porque $\beta = 0$), se
obtiene  que  $$C =  1  \qquad \Leftrightarrow  \qquad  \alpha  = \frac12  \quad
\mbox{y}  \quad p  = q  = \frac{1  \pm 1}{2}$$  Para $p  = q  = 0$  el  canal es
perfecto,  mientras que  para  $p  = q  =  1$ el  canal  es  llamado {\it  canal
  volteando}; en ambos casos, se  recupera la entrada (o directamente, o tomando
el opuesto) ``sin perdida''.

La figura~\ref{fig:SZ:ICanalBinario} representa  la informaci\'on mutua $I(X;Y)$
para unos canales  ($p$ y $q$ dados)  en funci\'on de $\alpha$.  Se  nota que la
curva   es   concava  y   tiene   un   m\'aximo,   capacidad  del   canal.    La
figura~\ref{fig:SZ:CCanalBinario} representa la capacidad del canal en funci\'on
de \ $p$ \ y \ $q$ as\'i que unos casos particulares/cortes.

En el caso particular $p = q$, conocido como {\it canal simetrico}, la capacidad
es $$C = 1 - h(p)$$ (alcanzada con una entrada uniforme).  Como visto en el caso
general, la capacidad vale 1 bit si y  solamente si $h(p) = 0$, es decir $p = 0$
o $p  = 1$.  Al rev\'es,  la capacidad es  m\'inima cuando $H$ est  m\'aximo, es
decir  para  $p =  q  = \frac12$,  y  $C  = 0$  (instancia  particular  de $q  =
1-p$). $h(p)$ es  la perdida en bit para cada bit  transmitido. La capacidad $C$
en funci\'on de $p$ es dada figura~\ref{fig:SZ:CCanalBinario}-(b).

En el  caso particular $p  = 0$,  conocido como {\it  canal en Z},  la capacidad
es $$C = \log_2\left( 1 +  2^{- \frac{h(q)}{1-q}} \right)$$ Se nota en este caso
tambi\'en que  la capacidad alcanza 1,  su m\'aximo, si  y solamente si $q  = 0$
(canal perfecto). Al rev\'es, cuando $q  \to 1$, $C \to 0$, instancia particular
de   $q   =   1-p$.  La   capacidad   $C$   en   funci\'on   de  $q$   es   dada
figura~\ref{fig:SZ:CCanalBinario}-(c).

%{\bf\color{red} C parecido a la de Shannon caso continuo. Interpretacion?}

\begin{figure}[h!]
\begin{center}\input{TEX/ICanalBinario}\end{center}
\leyenda{Informaci\'on  mutua  entrada-salida  $I(X;Y)$  del  canal  binario  en
  funci\'on de $\alpha = \Pr[X = 0]$. (a): $p = 0.4$ \ y \ $q = 0.01$; \ (b): $p
  = q = 0.05$ (canal simetrico); \ (c): $p = 0$ \ y \ $q = 0.05$ (canal en Z).}
\label{fig:SZ:ICanalBinario}
\end{figure}

\

\begin{figure}[h!]
\begin{center}\input{TEX/CCanalBinario}\end{center}
\leyenda{Capacidad  $C$ del canal  binario. (a):  en funci\'on  de \  $p$ \  y \
  $q$. \ (b): en  funci\'on de $p$ para el canal simetrico ($p  = q$); \ (c): en
  funci\'on de \ $q$ \ para \ $p = 0$ (canal en Z).}
\label{fig:SZ:CCanalBinario}
\end{figure}

En~\cite{CovTho06,  Rio07}   entre  otros,  se  estudia   diversos  otros  canal
discretos, binario o con mas estados.


{\color{red}  Canal a effacement?}


% ================================= canal gaussiano

\subseccion{Canal de transmisi\'on continuo gaussiano y su capacidad}
\label{ss:SZ:Canalgaussiano}

Un canal de  comunicaci\'on continuo relativamente simple es  conocido como {\it
  canal  gaussiano}~\cite[Sec.~25]{Sha48},~\cite{CovTho06,  Rio07}:  $X$ es  una
variable continua  definida sobre $\D \subseteq  \Rset^d$ y la salda  $Y$ es una
versi\'on ruidosa de $X$, \ie $Y =  X + \xi$ con el ruido $\xi$ independiente de
$X$; En  el canal gaussiano, $\xi  \equiv \Gauss$ es un  vector gaussiano.  Este
canal es tambi\'en definido por  su densidad de probabilidad ``de transici\'on''
$p_{Y|X}$,  \ie  por  la  distribuci\'on  del  ruido.   Tal  canal  es  descrito
figura~\ref{fig:SZ:CanalGaussiano}.  Se supone  conicida la matriz de covarianza
$\Sigma_\Gauss$ del ruide,  y se nota $\Sigma_X$ la de  la entrada. En practica,
no se puede mandar un mensaje a una  potencia tan alta que se quierre, lo que se
traduce  por  una limitaci\'on  $$\frac1d  \Tr\left(  \Sigma_X  \right) \le  P$$
potencia limite permitida por componente (sampleo).

\begin{figure}[h!]
\begin{center}\input{TEX/CanalGaussiano}\end{center}
\leyenda{Canal gaussiano. La entrada $X$,  modelisada por un vector aleatorio es
  corrupto aditivamente por un ruido gaussiano $\Gauss$ independiente de $X$. La
  salida es entonces $Y  = X + \Gauss$ y el canal  es completamente descrito por
  $p_{Y|X}(x,y)   =    p_{\Gauss}(y-x)$   (obviamente   independiente    de   la
  distribuci\'on de la entrada).}
\label{fig:SZ:CanalGaussiano}
\end{figure}


Por  definici\'on, la informaci\'on  mutua $I(X;Y)$  entrada-salida es  dada por
$I(X;Y) = H(Y) - H(Y|X) = H(Y) - H(\Gauss)$. Maximizar $I(X;Y)$ es equivalente a
maximizar  $H(Y) =  H(X  + \Gauss)$  sujeto  a $\Tr\left(  \Sigma_X \right)  \le
P$. Fijando un  $\Sigma_X$, la propiedad~\ref{prop:SZ:cotamaximagaussiana} de le
entropia diferencial  implica que $H(Y)$  sea maximal si  y solamente si  $Y$ es
gaussiana, es decir si y solamente si $X$ est gaussiana, dando $I(X;Y) = \frac12
\log \left| \Sigma_X + \Sigma_\Gauss \right| - \frac12 \log \left| \Sigma_\Gauss
\right|$.  En conclusi\'on,  tomando  en cuenta  el  limite de  potencia, $$C  =
\max_{\frac1d \Tr   \Sigma_X  \le  P} \:  \frac12   \log\left(  \frac{\left|   \Sigma_X  +
      \Sigma_\Gauss \right|}{\left| \Sigma_X \right|} \right)$$

{\color{red} Reste a resoudre matriciel~\cite[Sec.~9.4]{CovTho06} p. 277}

En   el   caso   escalar,   se   obtiene   $$C  =   \frac12   \log\left(   1   +
  \frac{P}{\sigma_\Gauss^2}   \right)$$  donde   $\frac{P}{\sigma_\Gauss^2}$  es
conocido como relaci\'on se\~nale-ruido~\footnote{Esta formula es muy parecida a
  la   de   Shannon,    Laplume,   Clavier~\cite{Sha48,   Lap48,   Cla48}   (ver
  tambi\'en~\cite[Sec.~9.3]{CovTho06} o~\cite[Sec.~11.2]{Rio07}).   De hecho, si
  se considera simbolos mandandos durante $T$ secundos (simbolos puesto en forma
  para dar una se\~nal analogica) usando  una banda de transmisi\'on $B$, por el
  teorema de Nyquist $B = \frac{1}{2 T}$ (caso limite). Si el ruido es blanco en
  la banda $B$, de densidad espectral de potencia por unidad de frecuencia igual
  a $N_0$, para un simbolo  la relaci\'on se\~nal-ruido se escribe $\frac{P}{N_0
    B}$. Ademas,  se calcula  en general  la capacidad por  unidad de  tiempo es
  decir la  capacidad por  simbolo divido  por $T$, \ie  $C =  B \log\left(  1 +
    \frac{\sigma_X^2}{N_0 B}  \right)$ por secundos,  lo que es  precisamente la
  capacidad  calculdada por Shannon.  Esa es  a veces  conocida como  formula de
  Shannon-Hartley.}

En~\cite{CovTho06,  Rio07}  por ejemplo,  se  dan  otros  ejemplos de  canal  de
comunicaci\'on en el contexto continuo.

% ================================= codificacion

\subseccion{Codificaci\'on entropica}
\label{ss:SZ:Codificacion}


% ================================= Gaz perfecto

\subseccion{Gas perfecto}
\label{ss:SZ:Gaz perfecto}

{\color{red} Va donne lien avec Boltzmann}


% ============================== Generalizadas================================ %

\seccion{Entropias y divergencias generalizadas}
\label{s:SZ:Generalizadas}

% ================================= Salicru

\subseccion{Entropias y propiedades}
\label{ss:SZ:Salicru}

{\bf  Salicru, Buerbea-Rao,  poner  ac\'a la  codificaci\'on  a la  Renyi, y  la
  cuantificacion fina; EPI generalizada  por Madiman, etc. Lutwak, Bercher etc.,
  Kagan}

% ================================= Salicru

\subseccion{Divergencias y propiedades}
\label{ss:SZ:Czizar}

{\bf Czizar, Bregmann, Burbea Rao}


% =============================== Cuanticas ================================== %

\seccion{Entropias cuanticas discretas}
\label{s:SZ:Cuanticas}

{\bf Mas alla caso de informaciones a partir de medida; caso infinito, continuo queda en discusiones}


% ================================= Entropia ================================= %




