\SZ{Poner antes las notationes $P_d^+(\Rset), \quad S_d(\Rset)$

\


  En la parte vectores aleatorios,  aparte el caso complejo, poner unas palabras
  sobre el caso matrix-variate general, y con simetrias

Poner la forma de la covarianza $\Esp[X  \otimes X] - m_X \otimes m_X$ que tiene
todos  los terminos de  covarianza $\Cov[X_{i,j},X_{k,l}]$  (coefficente $(k,l)$
del bloc $(i,j)$. Poner la  forma de la funcion caracter\'istica $\Phi_X(\Omega)
= \Esp \left[ e^{\imath \Tr\left( \Omega  X \right)} \right]$; Se puede salir de
la vectorizaci\'on~\cite{Har08} de \ $X$,  \ie poniendo las columnas una bajo la
otra, y  usar la definici\'on usual. Tomando  en cuenta la s\'imetria  de \ $X$,
eso es  equivalente a definirla como  \ $\Phi_X(\omega) =  \Esp \left[ e^{\imath
    \Tr\left(  \Omega  X \right)}  \right]$  \  con  \ $\Omega  \in  S_d(\Rset)$
conjunto de matrices reales s\'imetricas de $\Rset^{d \times d}$~\cite{PedRic91,
  And03};

Promedio: Para calcularlas, lo m\'as sencillo es salir de la funci\'on
caracter\'istica y ver que, con las simetrias, \ $\frac{\partial
\Phi_X}{\partial \Omega_{i,j}} = \imath (2-\un_{\{i\}}(j)) \Esp\left[ X_{i,j}
\right]$, y usar las reglas de derivaci\'on matricial~\cite[Cap.~8]{MagNeu99}.

Covarianza: Para calcularlas, en el caso e simetrias, lo m\'as sencillo es salir de nuevo de la
funci\'on caracter\'istica y ver que, con las simetrias, \ $\frac{\partial^2
\Phi_X}{\partial \Omega_{i,j} \partial \Omega_{k,l}} = - (2-\un_{\{i\}}(j))
(2-\un_{\{k\}}(l)) \Esp[X_{i,j} X_{k,l}]$ \ y usar las reglas de derivaci\'on
matricial~\cite[Cap.~8]{MagNeu99}

\

 $\otimes$ es el producto de  Kronecker, $A \otimes B$ \ es matriz bloc
de bloc \ $(i,j)$-\'esima \ $A_{i,j} B$.
%, $J$ \ la matriz bloc de
%bloc  $(i,j)$-\'esima \  $\un_j  \un_i^t$  \ y  $K$  \ la  matriz  bloc de  bloc
%$(i,j)$-\'esima \ $\un_i \un_j^t$.
}

\seccion{Algunos ejemplos de distribuciones de probabilidad}
\label{Sec:MP:EjemplosDistribucionesProb}


\emph{introducci\'on...}

\SZ{Citar los libroz de Kotz entre otros, mi HDR...}

% ================================= Variables discretas
\subseccion{Distribuciones de variable discreta}
\label{Ssec:MP:EjemplosDistribucionesDiscretas}


% --------------------------------- Certeza


\subsubseccion{Variable con certeza}
\label{Sssec:MP:Certeza}


El caso \ $X = a$ \ deterministico ($\forall \, \omega, \: X(\omega) = a$) puede
ser ver visto  como un caso degenerado de  variable/vector aleatorio. Visto como
vector  aleatorio,  sus  caracter\'isticas  principales  vistas  en  las  secciones
anteriores son resimudas en la tabla siguiente:
%
\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on & $\X = \{ a \}$\\[2mm]
\hline
%
Distribuci\'on de probabilidad & $p_X(x) = \un_{\{a\}}(x)$\\[2mm]
\hline
%%
%%Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
%%\hline
%%
%%Momento factorial & $\Esp\left[ (X)_k \right] = ?$\\[2mm]
%%\hline
%%
Promedio & $\displaystyle m_X = a$\\[2mm]
\hline
%
Varianza & $\displaystyle \Sigma_X = 0$\\[2mm]
\hline
%%
Asimetr\'ia & $\gamma_X = 0$\\[2mm]
\hline
%
Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = -3$\\[2mm]
\hline
%
Generadora de probabilidad & $\displaystyle G_X(z) = z^a$ \ para \ $z \in \Cset$
\ si $a \ge 0$ \ y \ $\Cset^*$ \ si no\\[2mm]
\hline
%
Generadora de momentos & $\displaystyle M_X(u) = e^{a u}$ \ para \ $u \in
\Cset$\\[2mm]
\hline
%
Funci\'on caracter\'istica & $\displaystyle \Phi_X(\omega) = e^{\imath a
\omega}$\\[2mm]
\hline
\end{tabular}
\end{center}
%

La funci\'on de masa y funci\'on de repartici\'on son representadas en la figura
Fig.~\ref{Fig:MP:Certeza}.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Certeza} \end{center}
% 
\leyenda{Ilustraci\'on  de una  distribuci\'on  cierta (a),  y  la funci\'on  de
  repartici\'on asociada (b).}
\label{Fig:MP:Certeza}
\end{figure}



% --------------------------------- Uniforme discreta
\subsubseccion{Ley Uniforme sobre un ``intervalo'' de $\Zset$}
\label{Sssec:MP:UniformeDiscreta}

Se denota $X \, \sim \, \U\{ a \, ,  \, b \}$ \ con $(a,b) \in \Zset^2, \: b \ge
a$.  Las caracter\'isticas de \ $X$ \ son las siguientes:

\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Parametros & $(a,b) \in \Zset^2, \: b \ge a$\\[2mm]
\hline
%
Dominio de definici\'on & $\X = \{ a  \, , \, a+1 \,  , \, \ldots \, ,  \, b \}$\\[2mm]
\hline
%
Distribuci\'on de probabilidad & $p_X(x) = \frac1{b-a+1}$\\[2mm]
\hline
%%
%%Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
%%\hline
%%
%%Momento factorial & $\Esp\left[ (X)_k \right] = ?$\\[2mm]
%%\hline
%%
Promedio & $\displaystyle m_X = \frac{a+b}{2}$\\[2mm]
\hline
%
Varianza & $\displaystyle \Sigma_X = \frac{(b-a) (b-a+2)}{12}$\\[2mm]
\hline
%%
Asimetr\'ia & $\gamma_X = 0$\\[2mm]
\hline
%
Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = -\frac65 \frac{(b-a)
(b-a+2)+2}{(b-a) (b-a+2)}$\\[2mm]
\hline
%%
Generadora de probabilidad & $\displaystyle G_X(z) = \frac{z^a-z^{b+1}}{1-z}$ \
para~\footnote{En el caso l\'imite \ $z \to 1$, \ $\lim_{z \to 1} \frac{ z^a -
z^{b+1}}{1-z} = b+1-a$} \ $z \in \Cset$ \ si $a \ge 0$ \ y \ $\Cset^*$ \ si
no\\[2mm]
\hline
%%
Generadora de momentos & $\displaystyle M_X(u) = \frac{ e^{a u} - e^{(b+1)
u}}{1-e^u}$ \ para~\footnote{En el caso l\'imite \ $u \to 0$, \ $\lim_{u \to 0}
\frac{ e^{a u} - e^{(b+1) u}}{1-e^u} = b+1-a$, y similarmente para la funci\'on
caracter\'istica.}  \ $u \in \Cset$\\[2mm]
\hline
%
Funci\'on caracter\'istica & $\displaystyle  \Phi_X(\omega) = \frac{ e^{\imath a
\omega} - e^{\imath (b+1) \omega}}{1-e^{\imath \omega}}$\\[2mm]
\hline
\end{tabular}
\end{center}
%
%% modo 0
%% Mediana \ln(2)/\lambda
%%% CDF 1-e^{-\lambda x}

La distribuci\'on  de masa de probabilidad  y funci\'on de  repartici\'on de una
variable uniforme  \ $\U\{  a \, ,  \, b  \}$ \ son  representadas en  la figura
Fig.~\ref{Fig:MP:UniformeDiscreta}.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/UniformeDiscreta} \end{center}
% 
\leyenda{Ilustraci\'on  de  una densidad  de  probabilidad  uniforme  (a), y  la
  funci\'on  de repartici\'on  asociada (b).  $a =  1,  \: b  = 6$  \ (ej.  dado
  equilibriado).}
\label{Fig:MP:UniformeDiscreta}
\end{figure}

Cuando \ $b = a$, la variable tiende a una variable cierta \ $X = a$.

La  distribuci\'on  uniforme  aparece  por   ejemplo  en  el  tiro  de  un  dado
equilibriado con \ $a = 1, \: b = 6$.
%% en el conteo de
%%conteo de une repetici\'on de  una experiencia de maneja independiente hasta que
%%occure un evento de probabilidad $p$; por ejemplo el n\'umero de tiro de un dado
%%equilibriado hasta que occurre un ``6'' sigue una ley geometrica de parametro $p
%%= \frac16$.


% --------------------------------- Bernoulli

\subsubseccion{Ley de Bernoulli}
\label{Sssec:MP:Bernoulli}

Se  denota \  $X \,  \sim \,  \B(p)$ \  con \  $p \in  [0 \;  1]$ \  y sus
caracter\'isticas son las siguientes:

\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on & $\X = \{ 0 \, , \, 1 \}$\\[2mm]
\hline
%
Parametro & $p \in [ 0 \; 1 ]$\\[2mm]
\hline
%
Distribuci\'on de probabilidad & $p_X(1) = 1 - p_X (0) = p$\\[2mm]
\hline
%
%Momentos & $ \Esp\left[ X^k \right] = p^k\\[2mm]
%\hline
%
%Momento factorial & $\Esp\left[ (X)_k \right] = p^k \un_{\{0 \, , \, 1 \}}(k)$\\[2mm]
%\hline
%
Promedio & $ m_X = p$\\[2mm]
\hline
%
Varianza & $\sigma_X^2 = p \, (1-p)$\\[2mm]
\hline
%
Asimetr\'ia & $\displaystyle \gamma_X =  \frac{1 - 2 \, p}{\sqrt{p \, (1-p)}}$\\[2mm]
\hline
%
Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = \frac{1 - 6 \, p + 6
\, p^2}{p \, (1-p)}$\\[2mm]
\hline
%
Generadora de probabilidad & $G_X(z) = 1 - p + p z$ \ sobre \ $\Cset$\\[2mm]
\hline
%
Generadora de momentos & $M_X(u) = 1 - p + p \, e^u$ \ sobre \ $\Cset$\\[2mm]
\hline
%
Funci\'on caracter\'istica & $\Phi_X(\omega) = 1 - p + p \, e^{\imath \omega}$\\[2mm]
\hline
\end{tabular}
\end{center}

Su masa  de probabilidad  y funci\'on de  repartici\'on son representadas  en la
figura Fig.~\ref{Fig:MP:Bernoulli}.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Bernoulli} \end{center}
%
\leyenda{Ilustraci\'on de una distribuci\'on de probabilidad de Bernoulli (a), y
  la funci\'on de repartici\'on asociada (b), con $p = \frac13$.}
\label{Fig:MP:Bernoulli}
\end{figure}

Nota que cuando $p = 0$ (resp. $p =  1$) la variable es cierta $X = 0$ (resp. $X
= 1$).

La  ley de Bernoulli tiene una propiedad de reflexividad trivial:
%
\begin{lema}[Reflexividad]
\label{Lem:MP:ReflexividadBernoulli}
%
  Sea \ $X \, \sim \, \B(p)$. Entonces
  %
  \[
  1-X \, \sim \, \B(1-p)
  \]
  %
\end{lema}

\SZ{Ref. Bernoulli}


% --------------------------------- Binomial

\subsubseccion{Ley Binomial}
\label{Sssec:MP:Binomial}

Se denota \ $X \, \sim \, \B(n,p)$ \ con \ $n \in \Nset \setminus \{ 0 \, , \, 1
\}, \quad p \in [0 \; 1]$ \ y sus caracter\'isticas son las siguientes:

\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on & $\X = \{ 0 \, , \, \ldots \, , \, n \}$\\[2mm]
\hline
%
Parametros & $n  \in \Nset \setminus \{0  \, , \, 1 \},  \quad p \in [0  \;
1]$\\[2mm]
\hline
%
Distribuci\'on  de  probabilidad  &  $\displaystyle  p_X(k)  =  \bino{n}{k}  p^k
(1-p)^{n-k}$\\[2mm]
\hline
%
%Momentos & $ \Esp\left[ X^k \right] = ??\\[2mm]
%\hline
%
%Momento factorial & $\Esp\left[ (X)_k \right] = \frac{n!}{(n-k)!} p^k \un_{\{ 0 \, , \, \ldots \, , \, n \}}(k)$\\[2mm]
%\hline
%
Promedio & $ m_X = n \, p$\\[2mm]
\hline
%
Varianza & $\sigma_X^2 = n \, p \, (1-p)$\\[2mm]
\hline
%
Asimetr\'ia & $\displaystyle \gamma_X = \frac{1 - 2 \, p}{\sqrt{n \, p \, (1-p)}}$\\[2mm]
\hline
%
Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = \frac{1 - 6 \, p + 6 \, p^2}{n \, p
\, (1-p)} $\\[2mm]
\hline
%
Generadora  de probabilidad  &  $\displaystyle  G_X(z) =  \left(  1 -  p  + p  z
\right)^n$ \ sobre \ $\Cset$\\[2mm]
\hline
%
Generadora  de momentos  &  $\displaystyle  M_X(u) =  \left(1  - p  +  p \,  e^u
\right)^n$ \ sobre \ $\Cset$\\[2mm]
\hline
%
Funci\'on caracter\'istica  & $\displaystyle \Phi_X(\omega) =  \left( 1 -  p + p
\, e^{\imath \omega} \right)^n$\\[2mm]
\hline
\end{tabular}
\end{center}
%
\noindent   con  \   $\displaystyle  \bino{n}{k}   =  \frac{n!}{k!   (n-k)!}$  \
coefficiente binomial.
% Modo $\left\lfloor (n+1) p \right\rfloor$
% Mediana $\left\lfloor n p \right\rfloor$ o $\left\lceil n p \right\rceil
% CDF	$I_{1-p}(n-k,k+1)$ regularized incomplete beta function

Su masa  de probabilidad  y funci\'on de  repartici\'on son representadas  en la
figura Fig.~\ref{Fig:MP:Binomial}.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Binomial} \end{center}
%
\leyenda{Ilustraci\'on de una distribuci\'on  de probabilidad Binomial (a), y la
  funci\'on de repartici\'on asociada (b), con $n = 6, \quad p = \frac13$.}
\label{Fig:MP:Binomial}
\end{figure}
\SZ{Otros ilustraciones para otros $p$?}

Cuando  $n  = 1$,  se  recupera  la lei  de  Bernoulli  $\B(p) \equiv  \B(1,p)$.
Ad\'emas, se muestra  sencillamente usando la generadora de  probabilidad que
%
\begin{lema}
\label{Lem:BinomilaSumaBernoulli}
%
  Sean \  $X_i \,  \sim \, \B(p),  \quad i  = 1, \ldots  , n$  \ independientes,
  entonces
  %
  \[
  \sum_{i=1}^n X_i \, \sim \, \B(n,p)
  \]
\end{lema}
%
De este resultado,  se puede notar que, por  ejemplo, le distribuci\'on binomial
aparece en el conteo de eventos independientes de misma probabilidad entre $n$.

Tambi\'en,  la ley binomial  tiene una  propiedad de  reflexividad, consecuencia
directa de la de Bernoulli:
%
\begin{lema}[Reflexividad]
\label{Lem:MP:ReflexividadBinomial}
%
  Sea \ $X \, \sim \, \B(n,p)$. Entonces
  %
  \[
  n-X \, \sim \, \B(n,1-p)
  \]
  %
\end{lema}

Nota que cuando $p = 0$ (resp. $p = 1$) la variable es cierta $X = 0$ (resp.  $X
= n$).



% --------------------------------- Binomial negativa
\subsubseccion{Ley Binomial negativa}
\label{Sssec:MP:BinomialNegativa}

Se denota \ $X \, \sim \, N\B(r,p)$ \  con \ $r \in \Nset^*, \quad p \in [0 \; 1)$ \ y sus caracter\'isticas son las siguientes:

\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on & $\X = \Nset$\\[2mm]
\hline
%
Parametros & $r  \in \Nset^*,  \quad p \in [0  \;
1)$\\[2mm]
\hline
%
Distribuci\'on  de  probabilidad  &  $\displaystyle  p_X(k)  =  \bino{k+r-1}{k}  p^k
(1-p)^r$\\[2mm]
\hline
%
%Momentos & $ \Esp\left[ X^k \right] = ??\\[2mm]
%\hline
%
%Momento factorial & $\Esp\left[ (X)_k \right] = \frac{(r+k-1)!}{(r-1)!} \left( \frac{p}{1-p} \right)^k$\\[2mm]
%\hline
% %
Promedio & $\displaystyle m_X = \frac{r \, p}{1-p}$\\[2mm]
\hline
%
Varianza & $\displaystyle \sigma_X^2 = \frac{r \, p}{(1-p)^2}$\\[2mm]
\hline
%
Asimetr\'ia & $\displaystyle \gamma_X = \frac{1 + p}{\sqrt{r \, p}}$\\[2mm]
\hline
%
Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = \frac{1 + 4 \, p +
p^2}{r \, p} $\\[2mm]
\hline
%
Generadora  de probabilidad  &  $\displaystyle  G_X(z) =  \left(  \frac{1 -  p}{1  - p \, z}
\right)^r$ \ para \ $|z| < p^{-1} $\\[2mm]
\hline
%
Generadora de momentos & $\displaystyle M_X(u) = \left( \frac{1 - p}{1 - p \,
e^u } \right)^r$ \ para \ $\real{u} < - \ln p$\\[2mm]
\hline
%
Funci\'on caracter\'istica & $\displaystyle \Phi_X(\omega) = \left( \frac{1 -
p}{1 - p \, e^{i \omega} } \right)^r$\\[2mm]
\hline
\end{tabular}
\end{center}
%
% % Modo $\left\lfloor (n+1) p \right\rfloor$
% % Mediana $\left\lfloor n p \right\rfloor$ o $\left\lceil n p \right\rceil
% % CDF	$I_{1-p}(n-k,k+1)$ regularized incomplete beta function

Su masa  de probabilidad  y funci\'on de  repartici\'on son representadas  en la
figura Fig.~\ref{Fig:MP:BinomialNegativa}.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/BinomialNegativa} \end{center}
%
\leyenda{Ilustraci\'on de  una distribuci\'on de  probabilidad binomial negativa
  (a), y  la funci\'on  de repartici\'on  asociada (b), con  $r =  3, \quad  p =
  \frac35$.}
\label{Fig:MP:BinomialNegativa}
\end{figure}
\SZ{Otros ilustraciones para otros $r, p$?}

Esta ley aparece cuando se repite una experencia binaria \ $X_i \in \{ 0 \, , \,
1 \},  i = 1, \ldots$  \ con \ $P(X_i=1)  = p$ \ de  manera independiente ($X_i$
independientes) hasta  que \  $r$ \  variables valen 0,  con \  $r$ \  fijo. Los
n\'umeros de  excitos \ $X_i =  1$ \ sigue una  ley \ $N\B(r,p)$  (el calculo es
directo).  Dicho de otra  manera, $X = \sum_{i=1}^N X_i$ \ con  \ $N$ \ variable
aleatoria tal que $X_N = 0$ \ y \ $r = \sum_{i=1}^N (1-X_i)$: condicionalmente a
\ $N$, la variable es binomial de parametro $p$.  Se puede ver que \ $P(N = n) =
\bino{n}{r-1} (1-p)^r  p^{n-r}$ \ y la  ley de la binomial  negativa se recupera
tambi\'en,          por         ejemplo,         a          trav\'es         del
teorema~\ref{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}. En el caso \ $r = 1$ \
aparece que \ $N \sim \G(1-p)$ \ y\ldots tambi\'en \ $X \sim \G(1-p)$.
%
% Blaise PAscal - Polya caso r real

Esta distribuci\'on se  generaliza para \ $r \in \Rset_+^*$ \  pero se pierde la
interpretaci\'on que v\'imos en el p\'arafo anterior.

Nota: cuando \ $p = 0$ \ la variable es cierta \ $X = r$.



% --------------------------------- Multinomial
\subsubseccion{Ley Multinomial}
\label{Sssec:MP:Multinomial}

Esta ley es una generalizaci\'on de la ley binomial y aparece por ejemplo cuando
se  repite  una  experiencia  a  \  $k$  \ estados  \  $n$  \  veces  de  manera
independiente y nos  interesamos a la probabilidad que  el primer evento aparece
$n_1$ veces, el secundo $n_2$ veces, \ldots.  Se denota \ $X \ \sim \ \M(n,p)$ \
con  \  $n   \in  \Nset^*$  \  y   \  $p  =  \begin{bmatrix}  p_1   &  \cdots  &
  p_k  \end{bmatrix}^t \in \Simp_{k-1}$  \ the  \ $(k-1)$-simplex  estandar (ver
figure~\ref{Fig:MP:Dirichlet}-(a)),
%
\[
\Simp_{k-1} = \left\{ x \in [ 0 \; 1 ]^k,  \: \sum_{i=1}^k x_i = 1 \right\}
\]
%
Entonces, a pesar de que se escribe \ $X$ \ de manera $k$-dimensional, el vector
partenece a una variedad claramente \ $d =  k-1$ \ dimensional y en el caso \ $k
= 2$  \ se recupera  la ley binomial.   Las caracter\'isticas de  \ $X \  \sim \
\M(n,p)$ \ son las siguientes:

\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on~\footnote{De hecho, se puede considerar que el vector
aleatorio es \ $(k-1)$-dimensional \ $\widetilde{X} = \begin{bmatrix}
\widetilde{X}_1 & \cdots & \widetilde{X}_{k-1} \end{bmatrix}^t$ \ definido sobre
el dominio \ $\widetilde{\X} = \left\{ m \in \{ 0 \, , \, \ldots \, , \,
n\}^{k-1}, \: \sum_{i=1}^{k-1} m_i \le n
\right\}$.\label{Foot:MP:MultinomialDominio}} & $\X = \left\{ m \in \{ 0 \, , \,
\ldots \, , \, n\}^k, \: \sum_{i=1}^k m_i = n \right\}$\\[2mm]
\hline
%
Parametros & $n \in \Nset^*, \quad p \in  \Simp_{k-1} = \left\{ p \in [ 0 \; 1
]^k, \: \sum_i p_i = 1 \right\}$\\[2mm]
\hline
%
Distribuci\'on de probabilidad~\footnote{Tratando de $\widetilde{X}$, su masa de
probabilidad es dada por \ $p_{\widetilde{X}}(m) = \frac{n!}{\prod_{i=1}^k m_i!}
\prod_{i=1}^{k-1} p_i^{m_i} \, \left( 1 - \sum_{i=1}^{k-1} p_i
\right)^{n-\sum_{i=1}^{k-1} m_i}$.\label{Foot:MP:MultinomialMasa}} &
$\displaystyle p_X(m) = \frac{n!}{\prod_{i=1}^k m_i!}  \prod_{i=1}^k
p_i^{m_i}$\\[2mm]
\hline
%
%Momentos & $ \Esp\left[ X^k \right] = ??\\[2mm]
%\hline
%
%Momento factorial & $\Esp\left[ (X)_k \right] = \frac{(r+k-1)!}{(r-1)!} \left( \frac{p}{1-p} \right)^k$\\[2mm]
%\hline
% %
Promedio & $\displaystyle m_X = n \, p$\\[2mm]
\hline
%%
Covarianza & $\displaystyle \Sigma_X = n \left( \diag p - p p^t \right)$\\[2mm]
\hline
%%
%Asimetr\'ia & $\displaystyle \gamma_X = \frac{1 + p}{\sqrt{r \, p}}$\\[2mm]
%\hline
%%
%Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = \frac{1 + 4 \, p +
%p^2}{r \, p} $\\[2mm]
%\hline
%%
Generadora de probabilidad & $\displaystyle G_X(z) = \left( p^t z
%\sum_{j=1}^k p_j z_j
\right)^n$ \ para \ $z \in \Cset^k$\\[2mm]
\hline
%%
Generadora de momentos & $\displaystyle M_X(u) = \left( p^t e^u
% \sum_{j=1}^k p_j e^{u_j}
\right)^n, \: e^u = \begin{bmatrix} e^{u_1} & \cdots & e^{u_k} \end{bmatrix}^t$  \ para \ $u \in \Cset^k$\\[2mm]
\hline
%
Funci\'on caracter\'istica & $\displaystyle \Phi_X(\omega) = \left( p^t e^{\imath \omega}
%\sum_{j=1}^k p_j e^{\imath \omega_j} 
\right)^n$\\[2mm]
\hline
\end{tabular}
%\footnotetext{De  hecho,  se puede  considerar  que  el  vector aleatorio  es  \
%  $(k-1)$-dimensional  \  $\widetilde{X}  =  \begin{bmatrix}  \widetilde{X}_1  &
%    \cdots & \widetilde{X}_{k-1} \end{bmatrix}^t$  \ definido sobre el dominio \
%  $\widetilde{\X} =  \left\{ m \in  \{ 0 \,  , \, \ldots  \, , \,  n\}^{k-1}, \:
%    \sum_{i=1}^k m_i  \le n \right\}$.   Su masa de  probabilidad es dada  por \
%  $p_{\widetilde{X}}(m)   =  \frac{n!}{\prod_{i=1}^k   m_i!}   \prod_{i=1}^{k-1}
%  p_i^{m_i}  \,  \left(  1  - \sum_{i=1}^{k-1}  p_i  \right)^{n-\sum_{i=1}^{k-1}
%    m_i}$.}
\end{center}
%
% % Modo $\left\lfloor (n+1) p \right\rfloor$
% % Mediana $\left\lfloor n p \right\rfloor$ o $\left\lceil n p \right\rceil
% % CDF	$I_{1-p}(n-k,k+1)$ regularized incomplete beta function

Es sencillo  ver que  cada componente  \ $X_j$ \  es binomial,  $X_j \,  \sim \,
\B(n,p_j)$.

Su masa  de probabilidad  y funci\'on de  repartici\'on son representadas  en la
figura Fig.~\ref{Fig:MP:Multinomial}.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Multinomial} \end{center}
%
\leyenda{Ilustraci\'on de una distribuci\'on  de probabilidad multinomial para \
  $k   =  3$   \   del   vector  \   $(k-1)$-dimensional   \  $\widetilde{X}   =
  \protect\begin{bmatrix}   X_1  &  X_2   \protect\end{bmatrix}^t$  \   ($X_3  =
  1-X_1-X_2$)  \ con  las marginales  \ $p_{X_1},  \: p_{X_2}$  \ (ver  notas de
  pie~\ref{Foot:MP:MultinomialDominio}   y~\ref{Foot:MP:MultinomialMasa}).   Los
  otros parametros son  \ $n = 5$  \ y \ $p =  \protect\begin{bmatrix} \frac25 &
    \frac13     &    \frac4{15}     \protect\end{bmatrix}^t$    (a),     $p    =
  \protect\begin{bmatrix} \frac13  & \frac12 &  \frac16 \protect\end{bmatrix}^t$
  (b).}
\label{Fig:MP:Multinomial}
\end{figure}


\SZ{Otros ilustraciones para otros $n, p$?}


% --------------------------------- Geometrica

\subsubseccion{Ley Geom\'etrica}
\label{Sssec:MP:Geometrica}

Se  denota  \  $X \,  \sim  \,  \G(p)$  \  con \  $p  \in  (0  \;  1]$ \  y  sus
caracter\'isticas son las siguientes:

\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on & $\X = \Nset^*$\\[2mm]
\hline
%
Parametro & $p \in (0 \; 1]$\\[2mm]
\hline
%

Distribuci\'on  de  probabilidad &  $\displaystyle  p_X(k)  =  (1-p)^{k-1} p$  \
(convenci\'on $0^0 = 1$)\\[2mm]
\hline
%
%Momentos & $ \Esp\left[ X^k \right] = ?$\\[2mm]
%\hline
%
%Momento factorial & $\Esp\left[ (X)_k \right] = \frac{p^{k-1} k!}{(1-p)^k}$\\[2mm]
%\hline
%
Promedio & $m_X = \frac1p$\\[2mm]
\hline
%
Varianza & $\displaystyle \sigma_X^2 = \frac{1-p}{p^2}$\\[2mm]
\hline
%
Asimetr\'ia & $\displaystyle \gamma_X = \frac{2-p}{\sqrt{1-p}}$\\[2mm]
\hline
%
Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = \frac{6 - 6 \, p + p^2}{1-p}$\\[2mm]
\hline
%
Generadora de  probabilidad & $\displaystyle  G_X(z) = \frac{p z}{1-(1-p)  z}$ \
para \ $|z| < \frac1{1-p}$\\[2mm]
\hline
%
Generadora de  momentos & $\displaystyle M_X(u)  = \frac{p \, e^u}{1  - (1-p) \,
e^u}$ \ para \ $\real{u} < - \ln(1-p)$\\[2mm]
\hline
%
Funci\'on caracter\'istica  & $\displaystyle \Phi_X(\omega)  = \frac{p \, e^{\imath
\omega}}{1 - (1-p) \, e^{\imath \omega}}$\\[2mm]
\hline
\end{tabular}
\end{center}
%
%\noindent con $\bino{n}{k} = \frac{n!}{k! (n-k)!}$ coefficiente binomial.
% Modo 1
% Mediana $\left\lceil \frac{-1}{\log_2(1-p)} \right\rceil$ 
% CDF	$1-(1-p)^k$

Su masa  de probabilidad  y funci\'on de  repartici\'on son representadas  en la
figura Fig.~\ref{Fig:MP:Geometrica}.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Geometrica} \end{center}
%
\leyenda{Ilustraci\'on de una distribuci\'on de probabilidad Geom\'etrica (a), y
  la funci\'on de repartici\'on asociada (b), con $p = \frac13$.}
\label{Fig:MP:Geometrica}
\end{figure}
\SZ{Otros ilustraciones para otros $p$?}

Esta distribuci\'on  aparece en el conteo  de conteo de une  repetici\'on de una
experiencia de maneja  independiente hasta que occure un  evento de probabilidad
$p$; por ejemplo  el n\'umero de tiro de un dado  equilibriado hasta que occurre
un ``6'' sigue una ley geom\'etrica de parametro $p = \frac16$.

Nota que cuando \  $p =  1$ \ la variable es cierta \  $X = 1$.   


\SZ{?`Que propiedad mas?}

% --------------------------------- Poisson

\subsubseccion{Ley de Poisson}
\label{Sssec:MP:Poisson}

Se denota $X \,  \sim \, \P(\lambda)$ \ con \ $\lambda  \in \Rset_+^*$ \ llamada
{\em taza}, y sus caracter\'isticas son las siguientes:

\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on & $\X = \Nset$\\[2mm]
\hline
%
Parametro & $\lambda \in \Rset_+^*$\\[2mm]
\hline
%
Distribuci\'on  de  probabilidad   &  $\displaystyle  p_X(k)  =  \frac{\lambda^k
e^{-\lambda}}{k!}$\\[2mm]
\hline
%
%Momentos & $ \Esp\left[ X^k \right] = ?$\\[2mm]
%\hline
%
%Momento factorial & $\Esp\left[ (X)_k \right] = \lambda^k$\\[2mm]
%\hline
%
Promedio & $ m_X = \lambda$\\[2mm]
\hline
%
Varianza & $\sigma_X^2 = \lambda$\\[2mm]
\hline
%
Asimetr\'ia & $\displaystyle \gamma_X = \frac1{\sqrt\lambda}$\\[2mm]
\hline
%
Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = \frac1\lambda$\\[2mm]
\hline
%
Generadora de probabilidad & $\displaystyle G_X(z) = e^{\lambda (z-1)}$ \ para \
$z \in \Cset$\\[2mm]
\hline
%
Generadora  de momentos  & $\displaystyle  M_X(u) =  e^{\lambda \left(  e^u  - 1
\right)}$ \ para \ $u \in \Cset$\\[2mm]
\hline
%
Funci\'on  caracter\'istica  &  $\displaystyle  \Phi_X(\omega) =  e^{\lambda  \,
\left( e^{\imath \omega} - 1 \right)}$\\[2mm]
\hline
\end{tabular}
\end{center}
%
% modo \lfloor \lambda \rfloor 
% Mediana \approx \lfloor \lambda +1/3-0.02/\lambda \rfloor 
% CDF {\frac {\Gamma
% (\lfloor k+1\rfloor  ,\lambda )}{\lfloor k\rfloor !}} where  $\Gamma (x,y)$ is
% the upper incomplete gamma function,

Su masa  de probabilidad  y funci\'on de  repartici\'on son representadas  en la
figura Fig.~\ref{Fig:MP:Poisson}.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Poisson} \end{center}
%
\leyenda{Ilustraci\'on de  una distribuci\'on de probabilidad de  Poisson (a), y
  la funci\'on de repartici\'on asociada (b), con $\lambda = 3$.}
\label{Fig:MP:Poisson}
\end{figure}
\SZ{Otros ilustraciones para otros $\lambda$?}

Ad\'emas, se muestra  sencillamente usando la generadora de  probabilidad que
%
\begin{lema}[Stabilidad]
\label{Lem:MP:StabilidadPoisson}
%
  Sean  \  $X_i  \,  \sim  \,  \P(\lambda_i),  \quad  i  =  1,  \ldots  ,  n$  \
  independientes, entonces
  %
  \[
  \sum_{i=1}^n X_i \, \sim \, \P\left( \sum_{i=1}^n \lambda_i \right)
  \]
\end{lema}

Cuando $\lambda = 0$ la variable es  cierta $X = 0$ (usando la convenci\'on $0^0
= 1$).  \SZ{ Esta distribuci\'on aparece... eventos despues tiempo exponencial; conteo. Ej en fisica. Ecuacion del telegrafo}
% en el conteo de
%conteo de une repetici\'on de  una experiencia de maneja independiente hasta que
%occure un evento de probabilidad $p$; por ejemplo el n\'umero de tiro de un dado
%equilibriado hasta que occurre un ``6'' sigue una ley geometrica de parametro $p
%= \frac16$.


\aver{
%%%%%%%

...

Estad\'istica  de  los  n\'umeros   de  ocupaci\'on  de  niveles  energ\'eticos:
distribuciones de Maxwell--Boltzmann, de Fermi--Dirac, y de Bose--Einstein

...


Leyes de los grandes n\'umeros


}


% ================================= Variables continuas

\subseccion{Distribuciones de variable continua}
\label{Ssec:MP:EjemplosDistribucionescontinuas}

\SZ{$\sigma \to 0$ caso cierto}


% --------------------------------- uniforme escalar

\subsubseccion{Distribuci\'on uniforme sobre un intervalo}
\label{Sssec:MP:UniformeContinua}

Se denota $X \, \sim \, \U([a \; b])$. Las caracter\'isticas de \ $X$ \ son las
siguientes:

\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on & $\X = [a \; b]$\\[2mm]
\hline
%
Parametros & $(a,b) \in \Rset, \: b > a$\\[2mm]
\hline
%
Densidad de probabilidad & $p_X(x) = \frac{1}{b-a}$\\[2mm]
\hline
%
%Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
%\hline
%
%Momento factorial & $\Esp\left[ (X)_k \right] = ?$\\[2mm]
%\hline
%
Promedio & $\displaystyle m_X = \frac{a+b}{2}$\\[2mm]
\hline
%
Varianza & $\displaystyle \Sigma_X = \frac{(b-a)^2}{12}$\\[2mm]
\hline
%
Asimetr\'ia & $\gamma_X = 0$\\[2mm]
\hline
%
Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = -\frac65$\\[2mm]
\hline
%
%Generadora de probabilidad & $G_X(z) = e^{\lambda (z-1)}$ \ para \ $z \in \Cset$\\[2mm]
%\hline
%
Generadora de momentos & $\displaystyle M_X(u) = \frac{ e^{b u} - e^{a u}}{u}$ \
para~\footnote{En el caso l\'imite \ $u \to  0$, \ $\lim_{u \to 0} \frac{ e^{b u}
- e^{a u}}{u} = b-a$, y similarmente para la funci\'on caracter\'istica}  \ $u \in \Cset^d$\\[2mm]
\hline
%
Funci\'on caracter\'istica & $\displaystyle  \Phi_X(\omega) = \frac{ e^{\imath a
\omega} - e^{\imath b \omega}}{\omega}$\\[2mm]
\hline
\end{tabular}
%\footnotetext{En el caso l\'imite \ $u \to  0$, \ $\lim_{u \to 0} \frac{ e^{b u}
%- e^{a u}}{u} = b-a$.}
\end{center}
%
% modo 0
% Mediana \ln(2)/\lambda
% CDF 1-e^{-\lambda x}

Obviamente, se puede escribir \ $X \, \egald  \, a + (b-a) U$ \ donde \ $\egald$
\ significa que la equalidad es en distribuci\'on (las variables tienen la misma
distribuci\'on de probabilidad), con \ \ $U \, \sim \, \U \left( [ 0 \; 1 ]
\right)$ \ llamada {\em uniforme estandar}.

La densidad de probabilidad y funci\'on de repartici\'on de la variable estandar
son representadas en la figura Fig.~\ref{Fig:MP:Uniformecontinua}.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/UniformeContinua} \end{center}
% 
\leyenda{Ilustraci\'on  de  una densidad  de  probabilidad  uniforme  (a), y  la
funci\'on de repartici\'on asociada (b).}
\label{Fig:MP:Uniformecontinua}
\end{figure}

De manera  general, para  cualquier ensemble $\D  \subset \Rset^d$ de  volumen \
$|\D|$ \,  la variable uniforma sobre $\D$  tiene la densidad con  respecto a la
medida  ``natural'' sobre  $\D$  (Lebesque, discreta,\ldots)  constante sobre  \
$\D$,
%
\[
p_X(x) = \frac{1}{|\D|} \un_{\D}(x)
\]
%
La media va a ser el centro de gravedad de $\D$.

%Cuando $\lambda \to +\infty$ la variable tiende a una variable cierta $X = 0$.
\SZ{Esta distribuci\'on aparece..., propiedades}
% en el conteo de
%conteo de une repetici\'on de  una experiencia de maneja independiente hasta que
%occure un evento de probabilidad $p$; por ejemplo el n\'umero de tiro de un dado
%equilibriado hasta que occurre un ``6'' sigue una ley geometrica de parametro $p
%= \frac16$.



% % --------------------------------- uniforme producto cartesiano

% \subsubseccion{Distribuci\'on uniforme sobre un producto cartesiano de intervalos}

% Se denota $X  \, \sim \, \U(\D)$ \  con, en este caso, \  $\D = \optimes_{i=1}^d
% [a_i \, ; \, b_i] \subset \Rset^d$.   De nuevo, se puede escribir \ $X \, \egald
% \,  a +  \diag(b-a)  U$ \  donde  \ $a  =  \begin{bmatrix} a_1  &  \cdots &  a_d
% \end{bmatrix}^d$, \ $b = \begin{bmatrix} b_1 & \cdots & b_d \end{bmatrix}^d$ \ y
% \ $U \, \sim \, \U \left( [ 0  \, ; \, 1 ]^d \right)$ \ {\em uniforme estandar}.
% Las caracteristicas de  \ $X \, \sim \, \U  \left( [ 0 \, ; \,  1 ]^d \right)$ \
% son  las  siguientes  (se  deducen   para  cualquier  uniforme  sobre  $\D$  por
% transformaci\'on lineal; ver secciones anteriores):

% \begin{center}
% \begin{tabular}
% {
% |>{\vspace{-2mm}}p{.35\textwidth}|
% >{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
% }
% %
% \hline
% %
% Dominio de definici\'on & $\D = [0 \, ; \, 1 ]^d$\\[2mm]
% \hline
% %
% Densidad de probabilidad & $p_X(x) = 1$\\[2mm]
% \hline
% %
% %Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
% %\hline
% %
% Promedio & $\displaystyle m_X = \frac12 \begin{bmatrix} 1 & \cdots & 1
% \end{bmatrix}^t$\\[2mm]
% \hline
% %
% Covarianza & $\displaystyle \Sigma_X = \frac1{12} \, I$\\[2mm]
% \hline
% %
% %Generadora de probabilidad & $G_X(z) = e^{\lambda (z-1)}$ \ para \ $z \in \Cset$\\[2mm]
% %\hline
% %
% Generadora de momentos & $\displaystyle  M_X(u) = \prod_{k=1}^d \frac{ e^{u_k} -
% 1}{u_k}$ \  para~\footnote{Con el  l\'imite $\lim_{u_k \to  0} \frac{  e^{u_k} -
% 1}{u_k} = 1$.}  \ $u \in \Cset^d$\\[2mm]
% \hline
% %
% Funci\'on  caracter\'istica   &  $\displaystyle  \Phi_X(\omega)   =  (-\imath)^d
% \prod_{k=1}^d \frac{ e^{\imath \omega_k} - 1}{\omega_k}$\\[2mm]
% \hline
% \end{tabular}
% \end{center}
% %
% % modo 0
% % Mediana \ln(2)/\lambda
% % CDF 1-e^{-\lambda x}

% \SZ{Hacer el caso $d = 2$}

% %Cuando $\lambda \to +\infty$ la variable tiende a una variable cierta $X = 0$.
% \SZ{Esta distribuci\'on aparece..., propiedades}
% % en el conteo de
% %conteo de une repetici\'on de  una experiencia de maneja independiente hasta que
% %occure un evento de probabilidad $p$; por ejemplo el n\'umero de tiro de un dado
% %equilibriado hasta que occurre un ``6'' sigue una ley geometrica de parametro $p
% %= \frac16$.



% --------------------------------- Exponencial

\subsubseccion{Distribuci\'on exponencial}
\label{Sssec:MP:Exponencial}

Se denota $X \,  \sim \, \E(\lambda)$ \ con \ $\lambda  \in \Rset_+^*$ \ llamada
{\em  taza}  (inversa  de  {\em   escala}),  y  sus  caracter\'isticas  son  las
siguientes:

\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on & $\X = \Rset_+$\\[2mm]
\hline
%
Parametro & $\lambda \in \Rset_+^*$\\[2mm]
\hline
%
Densidad  de probabilidad &  $\displaystyle p_X(x)  = \lambda  e^{-\lambda x}$\\[2mm]
\hline
%
%Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
%\hline
%
%Momento factorial & $\Esp\left[ (X)_k \right] = ?$\\[2mm]
%\hline
%
Promedio & $\displaystyle m_X = \frac1\lambda$\\[2mm]
\hline
%
Varianza & $\displaystyle \sigma_X^2 = \frac1{\lambda^2}$\\[2mm]
\hline
%
Asimetr\'ia & $\gamma_X = 2$\\[2mm]
\hline
%
Curtosis por exceso & $\widebar{\kappa}_X = 6$\\[2mm]
\hline
%
%Generadora de probabilidad & $G_X(z) = e^{\lambda (z-1)}$ \ para \ $z \in \Cset$\\[2mm]
%\hline
%
Generadora de  momentos &  $\displaystyle M_X(u) =  \frac{\lambda}{\lambda-u}$ \
para \ $\real{u} < \lambda$\\[2mm]
\hline
%
Funci\'on     caracter\'istica     &     $\displaystyle     \Phi_X(\omega)     =
\frac{\lambda}{\lambda - \imath \omega}$\\[2mm]
\hline
\end{tabular}
\end{center}
%
% modo 0
% Mediana \ln(2)/\lambda
% CDF 1-e^{-\lambda x}

Su densidad  de probabilidad  y funci\'on de  repartici\'on son representadas  en la
figura Fig.~\ref{Fig:MP:Exponencial}.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Exponencial} \end{center}
% 
\leyenda{Ilustraci\'on  de una densidad  de probabilidad  exponencial (a),  y la
funci\'on de repartici\'on asociada (b), con $\lambda = 1.5$.}
\label{Fig:MP:Exponencial}
\end{figure}
\SZ{Poner escalas; Otros ilustraciones para otros $\lambda$?}

Cuando $\lambda \to +\infty$ la variable tiende a una variable cierta $X = 0$.
\SZ{Esta distribuci\'on aparece... Propiedades}
% en el conteo de
%conteo de une repetici\'on de  una experiencia de maneja independiente hasta que
%occure un evento de probabilidad $p$; por ejemplo el n\'umero de tiro de un dado
%equilibriado hasta que occurre un ``6'' sigue una ley geometrica de parametro $p
%= \frac16$.


\SZ{
% --------------------------------- Laplace
\subsubseccion{Distribuci\'on de Laplace o doble exponencial}
\label{Sssec:MP:Laplace}
}


% --------------------------------- Gaussiana

\subsubseccion{Distribuci\'on normal o Gaussiana multivariada real}
\label{Sssec:MP:Gaussiana}

Se denota $X \, \sim \, \N(m,\Sigma)$ \  con \ $m \in \Rset^d$ \ y \ $\Sigma \in
P_d^+(\Rset)$  \  conjunto  de  las   matrices  de  \  $\Rset^{d  \times  d}$  \
s\'imetricas definidas positivas. Las  caracter\'isticas de la Gaussiana son las
siguientes:

\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on & $\X = \Rset^d$\\[2mm]
\hline
%
Parametros & $m \in \Rset^d, \: \Sigma \in P_d^+(\Rset)$\\[2mm]
\hline
%
Densidad de probabilidad & $\displaystyle p_X(x) = \frac{1}{(2
\pi)^{\frac{d}{2}} \left| \Sigma \right|^{\frac12}} \, e^{-\frac12 (x-m)^t
\Sigma^{-1} (x-m)}$\\[2.5mm]
\hline
%
%Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
%\hline
%
%Momento factorial & $\Esp\left[ (X)_k \right] = ?$\\[2mm]
%\hline
%
Promedio & $ m_X = m$\\[2mm]
\hline
%
Covarianza & $\Sigma_X = \Sigma$\\[2mm]
\hline
%
Asimetr\'ia (caso escalar) & $\gamma_X = 0$\\[2mm]
\hline
%
Curtosis por exceso (caso escalar) & $\widebar{\kappa}_X = 0$\\[2mm]
\hline
%
%Generadora de probabilidad & $G_X(z) = e^{\lambda (z-1)}$ \ para \ $z \in \Cset$\\[2mm]
%\hline
%
Generadora de  momentos &  $\displaystyle M_X(u) =  e^{u^t \Sigma u + u^t m}$  \ para \  $u \in
\Cset^d$\\[2mm]
\hline
%
Funci\'on  caracter\'istica   &  $\displaystyle  \Phi_X(\omega)   =  e^{-\frac12
\omega^t \Sigma \omega + \imath \omega^t m}$\\[2mm]
\hline
\end{tabular}
\end{center}
%
% modo 0
% Mediana 0
Nota: trivialmente, se puede escribir $X  \, \egald \, \Sigma^{\frac12} N + m$ \
con \ $N \, \sim \, \N(0,I)$ \ donde \  $I$ \ es la identidad y \ $N$ \ es dicha
{\em Gausiana estandar} o {\em centrada-normalizada}. Las caracter\'isticas de \
$X$ \  son v\'inculadas  a las de  \ $N$  \ (y vice-versa)  por transformaci\'on
afine (ver secciones anteriores).


La densidad de probabilidad gausiana y  la funci\'on de repartici\'on en el caso
escalar son  representadas en la figura Fig.~\ref{Fig:MP:Gaussiana}-(a)  y (b) y
una      densidad      en       un      contexto      bi-dimensional      figura
Fig.~\ref{Fig:MP:Gaussiana}(c).
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Gaussiana} \end{center}
% 
\leyenda{Ilustraci\'on  de  una   densidad  de  probabilidad  gaussiana  escalar
  estandar  (a), y la  funci\'on de  repartici\'on asociada  (b), as\'i  que una
  densidad  de probabilidad  gaussiana bi-dimensional  centrada y  de  matriz de
  covarianza \ $\Sigma_X = R(\theta)  \Delta^2 R(\theta)^t$ \ con \ $R(\theta) =
  \protect\begin{bmatrix}   \cos\theta  &   -  \sin\theta\\[2mm]   \sin\theta  &
    \cos\theta  \protect\end{bmatrix}$ \  matriz  de rotaci\'on  y  \ $\Delta  =
  \diag\left(\protect\begin{bmatrix}  1   &  a\protect\end{bmatrix}  \right)$  \
  matriz  de   cambio  de  escala,   y  sus  marginales   \  $X_1  \,   \sim  \,
  \N\left(0,\cos^2\theta  + a^2  \sin^2\theta \right)$  \ y  \ $X_2  \,  \sim \,
  \N\left(0,\sin^2\theta + a^2 \cos^2\theta  \right)$ \ (ver m\'as adelante). En
  la figura, $a = \frac14$ \ y \ $\theta = \frac{\pi}{6}$.}
\label{Fig:MP:Gaussiana}
\end{figure}

%\SZ{Poner los ticks de axis detras}
%\begin{figure}[h!]
%\begin{center} \input{TIKZ_MP/Gaussiana2D} \end{center}
%% 
%\leyenda {Ilustraci\'on de una densidad de probabilidad gaussiana bi-dimensional
%  centrada  y  de  matriz  de   covarianza  \  $\Sigma_X  =  R(\theta)  \Delta^2
%  R(\theta)^t$  \ con  \  $R(\theta) =  \protect\begin{bmatrix}  \cos\theta &  -
%    \sin\theta\\[2mm] \sin\theta & \cos\theta \protect\end{bmatrix}$ \ matriz de
%  rotaci\'on   y   \    $\Delta   =   \diag\left(\protect\begin{bmatrix}   1   &
%      a\protect\end{bmatrix}  \right)$  \ matriz  de  cambio  de  escala, y  sus
%  marginales  \  $X_1  \,  \sim  \, \N\left(0,\cos^2\theta  +  a^2  \sin^2\theta
%  \right)$  \ y  \ $X_2  \, \sim  \, \N\left(0,\sin^2\theta  +  a^2 \cos^2\theta
%  \right)$ \ (ver m\'as  adelante). En la figura, $a = \frac14$  \ y \ $\theta =
%  \frac{\pi}{6}$.}
%\label{Fig:MP:Gaussiana2D}
%\end{figure}

La gaussiana tiene un par de propiedades particulares:
%
\begin{teorema}[Stabilidad]
\label{Teo:MP:StabilidadGaussiana}
%
  Sean \ $A_i , i = 1,\ldots,n$ \  matrices de \ $\Rset^{d' \times d}, d' \le d$
  \ de rango lleno, $b_i \in \Rset^{d'}$ \ y \ $X_i \, \sim \, \N(m_i,\Sigma_i)$
  \ independientes, entonces
  %
  \[
  \sum_{i=1}^n \left(  A_i X_i  + b_i \right)  \, \sim \,  \N\left( \sum_{i=1}^n
    \left( m_i + b_i \right) \, , \, \sum_{i=1}^n A_i \Sigma_i A_i^t \right)
  \]
  % 
  En particular, cualquier combinaci\'on lineal  de los componentes de un vector
  Gaussiano da una gaussiana.  Reciprocamente, si cualquier combinaci\'on lineal
  de los componentes de un vector aleatorio sigue una ley gaussiana, entonces el
  vector es gaussiano.
\end{teorema}
%
\begin{proof}
  Este  resultato se proba  usando funci\'on  caracter\'istica de  la gaussiana,
  conjuntalmente al teorema~\ref{Teo:MP:PropiedadesFuncionCaracteristica}.
\end{proof}

%
\begin{teorema}[Independencia]
\label{Teo:MP:IndependenciaGaussiana}
%
  Sea   \   $X  \,   \sim   \,   \N(m,\Delta)$  \   con   \   $\Delta  =   \diag
  \left(  \begin{bmatrix}  \sigma_1^2  &  \cdots  &  \sigma_d^2  \end{bmatrix}^t
  \right)$   \  diagonal.   Entonces  las   componentes  \   $X_i  \,   \sim  \,
  \N(m_i,\sigma_i^2)$ \ son independientes.
\end{teorema}
%
\begin{proof}
  Este resultato se proba  trivialmente escribiendo la densidad de probabilidad,
  notando que se factorisa.
\end{proof}
%
Hemos visto que cuando un  vector tiene componentes independientes, la matriz de
covarianza  es   diagonal  (lema~\ref{Lem:MP:IndependenciaCov}),  pero   que  la
reciproca es falsa en general. El \'ultimo teorema muestra que la reciproca vale
en el caso gausiano.

%Cuando $\lambda \to +\infty$ la variable tiende a una variable cierta $X = 0$.
\SZ{Esta distribuci\'on aparece...}
% en el conteo de
%conteo de une repetici\'on de  una experiencia de maneja independiente hasta que
%occure un evento de probabilidad $p$; por ejemplo el n\'umero de tiro de un dado
%equilibriado hasta que occurre un ``6'' sigue una ley geometrica de parametro $p
%= \frac16$.


\SZ{
% --------------------------------- Gaussiana complejas
\subsubseccion{Distribuci\'on normal o Gaussiana multivariada complejas}
\label{Sssec:MP:GaussianaComplejas}
caso general y caso circular
}


% --------------------------------- Gamma

\subsubseccion{Distribuci\'on Gamma}
\label{Sssec:MP:Gamma}

Se  denota $X \,  \sim \,  \G(a,b)$ \  con \  $a \in  \Rset_+^*$ \  llamado {\em
parametro de  forma} \ y \  $b \in \Rset_+^*$  \ llamada {\em taza}  (inversa de
{\em escala}).  Las caracter\'isticas son:

\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on & $\X = \Rset_+$\\[2mm]
\hline
%
Parametros & $a \in \Rset_+^*$ \ (forma), \: $b \in \Rset_+^*$ \ (taza)\\[2mm]
\hline
%
Densidad  de probabilidad  &  $\displaystyle p_X(x)  =  \frac{b^a \, x^{a-1} \,  e^{-b
x}}{\Gamma(a)}$\\[2mm]
\hline
%
%Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
%\hline
%
%Momento factorial & $\Esp\left[ (X)_k \right] = ?$\\[2mm]
%\hline
%
Promedio & $\displaystyle m_X = \frac{a}{b}$\\[2mm]
\hline
%
Varianza & $\displaystyle \sigma_X^2 = \frac{a}{b^2}$\\[2mm]
\hline
%
Asimetr\'ia & $\displaystyle \gamma_X = \frac2{\sqrt{a}}$\\[2mm]
\hline
%
Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = \frac6{a}$\\[2mm]
\hline
%
%Generadora de probabilidad & $G_X(z) = e^{\lambda (z-1)}$ \ para \ $z \in \Cset$\\[2mm]
%\hline
%
Generadora  de momentos  & $\displaystyle  M_X(u) =  \left( 1  - \frac{u}{b}
\right)^{-a}$ \ para \ $\real{u} < b$\\[2mm]
\hline
%
Funci\'on  caracter\'istica  &  $\displaystyle   \Phi_X(\omega)  =  \left(  1  -
\frac{ \imath \omega}{b} \right)^{-a}$\\[2mm]
\hline
\end{tabular}
\end{center}
%
% modo max(a-1,0)
% Mediana no close ver inverse gamma
$\displaystyle  \Gamma(a) =  \int_{\Rset_+} x^{a-1}  \, e^{-x}  \, dx$  \  es la
funci\'on Gamma~\cite{AbrSte70, AndAsk99, GraRyz15}.

Nota: trivialmente, se puede escribir $X \,  \egald \, \frac{1}{b} G$ \ con \ $G
\, \sim \, \G(a,1)$  \ donde \ $G$ \ es estandardizada  o normalizada. De nuevo,
las  caracter\'isticas  de \  $X$  \  son  v\'inculadas a  las  de  \ $G$  \  (y
vice-versa) por transformaci\'on afine (ver secciones anteriores).

Una densidad de probabilidad gamma  y la funci\'on de repartici\'on asociada son
representadas en  la figura Fig.~\ref{Fig:MP:Gamma} para  varios $a$ \ y  \ $b =
1$.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Gamma} \end{center}
%
\leyenda{Ilustraci\'on de una densidad de probabilidad gamma (a), y la funci\'on
de  repartici\'on asociada  (b).   $b  = 1$  \  y \  $a  = 0.5$  (linea
punteada), $1$ (linea mixta), $2$ (linea guionada) y $3$ (linea llena).}
\label{Fig:MP:Gamma}
\end{figure}

Nota  que para  \  $X \,  \sim  \, \G(1,b)$  \ es  una  variable exponencial  de
parametro \  $b$, \ie \ $X \,  \sim \, \E(b)$. Cuando  \ $a < 1$,  la densidad \
$p_X$  \ diverge  para \  $x  \to 0$  \ (divergencia  integrable). Adem\'as,  se
muestra tambi\'en sencillamente con las funciones caracter\'isticas que:
%
\begin{lema}[Stabilidad]
\label{Lem:MP:StabilidadGamma}
%
  Sean $X_i  \, \sim  \, \G\left( a_i  , b  \right), \: i  = 1 ,  \ldots ,  n$ \
  independientes. Entonces
  % 
  \[
  \sum_{i=1}^n X_i \, \sim \, \G\left( \sum_{i=1}^n a_i \, , \, b \right)
  \]
\end{lema}

Adem\'as,  se muestra  sencillamente  por  cambio de  variables  y la  funci\'on
caracter\'istica un v\'inculo con variables gausianas:
%
\begin{lema}[V\'inculo con la gaussiana]
\label{Lem:MP:VinculoGammaGaussiana}
%
  Sean $X_i \, \sim \,  \N\left( 0 , \sigma^2 \right), \: i = 1  , \ldots , n$ \
  independientes. Entonces
  %
  \[
  \sum_{i=1}^n  X_i^2 \,  \sim \,  \G\left( \frac{n}{2}  \, ,  \,  \frac{1}{2 \,
      \sigma^2} \right)
  \]
\end{lema}

La distribuci\'on Gamma aparece entre  otros en problema de inferencia Bayesiana
como distribuci\'on a priori conjugado~\footnote{En la inferencia Bayesiana, nos
  interesamos al paremetro (posiblemente  multivariado) \ $\theta$ \ subyancente
  a una  distribuci\'on. Por ejemplo,  sabemos tener observaciones  sorteados de
  una distribuci\'on de Poisson, pero con el parametro \ $\lambda$ \ desconocido
  y nos interesamos a \ $\theta \equiv \lambda$. El enfoque Bayesiano consiste a
  considerar el paremetro  \ $\Theta$ \ aleatorio, tal  que la distribuci\'on de
  las observaciones  sea vista como distribuci\'on condicional  \ $p_{X|\Theta =
    \theta}(x)$, llamada distribuci\'on de sampleo. Dados las observaciones $X =
  x$,  la  meta  es  de  determinar  la  distribuci\'on  dicha  a  posteriori  \
  $p_{\Theta|X =  x}$ \ a  partir de  la cual se  puede hacer estimaci\'on  de \
  $\theta$ dados  las observaciones, calcular intervalos de  confianza, etc. Por
  eso,  el metodo  se  apoya  sobre la  regla  de Bayes  $p_{\Theta|X=x}(\theta)
  \propto  p_{X|\Theta=\theta}(x)  p_\Theta(\theta)$  \  as\'i que  se  necesita
  elegir  una distribuci\'on  \ $p_\Theta$  \  dicha a  priori.  Una  elecci\'on
  posible es  tomarla en una familia  parametrizada tal que  la distribuci\'on a
  posterior  partenece  tambi\'en a  esta  familia: es  lo  que  se llama  prior
  conjugado. La idea es que si  vienen observaciones, en lugar de re-calcular el
  posterior,   se   puede   actualizar   solamente  los   parametros   (llamados
  hiperparametros).    \SZ{Ver    nota    de    pie    en    el    cap    2    a
    modificar}.\label{Foot:MP:BayesPrior} } del parametro $\lambda$ de la ley de
Poisson~\cite{Rob07}.

\SZ{Esta distribuci\'on aparece...}
% en el conteo de
%conteo de une repetici\'on de  una experiencia de maneja independiente hasta que
%occure un evento de probabilidad $p$; por ejemplo el n\'umero de tiro de un dado
%equilibriado hasta que occurre un ``6'' sigue una ley geometrica de parametro $p
%= \frac16$.



% --------------------------------- Wishart
\subsubseccion{Distribuci\'on matriz-variada de Wishart}

Este ejemplo es una  generalizaci\'on matriz-variada de la distribuci\'on gamma.
Se puede ver  una matriz como un vector, guardando por  ejemplo sus columnas una
bajo la  precediente.  Sin embargo, tal  distribuci\'on apareciendo naturalmente
en un contexto de estimaci\'on de  matriz de covarianza (ver m\'as adelante), es
m\'as  natural  verla  matriz-variate.    Tal  distrubuci\'on  es  debido  a  J.
Wishart~\cite{Wis28, GupNag99, And03}, y se denota \ $X \, \sim \, W_d(V,\nu)$ \
donde  el  dominio  de  definici\'on  es  \  $P_d^+(\Rset)$,  conjunto  matrices
simetricas definida positivas, $V \in P_d^+(\Rset)$ parametro de escala y \ $\nu
> d-1$ \ grados de libertad.  Las caracter\'isticas de la distribuci\'on son las
siguientes:

\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on~\footnote{De hecho, se puede considerar que la matriz
aleatoria es equivalent a tener un vector \ $\frac{d (d+1)}{2}$-dimensional; por
la simetria, claramente \ $X$ \ tiene solamente \ $\frac{d (d+1)}{2}$ \
componentes diferentes; adem\'as, se puede probar que cualquier matriz \ $A \in
P_d^+(\Rset)$ \ se descompone bajo la forma \ $A = L L^t$ \ con \ $L$ \
triangular inferior con elementos no nulos sobre su diagonal, llamado
descomposici\'on de Cholesky~\cite{GupNag99, Bha07, Har08, HorJoh13} y
reciprocamente. Eso muestra que \ $A$ \ se define a partir de \ $\frac{d
(d+1)}{2}$ \ ``grados de libertad''.\label{Foot:MP:WishartXtilde}} & $\X =
P_d^+(\Rset), \: d \in \Nset^*$\\[2mm]
\hline
%
Parametros & $V \in  P_d^+(\Rset)$ (escala) y \ $\nu > d-1$ \ (grados de libertad)\\[2mm]
\hline
%%
Densidad de probabilidad~\footnote{La densidad de probabilidad es dada con
respeto a la \SZ{medida de Lebesgue restricta} a \ $P_d^+(\Rset) \subset \Rset^{d
\times d}$.\label{Foot:MP:WishartDensidad}} & $\displaystyle p_X(x) =
\frac{|x|^{\frac{\nu-d-1}{2}} \, e^{-\frac12 \Tr\left( V^{-1}
x\right)}}{2^{\frac{d \nu}{2}} \, |V|^{\frac{\nu}{2}} \, \Gamma_d\left(
\frac{\nu}{2} \right)}$\\[2mm]
\hline
%%
%%Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
%%\hline
%%
%%Momento factorial & $\Esp\left[ (X)_k \right] = ?$\\[2mm]
%%\hline
%%
Promedio & $\displaystyle m_X = \nu \, V$\\[2mm]
\hline
%%
Covarianza & $\displaystyle \Sigma_X = \nu \big( J (V \otimes V) + (V \otimes I)
K (V \otimes I) \big)$\\[2mm]
% $\Cov[X_{i,j},X_{k,l}] = nu \left( V_{i,k} V_{j,l} +V_{i,l} V_{j,k} \right)$}
\hline
%
Funci\'on caracter\'istica~\footnote{\SZ{Se proba que la funci\'on generadora de
momentos no existe en
general}.\label{Foot:MP:CaracteristicaWishart}} &
$\displaystyle \Phi_X(\Omega) = \left| I - 2 \imath \Omega V
\right|^{-\frac{\nu}{2}}, \quad \Omega \in S_d(\Rset)$\\[2mm]
\hline
\end{tabular}
\end{center}
%
$\Tr$ \ denota  el operador traza y \ $\Gamma_d(y)  = \pi^{\frac{d \, (d-1)}{4}}
\prod_{j=1}^d  \Gamma\left(  y -  \frac{j-1}{2}  \right)$,  definida  para $y  >
\frac{d-1}{2}$, \ es la  funci\'on gamma multivariada~\cite{And03, GupNag99} y \
$S_d(\Rset)$  \ es  el conjunto  de  matrices reales  s\'imetricas de  $\Rset^{d
  \times d}$.   $\otimes$ es el producto de  Kronecker, $J$ \ la  matriz bloc de
bloc $(i,j)$-\'esima  \ $\un_j \un_i^t$ \ y \ $K$ \
la matriz bloc de bloc  $(i,j)$-\'esima \ $\un_i \un_j^t$,
%
\[
J = \begin{bmatrix}
\un_1 \un_1^t & \un_2 \un_1^t & \cdots & \un_d \un_1^t\\
    \vdots    &    \vdots    & \cdots &     \vdots   \\
\un_1 \un_d^t & \un_2 \un_d^t & \cdots & \un_d \un_d^t
\end{bmatrix}
%
\qquad\qquad
%
K = \begin{bmatrix}
\un_1 \un_1^t & \un_1 \un_2^t & \cdots & \un_1 \un_d^t\\
    \vdots    &    \vdots    & \cdots &     \vdots   \\
\un_d \un_1^t & \un_d \un_2^t & \cdots & \un_d \un_d^t
\end{bmatrix}
\]
%
(ver~\cite{PedRic91, SulTra96, And03}).

Firense que $p_X$ no es la distribuci\'on conjuntos de los componentes de \ $X$:
el hecho de  que \ $X$ \ sea  uan matriz aleatoria de \  $P_d^+(\Rset)$ \ impone
v\'inculos sobre sus compnentes; entre otros, $X_{i,j} = X_{j,i}$.

Inmediatamente, si  $d = 1$, la  distribuci\'on de Whishart \  $W_1(V,\nu)$ \ se
reduce  a la  distribuci\'on Gamma  $\G\left(\frac{\nu}{2} \,  , \,  \frac1{2 V}
\right)$. De este  hecho, se la podr\'ia ver  como extensi\'on matriz-variada de
la  distribuci\'on  gamma.  La  distribuci\'on  de  Wishart  tiene varias  otras
propiedades como las siguientes.
%
\begin{lema}[Stabilidad por transformaci\'on lineal]
\label{Lem:MP:StabilidadWishartLineal}
%
  Sea $X \,  \sim \, W_d(V,\nu)$ \ y \  $A \in \Rset^{d \times d'}$  \ con \ $d'
  \le d$ \ y de rango lleno. Entonces
  \[
  A^t X A \, \sim \, W_{d'}\left( A^t V A , \nu \right)
  \]
  %
  En particular, si $d'  = 1$, \ $A^t X A \, \sim  \, G\left( \frac{\nu}{2} \, ,
    \, \frac1{2 \, A^t V A} \right)$. M\'as all\'a, tomando $A = \un_j$, aparece
  de que  las componentes diagonales de \  $X$ \ son de  distribuci\'on gamma, \
  $X_{j,j}  \, \sim  \,  \G\left( \frac{\nu}{2}  \,  , \,  \frac1{2 \,  V_{j,j}}
  \right)$.
\end{lema}
%
\begin{proof}
  El     resultado     es     inmediato     saliendo     de     la     funci\'on
  caract\'eristica~\footref{Foot:MP:CaracteristicaWishart} y notando de que
%
\begin{eqnarray*}
\Phi_{A^t X A}(\omega) & = & \Esp\left[ e^{\imath \Tr\left( \omega^t A^t X A \right)}\right]\\[2mm]
%
& = & \Phi_X\left( A \omega^t A^t\right)\\[2mm]
%
& = &  \left| I - 2 \imath A \omega^t A^t V \right|^{-\frac{\nu}{2}}\\[2mm]
%
& = &  \left| I - 2 \imath \omega^t A^t V A \right|^{-\frac{\nu}{2}}
%
\end{eqnarray*}
%
de   \   $\Tr(AB)   =   \Tr(BA)$~\cite{Har08}   \   y   de   la   identidad   de
Sylvester~\cite{Syl51,  AkrAkr96}  o~\cite[\S~18.1]{Har08} \  $\left|  I  + A  B
\right| = \left| I + B A \right|$.  .
\end{proof}
%
De hecho, si los elementos diagonales son de distribuci\'on gamma, no es el caso
de         los        elementos         no-diagonales~\cite{Seb04,        And03}
o~\cite[Teo.~3.3.4]{GupNag99}.    De    eso   resuelte   delicado    llamar   la
distribuci\'on como gamma matriz-variada.

\begin{lema}[Stabilidad por suma]
\label{Lem:MP:StabilidadWishartSuma}
%
  Sea $X_i \,  \sim \, W_d(V,\nu_i), \: i = 1,\ldots,n$ independientes. Entonces
  \[
  \sum_{i=1}^n X_i \, \sim \, W_d\left( V \, , \, \sum_{i=1}^n \nu_i \right)
  \]
\end{lema}
%
\begin{proof}
  El     resultado     es     inmediato     saliendo     de     la     funci\'on
  caract\'eristica~\footref{Foot:MP:CaracteristicaWishart} y notando que como el
  el context vectorial $\Phi_{\sum_i X_i} = \prod_i \Phi_{X_i}$.
\end{proof}

La distribuci\'on  de Wishart aparece naturalmente en  problemas de estimaci\'on
de matriz de covarianza en el contexto gausiano:
%
\begin{lema}[V\'inculo con vectores gausianos~\cite{Seb04}]
  Sean \ $X_i \, \sim \, \N(0,V), \: i = 1, \ldots , n > d-1$ \ independientes y
  la  matriz  \  $S  =  \sum_{i=1}^n   X_i  X_i^t$  \  llamada  {\em  matriz  de
    dispersi\'on} (scatter matrix en ingl\'es). Entonces, \ $S \in P_d^+(\Rset)$
  (c.   s.)   \  ($S$ es  sim\'etrica  definida  positiva  casi siempre,  o  con
  probabilidad uno) y \ $S \,\sim \, W_d(V,n)$.
\end{lema}
%
Este resultado  permite tambi\'en probar  el lema~\ref{Lem:MP:StabilidadWishart}
para \ $\nu = n$ \ entero  escribiendo \ $X \egald \sum_{i=1}^n X_i X_i^t$ \ tal
que \ $A^t X A \egald \sum_{i=1}^n A^t X_i X_i^t A = \frac1n \sum_{i=1}^n \left(
  A^t X_i \right)  \left( A^t X_i \right)^t$ \  y notando que los \  $A^t X_i \,
\sim \,  \N(0, A^t V  A)$ \ son independientes~\cite{Seb04}.   Adem\'as, permite
re-obtener las  expreciones del promedio y de  las covarianzas~\footnote{Para la
  covarianza, su usa la formula \ $\Esp[Y_1 Y_2 Y_3 Y_4] = \Esp[Y_1 Y_2]\Esp[Y_3
  Y_4]  + \Esp[Y_1 Y_3]\Esp[Y_2  Y_4] +  \Esp[Y_1 Y_4]\Esp[Y_2  Y_3]$ \  para $Y
  = \begin{bmatrix}  Y_1 & Y_2 &  Y_3 & Y_4 \end{bmatrix}^t$  \ vector gausiano,
  formula que se  obtiene por ejemplo a partir  de la funci\'on caracter\'istica
  de un vecor  gausiano.}. Notar que cuando los \ $X_i$  \ tienen un promemedio,
el  lema conduce  a  lo que  es  conocido como  Wishart no  central~\cite{And03,
  Seb04}.

La distribuci\'on  Wishart aparece as\'i naturalmente en  problema de inferencia
Bayesiana  como distribuci\'on  a  priori conjugado~\footref{Foot:MP:BayesPrior}
del parametro $p$ de la ley gaussiana multivariada~\cite{Rob07}

% --------------------------------- Beta

\subsubseccion{Distribuci\'on Beta}
\label{Sssec:MP:Beta}

Se denota $X  \sim \beta(a,b)$ \ con  \ $(a,b) \in \Rset_+^{* \,  2}$ \ llamados
{\em parametros de forma}.  Las caracter\'isticas son:

\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on & $\X = [0 \; 1]$\\[2mm]
\hline
%
Parametros & $(a,b) \in \Rset_+^{* \, 2}$ (forma)\\[2mm]
\hline
%
Densidad   de    probabilidad   &   $\displaystyle    p_X(x)   =   \frac{x^{a-1}
(1-x)^{b-1}}{B(a,b)}$\\[2mm]
\hline
%
%Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
%\hline
%
%Momento factorial & $\Esp\left[ (X)_k \right] = ?$\\[2mm]
%\hline
%
Promedio & $\displaystyle m_X = \frac{a}{a+b}$\\[2mm]
\hline
%
Varianza &  $\displaystyle \sigma_X^2  = \frac{a b}{(a  + b)^2  (a + b  + 1)}$\\[2mm]
\hline
%
Asimetr\'ia & $\displaystyle \gamma_X = \frac{2 \, (b - a) \sqrt{a + b + 1}}{( a
+ b + 2) \sqrt{a b}}$\\[2mm]
\hline
%
Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = \frac{6 \, \left( (a - b)^2 (a + b + 1) - a
b (a  + b  + 2)  \right)}{a \, b  \left( a  + b  + 2 \right)  \left( a  + b  + 3
\right)}$\\[2mm]
\hline
%
%Generadora de probabilidad & $G_X(z) = e^{\lambda (z-1)}$ \ para \ $z \in \Cset$\\[2mm]
%\hline
%
Generadora de momentos & $\displaystyle M_X(u)  = \hypgeom{1}{1}\left( a , a + b
\, ; \, u \right)$ \ para \ $u \in \Cset$\\[2mm]
\hline
%
Funci\'on     caracter\'istica     &     $\displaystyle     \Phi_X(\omega)     =
\hypgeom{1}{1}\left( a , a + b \, ; \, \imath \omega \right)$\\[2mm]
\hline
\end{tabular}
\end{center}
%
% modo 
% Mediana 
$\displaystyle          B(a,b)          =          \frac{\Gamma(a)
\Gamma(b)}{\Gamma(a+b)}$ \ es la  funci\'on beta y \ $\displaystyle
\hypgeom{1}{1}$  \ es  la funci\'on  confluent hipergeom\'etrica~\cite{AbrSte70,
AndAsk99, GraRyz15}.

Su densidad de probabilidad y funci\'on de repartici\'on son representadas en la
figura Fig.~\ref{Fig:MP:Beta} para varios $a$ \ y \ $b$.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Beta} \end{center}
%
\leyenda{Ilustraci\'on de una densidad de  probabilidad beta (a), y la funci\'on
de  repartici\'on asociada (b).   $(a,b) =  (0.5 \,  , \,  0.5)$ (linea
punteada), $(3 \, , \, 1)$ (linea  mixta doble punteada), $(3 \, , \, 2)$ (linea
mixta), $(3 \, , \, 3)$ (linea
guionada), $(3 \, , \, 7)$ (linea llena).}
\label{Fig:MP:Beta}
\end{figure}

Nota que se recupera la  ley uniforme sobre \ $[0 \; 1]$ \  para \ $a = b =
1$. Se conoce la ley de $ 2 \, \beta\left( \frac12 , \frac12 \right) - 1$ \ como
{\em ley arcseno}.

Variables beta  tienen tambi\'en unas propiedades notables.  Primero, por cambio
de variables, se demuestra el lema siguiente:
%
\begin{lema}[Reflexividad]
\label{Lem:MP:ReflexividadBeta}
%
  Sea \ $X \, \sim \, \beta(a,b)$. Entonces
  %
  \[
  1-X \, \sim \, \beta(b,a)
  \]
  %
\end{lema}
%
\begin{lema}[V\'inculo con la ley gamma]
\label{Lem:MP:VinculoBetaGamma}
%
  Sea  \   $X  \,  \sim  \,   \G(a,c)$  \  e  \   $Y  \,  \sim   \,  \G(b,c)$  \
  independientes. Entonces
  %
  \[
  \frac{X}{X+Y} \, \sim \, \beta(a,b)
  \]
  %
  (independientemente  de $c$).   Adem\'as, $\frac{X}{X+Y}$  \ y  \ $X+Y$  \ son
  independientes.
\end{lema}
%
\begin{proof}
  La independencia de \ $c$ \ es obvia del hecho de que para cualquier $\theta >
  0, \: \theta^{-1} X \, \sim \, \G(a,\theta c)$ \ e \ $\theta^{-1} Y \, \sim \,
  \G(b,\theta  c)$,  la independencia  con  respeto  a \  $c$  \  viniendo de  \
  $\frac{\theta^{-1}     X}{\theta^{-1}     X     +     \theta^{-1}     Y}     =
  \frac{X}{X+Y}$.  Entonces, se  puede considerar  \ $c  = 1$  \ sin  perdida de
  generalidad. Ahora, sea la transformaci\'on
  %
  \[
  \begin{array}{lccl}
    g\ : & \Rset_+^2 & \mapsto & [0 \; 1] \times \Rset_+\\[1.5mm]
    %
    & (x,y) & \to & (u,v) = \left( \frac{x}{x+y} \, , \, x+y \right)
  \end{array}
  \]
  %
  Entonces, la transformaci\'on inversa se escribe
  %
  \[
  g^{-1}(u,v) = \left( u v \, , \, (1-u) v \right)
  \]
  %
  de matriz Jacobiana
  %
  \[
  \Jac_{g^{-1}} = \begin{bmatrix} v & u \\[2mm] -v & 1-u \end{bmatrix}
  \]
  %
  Del          teorema          de          cambio         de          variables
  teorema~\ref{Teo:MP:TransformacionBiyectiva},      notando     que     $\left|
    \Jac_{g^{-1}} \right| =  v$ \ y de la  independencia de \ $X$ \ e  \ $Y$, se
  obtiene para el vector aleatorio \ $W = \begin{bmatrix} U & V \end{bmatrix}^t$
  \ la densidad de probabilidad
  %
  \begin{eqnarray*}
    p_W(u,v) & = & p_X( u v ) \, p_Y( (1-u) v ) \, v\\[2mm]
    %
    & = & \frac{\left( u v \right)^{a-1} \, e^{- u v}}{\Gamma(a)} \times
    \frac{\left( (1-u) v \right)^{b-1} \, e^{- (1-u) v}}{\Gamma(b)} \times v\\[2mm]
    %
    & = & \frac{u^{a-1} (1-u)^{b-1}}{B(a,b)} \times \frac{v^{a+b-1} e^{-v}}{\Gamma(a+b)}
  \end{eqnarray*}
  %
  La  densidad se  obtiene por  marginalizaci\'on, \ie  integrando sobre  $v$ la
  densidad conjunta,  lo que  cierra la prueba  de la primera  parte.  Adem\'as,
  aparece claramente que \  $U$ \ y \ $V$ \ son  independientes (se factorisa la
  densidad de probabilidad).   Pasando, se recupera el hecho que  \ $X+Y \, \sim
  \, \G(a+b,1)$.
\end{proof}
%
\begin{lema}[Stabilidad por producto]
\label{Lem:StabilidadBeta}
%
  Sea  \  $X \,  \sim  \, \beta(a,b)$  \  e  \ $Y  \,  \sim  \, \beta(a+b,c)$  \
  independientes. Entonces
  %
  \[
  X Y \, \sim \, \beta(a,b+c)
  \]
  %
\end{lema}
%
\begin{proof}
  Sean \ $U  \, \sim \, \G(a,1)$, \ $V \,  \sim \, \G(b,1)$ \ y \  $W \, \sim \,
  \G(c,1)$   \   independientes  y   sean   \   $X   =  \frac{U}{U+V}$,   $Y   =
  \frac{U+V}{U+V+V}$  \ y  \ $Z  = U+V+W$.  Del lema  anterior \  $X \,  \sim \,
  \beta(a,b)$ \ y \ $Y \, \sim \, \beta(a+b,c)$. Sea la transformaci\'on
  %
  \[
  \begin{array}{lccl}
    g\ : & \Rset_+^3 & \mapsto & [0 \; 1]^2 \times \Rset_+\\[1.5mm]
    %
    & (u,v,w) & \to & (x,y,z) = \left( \frac{u}{u+v} \, , \, \frac{u+v}{u+v+w} \, , \, u+v+w \right)
  \end{array}
  \]
  %
  Entonces, la transformaci\'on inversa se escribe
  %
  \[
  g^{-1}(x,y,z) = \left( x y z \, , \, (1-x) y z \, , \, z (1-y) \right)
  \]
  %
  de matriz Jacobiana
  %
  \[
  \Jac_{g^{-1}} = \begin{bmatrix}
  %
    y z  &   x z   &   x y   \\[2mm]
  %
  - y z  & (1-x) z & (1-x) y \\[2mm]
  %
    0    &  - z    &  1-y
  %
  \end{bmatrix}
  \]
  %
  De      nuevo,      del      teorema      de     cambio      de      variables
  teorema~\ref{Teo:MP:TransformacionBiyectiva}, notando que $\left| \Jac_{g^{-1}}
  \right| = y  z^2$ \ y de la independencia  de \ $U, V, W$,  se obtiene para el
  vector aleatorio  \ $  T =  \begin{bmatrix} X &  Y &  Z \end{bmatrix}^t$  \ la
  densidad de probabilidad probabilidad
  %
  \begin{eqnarray*}
    p_T(x,y,z) & = & p_u( x y z ) \, p_V( (1-x) y z ) \, p_W( y (1-z) ) \, y z^2\\[2mm]
    %
    & = & \frac{\left( x y z \right)^{a-1} \, e^{- x y z}}{\Gamma(a)} \times
    \frac{\left( (1-x) y z \right)^{b-1} \, e^{- (1-x) y z}}{\Gamma(b)} \times
    \frac{\left( z (1-y) \right)^{c-1} \, e^{- z (1-y)}}{\Gamma(c)} \times y
    z^2\\[2mm]
    %
    & = & \frac{x^{a-1} (1-x)^{b-1}}{B(a,b)} \times \frac{y^{a+b-1}
      (1-y)^{c-1}}{B(a+b,c)} \times \frac{z^{a+b+c-1} e^{-z}}{\Gamma(a+b+c)}
  \end{eqnarray*}
  %
  Eso proba  que \  $X, \ Y$  \ y  $Z$ \ son  independientes (las  densidades se
  factorizan). Adem\'as,
  %
  \[
  X  Y =  \frac{U}{U+V} \times  \frac{U+V}{U+V+W} =  \frac{U}{U+V+W} \,  \sim \,
  \beta(a,b+c)
  \]
  %
  el      \'ultimo       resultado      como      consecuencia       de      los
  lemas~\ref{Lem:MP:VinculoBetaGamma}    y~\ref{Lem:MP:StabilidadGamma}.     Eso
  cierra la prueba.
\end{proof}

La distribuci\'on beta  aparece entre otros en problema  de inferencia Bayesiana
como   distribuci\'on   a   priori  conjugado~\footref{Foot:MP:BayesPrior}   del
parametro $p$ de la ley Binomial~\cite{Rob07}.

\SZ{Esta distribuci\'on aparece...}
% en el conteo de
%conteo de une repetici\'on de  una experiencia de maneja independiente hasta que
%occure un evento de probabilidad $p$; por ejemplo el n\'umero de tiro de un dado
%equilibriado hasta que occurre un ``6'' sigue una ley geometrica de parametro $p
%= \frac16$.



% --------------------------------- Dirichlet
\subsubseccion{Distribuci\'on de Dirichlet}
\label{Sssec:MP:Dirichlet}

Esta distribuci\'on teniendo su nombre de integrales on a simplex estudiados por
M. Lejeune-Dirichlet y J. Liouville  en 1839~\cite{GupRic01, Dir39, Lio39} en es
una extensi\'on  multivariada de las variables  beta a veces  conocida como {\em
  Beta multivariada}~\cite{OlkRub64}. Se  nota \ $X \, \sim \,  \Dir(a)$ \ con \
$a \in \Rset_+^{*  \, k}$ \ y \  $X$ \ vive sobre el  $(k-1)$-simplex estandar \
$\Simp_{k-1}$.  $a$ es  llamado parametro de forma. Como en  el caso de vectores
de  distribuci\'on multinomial, a  pesar de  que se  escribe \  $X$ \  de manera
$k$-dimensional, el vector partenece a una  variedad \ $d = k-1$ \ dimensional y
en el caso  \ $k = 2$ \ se  recupera la ley beta. A veces  se parametriza la ley
con un  parametro escalar \ $\alpha  > 0$ \ y  un vector del  simplex estandar \
$\bar{a} \in \Simp_{k-1}$ \ tal que
%
\[
a = \alpha \bar{a}, \quad \mbox{\ie} \quad \alpha = \sum_{i=1}^k \alpha_i, \quad
\bar{a} = \frac{a}{\alpha}
\]
%
$\alpha$ \  es conocido como  parametro de {\em  concentraci\'on} y el  vector \
$\bar{a}$ \ como {\em medida de base}.


Las caracter\'isticas de un vector de Dirichlet son:
%
\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on~\footnote{De hecho, se puede considerar que el vector
aleatorio es \ $(k-1)$-dimensional \ $\widetilde{X} = \begin{bmatrix}
\widetilde{X}_1 & \cdots & \widetilde{X}_{k-1} \end{bmatrix}^t$ \ definido sobre
el hipertriangulo \ $\widetilde{\X} = \Tri_{k-1} = \left\{ \widetilde{x} \in [0
\; 1]^{k-1}, \: \sum_{i=1}^{k-1} \widetilde{x}_i \le 1 \right\}$, proyecci\'on
del simplex sobre el hiperplano \ $x_k = 0$.\label{Foot:MP:DirichletXtilde}} &
$\X = \Simp_{k-1}, \: k \in \Nset \setminus \{ 0 \, , \, 1 \}$\\[2mm]
\hline
%
Parametros & $a = \alpha \, \bar{a} \, \in \, \Rset_+^{* \, k}$ \ (forma) \ con
\ $\alpha \in \Rset_+^*$ \ (concentraci\'on) y \ $\bar{a} \in \Simp_{k-1}$
(medida de base)\\[2mm]
\hline
%
Densidad de probabilidad~\footnote{La densidad de probabilidad es dada con
respeto a la medida de Lebesgue restricta al simplex \ $\Simp_{k-1}$. Tratando
de \ $\widetilde{X}$, el vector tiene una densidad con respeto a la medida de
Lebesgue usual, y es dada por \ $p_{\widetilde{X}}\left( \widetilde{x} \right) =
\frac{\prod_{i=1}^{k-1} \widetilde{x}_i^{\, a_i-1} \, \left( 1 -
\sum_{i=1}^{k-1} \widetilde{x}_i
\right)^{a_k-1}}{B(a)}$.\label{Foot:MP:DirichletDensidad}} & $\displaystyle
p_X(x) = \frac{\prod_{i=1}^k x_i^{a_i-1}}{B(a)}$\\[2mm]
\hline
%%
%%Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
%%\hline
%%
%%Momento factorial & $\Esp\left[ (X)_k \right] = ?$\\[2mm]
%%\hline
%%
Promedio & $\displaystyle m_X = \bar{a}$\\[2.5mm]
%\frac{a}{\sum_{i=1}^k a_i} \equiv \overline{a}$\\[2.5mm]
\hline
%
Covarianza & $\displaystyle \Sigma_X = \frac{\diag\left( \bar{a} \right) -
\bar{a} \bar{a}^t}{1 + \alpha}$\\[2.5mm]
%\sum_{i=1}^k a_k}$\\[2.5mm]
\hline
%%
%Asimetr\'ia & $\displaystyle \gamma_X = \frac{2 \, (b - a) \sqrt{a + b + 1}}{( a
%+ b + 2) \sqrt{a b}}$\\[2mm]
%\hline
%%
%Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = \frac{6 \, \left( (a - b)^2 (a + b + 1) - a
%b (a  + b  + 2)  \right)}{a \, b  \left( a  + b  + 2 \right)  \left( a  + b  + 3
%\right)}$\\[2mm]
%\hline
%%
%Generadora de probabilidad & $G_X(z) = e^{\lambda (z-1)}$ \ para \ $z \in \Cset$\\[2mm]
%\hline
%
Generadora de momentos~\footnote{El v\'inculo entre  las funciones  generadoras de
  momento  de \ $X$ \ y \ $\widetilde{X}$ \ es trivialmente  \ $M_{\widetilde{X}}\left(  \widetilde{u}  \right) =
  M_X\left(  \begin{bmatrix} \widetilde{u} &  0 \end{bmatrix}^t  \right)$ \  o \
  $M_X(u) =  e^{\imath u_k} M_{\widetilde{X}}\left( \begin{bmatrix} u_1  - u_k &
      \cdots &  u_{k-1} - u_k  \end{bmatrix}^t \right)$, y similarmente  para la
  funci\'on caracter\'istica.\label{Foot:MP:Dirichlet}} & $\displaystyle M_X(u) = \Phi_2^{(k)}( a , \alpha \, ;
\, u )$ \ para \ $u \in \Cset$\\[2mm]
\hline
%%
Funci\'on caracter\'istica & $\displaystyle \Phi_X(\omega) = \Phi_2^{(k)}(
a , \alpha \, ; \, \imath \omega )$\\[2mm]
\hline
\end{tabular}
%\footnotetext{De  hecho,  se puede  considerar  que  el  vector aleatorio  es  \
%  $(k-1)$-dimensional  \  $\widetilde{X}  =  \begin{bmatrix}  \widetilde{X}_1  &
%    \cdots   &  \widetilde{X}_{k-1}   \end{bmatrix}^t$  \   definido   sobre  el
%  hipertriangulo \  $\widetilde{\X} = \Tri_{k-1} = \left\{  \widetilde{x} \in [0
%    \;   1]^{k-1},  \:   \sum_{i=1}^{k-1}  \widetilde{x}_i   \le   1  \right\}$,
%  proyecci\'on del  simplex sobre  el hiperplano  \ $x_k =  0$.  Su  densidad de
%  probabilidad,  con   respeto  a  la  medida   de  Lebesgue,  es   dada  por  \
%  $p_{\widetilde{X}}\left(   \widetilde{x}  \right)   =  \frac{\prod_{i=1}^{k-1}
%    \widetilde{x}_i^{\,  a_i-1} \, \left(  1 -  \sum_{i=1}^{k-1} \widetilde{x}_i
%    \right)^{a_k-1}}{B(a)}$.   El v\'inculo entre  las funciones  generadoras de
%  momento  es trivialmente  \ $M_{\widetilde{X}}\left(  \widetilde{u}  \right) =
%  M_X\left(  \begin{bmatrix} \widetilde{u} &  0 \end{bmatrix}^t  \right)$ \  o \
%  $M_X(u) =  e^{\imath u_k} M_{\widetilde{X}}\left( \begin{bmatrix} u_1  - u_k &
%      \cdots &  u_{k-1} - u_k  \end{bmatrix}^t \right)$, y similarmente  para la
%  funci\'on caracter\'istica.}
\end{center}
%
$\displaystyle B(a) = \frac{\prod_{i=1}^k \Gamma\left( a_i \right)}{\Gamma\left(
    \sum_{i=1}^k     a_i     \right)}$     \     es    la     funci\'on     beta
multivariada~\cite[Teo.~1.8.6]{AndAsk99} \ y \ $\displaystyle \Phi_2^{(k)}$ \ es
la           funci\'on          confluent           hipergeom\'etrica          \
$k$-variada~\footnote{$\Phi_2^{(k)}(a;b;z)     =     \sum_{m    \in     \Nset^k}
  \frac{(a_1)_{(m_1)}     \ldots    (a_k)_{(m_k)}     \,     z_1^{m_1}    \ldots
    z_k^{m_k}}{(b)_{(m_1+\cdots+m_k)} m_1!  \ldots m_k!}$  \ con \ $(x)_{(n)}$ \
  s\'imbolo  de Pochhammer  usual o  factorial creciente,  $(x)_{(n)} =  x (x+1)
  \ldots (x+n)$ \,  con la convenci\'ion \ $(x)_{(0)} = 1$.   De hecho, la forma
  de la  funci\'on generadora de momento  viene directamente de  la escritura de
  las series de Taylor de $e^{u_i x_i}$ \ o de la forma integral de la funci\'on
  confluente  hipergeom\'etrica~\cite{Phi88}.},  tambi\'en  conocida como  forma
confluente    de   series    de    Lauricella~\cite[\S~1.4,   ec.~(8)]{SriKar85}
o~\cite{Hum22, App25, AppKam26, Erd37, Erd40}.
%
\SZ{Erd\'elyi,  A.,  Beitrag  zur  Theorie  der  konfluenten  hypergeometrischen
  Funktionen von mehreren  Ver\"anderlichen, Wiener Sitzungsberichte 146 (1937),
  431-467, ec. 7.2

  P. Appell and J. Kamp\'e  de F\'eriet, Fonctions  hyperg\'eom\'etriques et
  hypersph\'eriques, Gauthier Villars, Paris, 1926, pp. 124-125, 116

H. Exton, Multiple hypergeometric functions and applications, Ellis Horwood, Chichester, U.K., 1976
P42 
}

La figura Fig.~\ref{Fig:MP:Dirichlet} representa  el dominio de definici\'on del
vector (a) y su densidad de  probabildad con las marginales (ver m\'as adelante)
para $k = 3$.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Dirichlet} \end{center}
%
\leyenda{Ilustraci\'on del  dominio $\Simp_{k-1}$ de  definici\'on de la  ley de
  Dirichlet   para  \   $k  =   3$   \  (grise   oscuro),  con   el  dominio   \
  $(k-1)$-dimensional   \  $\Tri_{k-1}$   \  del   vector  \   $\widetilde{X}  =
  \protect\begin{bmatrix}   X_1  &  X_2   \protect\end{bmatrix}^t$  \   ($X_3  =
  1-X_1-X_2$) \ (grise claro) (a), y densidad de probabilidad de $\widetilde{X}$
  \   con   las   marginales   \   $p_{X_1},   \:   p_{X_2}$   (ver   notas   de
  pie~\ref{Foot:MP:DirichletDominio}   y~\ref{Foot:MP:DirichletDensidad}).   Los
  parametros   son    \   $a   =    \protect\begin{bmatrix}   3   &   2    &   2
    \protect\end{bmatrix}^t$  (b) y \  $a =  \protect\begin{bmatrix} 3  & 1  & 2
    \protect\end{bmatrix}^t$ (c).}
\label{Fig:MP:Dirichlet}
\end{figure}


Vectores  de  distribuci\'on  de  Dirichlet tienen  tambi\'en  unas  propiedades
notables, parecidas a las de la beta:
%
\begin{lema}[Reflexividad]
\label{Lem:MP:ReflexividadDir}
%
Sea \ $X \, \sim  \, \Dir(a), \: a \in \Rset_+^{* \, k}$ \  y \ $\Pi$ \ matriz \
$k \times k$ \ de permutaci\'on. Entonces
  %
  \[
  \Pi X \, \sim \, \Dir\left( \Pi a \right)
  \]
  %
\end{lema}
%
Adem\'as, se muestra una stabilidad remplazando dos componentes por su suma:
%
\begin{lema}[Stabilidad por agregaci\'on]
\label{Lem:MP:StabSumaDir}
%
  Sea  \ $X =  \begin{bmatrix} X_1  & \cdots  & X_k  \end{bmatrix}^t \,  \sim \,
  \Dir(a), \: a = \begin{bmatrix} a_1 & \cdots & a_k \end{bmatrix}^t \in
  \Rset_+^{* \, k}$ \ y \ $J^{(i,j)} = \begin{bmatrix} I_{j-1} & \un_i & 0 \\[1mm]
    0 & 0 & I_{k-j}\end{bmatrix}$ \ matriz  \ $(k-1) \times k$ \ donde \ $I_l$ \
  es  la identidad $l  \times l$,  $\un_i$ el  vector con  component $i$-\'esima
  igual a  1 (0 si no)  y $0$ matrices de  0 (todas de tama\~no  adcuado).  En \
  $J^{(i,j)} X$ \ se saca la componente $j$-\'esima, se re-emplaza la componente
  $i$-\'esima  por   la  suma  $X_i   +  X_j$  \   y  se  conservan   las  otras
  componentes. Entonces,
  %
  \[
  J^{(i,j)} X \, \sim \, \Dir\left( J^{(i,j)} a \right)  
  \]
  %
\end{lema}
%
\begin{proof}
  Del lema precediente, notando que existe una matriz de permutaci\'on \ $\Pi$ \
  tal que  \ $J^{(i,j)}  = J^{(1,2)} \Pi$,  se puede  concentrarse en el  caso \
  $(i,j) = (1,2)$. Sea el cambio de variables $g: x = (x_1,\ldots,x_k) \mapsto u
  =  (u_1,\ldots,u_k) = (x_1,x_1+x_2,x_3,\ldots,x_k)$.  Entonces \  $g^{-1}(u) =
  (u_1,u_2-u_1,u_3,\ldots,u_k)$ \ es de determinente de matriz Jacobiana igual a
  \ $1$ \ dando para $U = g(X)$ \ la densidad
  %
  \[
  p_U(u)  = \frac{u_1^{a_1-1}  \left(  u_2 -  u_1 \right)^{a_2-1}  \prod_{i=3}^k
    u_i^{a_i-1}}{B(a)}
  \]
  %
  sobre $g\left(  \Simp_{k-1} \right)$. Para $u_2  \in [0 \; 1]$  \ tenemos $u_1
  \in [  0 \;  u_2]$ \ as\'i  que, por  marginalizaci\'on en $u_1$  obtenemos la
  densidad
  %
  \begin{eqnarray*}
  p_{J^{(1,2)} X}(u_2,\ldots,u_k) & = & \frac{\prod_{i=3}^k u_i^{a_i-1}}{B(a)}
  \int_0^{u_2} u_1^{a_1-1} \left( u_2 - u_1 \right)^{a_2-1} \, du_1\\[2mm]
  %
  & = & \frac{\prod_{i=3}^k u_i^{a_i-1}}{B(a)} \, u_2^{a_1+a_2-1} \int_0^1
  v_1^{a_1-1} \left( 1 - v_1 \right)^{a_2-1} \, dv_1
  \end{eqnarray*}
  %
  con el cambio de variables $u_1 = u_2 v_1$. Se cierra la prueba notando que la
  integral   vale  \  $B(a_1,a_2)$   \  y   que  \   $\frac{B(a_1,a_2)}{B(a)}  =
  \frac{1}{B\left( J^{(1,2)} a \right)}$.
\end{proof}

De este lema, aplicado de manera recueriva, se obtiene en corolario siguiente:
%
\begin{corolario}
\label{Cor:MP:MarginalDirichletBeta}
%
  Sea  \ $X  \,  \sim \,  \Dir(a)$, entonces  \  $\displaystyle X_i  \, \sim  \,
  \beta\left( a_i \, , \, \alpha-a_i \right)$.
\end{corolario}

Naturalmente,  la ley de  Dirichelt siendo  una extensi\'on  de la  beta, existe
tambi\'en un v\'inculo entre esta ley y variables de distribuci\'on gamma:
%
\begin{lema}[V\'inculo con la ley gamma]
\label{Lem:MP:VinculoDirichletGamma}
%
Sea \ $X$ \ vector $k$-dimensional de componente \ $i$-\'esima \ $X_i \, \sim \,
\G(a_i,c), \: i = 1, \ldots , k$ \ independientes \ y \ $a$ vector de componente
$i$-\'esima \ $a_i$. Entonces
  %
  \[
  \frac{X}{\sum_{i=1}^k X_i} \, \sim \, \Dir(a)
  \]
  %
  (independientemente  de $c$).   Adem\'as, $\frac{X}{\sum_{i=1}^k  X_i}$ \  y \
  $\sum_{i=1}^k X_i$ \ son independientes.
\end{lema}
%
\begin{proof}
  La    prueba   sigue    exactamente   los    mismos   pasos    que    la   del
  lema~\ref{Lem:MP:VinculoBetaGamma} \ trabajando con \ $\widetilde{X}$.
\end{proof}

Naturalmente,  la  distribuci\'on de  Dirichlet,  extensi\'on  de  la ley  beta,
aparece entre  otros en problema  de inferencia Bayesiana como  distribuci\'on a
priori  conjugado~\footref{Foot:MP:BayesPrior}  del  parametro  $p$  de  la  ley
multinomial~\cite{Rob07}, extensi\'on de la ley binomial.

\SZ{
Polya urn schemes, Chinese restaurant
}



% --------------------------------- Student-t
\subsubseccion{Distribuci\'on Student-$t$ multivariada}
\label{Sssec:MP:Student}

En   el  caso   escalar,  esta   ley   fue  introducida   inicialmente  por   F.
R. Helmert~\cite{Hel75, Hel76, She95}  y J.  L\"uroth~\cite{Lur76, Pfa96}.  Pero
es m\'as  conocida por su  intruducci\'on por William  Sealy Gosset~\footnote{De
  hecho,  Gosset  fue  un  estudiante  trabajando en  la  f\'abrica  de  cerveza
  irlandesa  Guiness  sobre estad\'istica  relacionada  a  la  qui\'imica de  la
  cerveza.   A pesar  que hay  varias  explicaciones sobre  el hecho  de que  se
  public\'o este trabajo bajo el nombre ``Student''. Unas es que fue para que no
  se  sabe que  la f\'abrica  estaba  trabajando sobre  estas estadisticas  para
  estudiar la  calidad de la  cerveza~\cite{Wen16}.}  en 1908,  trabajando sobre
variables    centradas    normalizadas    por    el    promedio    y    varianza
empiricos~\cite{Stu08}.  Fue estudiada entre  otros intensivamente por el famoso
matematico R. Fisher~\cite{Fis25}.  En la  literatura, esta ley es conocida bajo
los nombres {\em Student},  {\em Student-$t$} o simplemente {\em $t$-distribuci\'on}
o  a\'un  bajo  el  nombre  {\em  Pearson  tipo  IV}  debido  a  la  familia  de
Pearson~\cite{Pea95}.

Se denota con \ $X \sim t_\nu(m,\Sigma)$  \ con \ $m \in \Rset^d$, \ $\Sigma \in
P_d^+(\Rset)$  \  conjunto  de  las   matrices  de  \  $\Rset^{d  \times  d}$  \
s\'imetricas definidas positivas. $m$ \ es llamado {\em parametro de posici\'on}
(no es  la media  que puede  no existir), \  $\Sigma$ \  es llamada  {\em matriz
  caracter\'istica} (no es [proporcional a]  la covarianza que puede no existir)
y \ $\nu > 0$ \ llamado  {\em grados de libertad}.  Las caracter\'isticas de una
Student-$t$ son las siguientes:
%
\begin{center}
\begin{tabular}
{
|>{\vspace{-2mm}}p{.35\textwidth}|
>{\vspace{-2mm}\hspace{2mm}}p{.6\textwidth}|
}
%
\hline
%
Dominio de definici\'on &
$\X = \Rset^d$\\[2mm]
\hline
%
Parametro & $\nu \in \Rset_+^*$ \ (grados de libertad), \ $m \in \Rset^d$ \
(posici\'on), \ $\Sigma \in P_d^+(\Rset)$ \ (matriz caracer\'istica)\\[2mm]
\hline
%
Densidad de probabilidad & $\displaystyle p_X(x) = \frac{\Gamma\left(
\frac{\nu+d}{2} \right)}{\pi^{\frac{d}{2}} \nu^{\frac{d}{2}} \Gamma\left(
\frac{\nu}{2} \right) \, \left| \Sigma \right|^{\frac12}} \, \left( 1 +
\frac{(x-m)^t \Sigma^{-1} (x-m)}{\nu} \right)^{- \, \frac{\nu+d}{2}}$\\[2mm]
\hline
%
%Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
%\hline
%
%Momento factorial & $\Esp\left[ (X)_k \right] = ?$\\[2mm]
%\hline
%
Promedio & $\displaystyle m_X = m$ \ si \ $\nu > 1$; \ no
existe si no~\footnote{De manera general, esta ley admite momentos de orden \ $k$ \
si y solamente si \ $\nu > k$.\label{Foot:MP:ExistenciaMomentosStudent}}.\\[2.5mm]
\hline
%
Covarianza~\footnote{Fijense de que $\Sigma$ no es la covarianza, pero es
proporcional a la covarianza\ldots cuando existe. Se podr\'ia imaginar
renormalizar la ley tal que \ $\Sigma_X$ \ y \ $\Sigma$ \ coinciden, pero no
ser\'ia posible en el caso \ $\nu \le 2$.} & $\displaystyle \Sigma_X =
\frac{\nu}{\nu-2} \, \Sigma$ \ si \ $\nu > 2$; \ no existe si
no~\footref{Foot:MP:ExistenciaMomentosStudent}.\\[2.5mm]
\hline
%%%
Asimetr\'ia (caso escala) & $\displaystyle \gamma_X = 0$ \ si \ $\nu > 3$; \ no
existe si no~\footref{Foot:MP:ExistenciaMomentosStudent}.\\[2mm]
\hline
%%%
Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = \frac{6}{\nu-4}$ \ si
\ $\nu > 4$; \ no existe si no~\footref{Foot:MP:ExistenciaMomentosStudent}.\\[2mm]
\hline
%
Funci\'on caracter\'istica~\footnote{Se muestra sencillamente que la funci\'on
generatriz de momentos puede existir si y solamente si \ $\real{u} = 0$. La
funci\'on genetratriz de momentos restricta al producto cartesiano de bandas \
$\real{u} = 0$ \ es nada m\'as que la funci\'on caracter\'istica. Adem\'as, esta
funci\'on fue calculdada, especialmente en el caso multivariado, relativamente
recientemente~\cite{Sut86, Hur95, KibJoa06, SonPar14}.} & $\displaystyle
\Phi_X(\omega) = \frac{\nu^{\frac{\nu}{4}}}{2^{\frac{\nu}{2}-1} \Gamma\left(
\frac{\nu}{2} \right)} \, e^{\imath \omega^t m} \, \left( \omega^t \Sigma \omega
\right)^{\frac{\nu}{4}} K_{\frac{\nu}{2}}\left( \sqrt{\nu \, \omega^t \Sigma
\omega} \right) $\\[2mm]
\hline
\end{tabular}
\end{center}
%
$K_\alpha$ \  es la funci\'on  Bessel modificada de  segunda especie y  de orden
$\alpha$~\cite{AbrSte70,  GraRyz15, Wat22,  GraMat95},  tambi\'en conocida  como
funci\'on de MacDonald~\cite{Mac98}.

Nota: nuevamente se puede escribir $X \, \egald \, \Sigma^{\frac12} S + m$ \ con
\ $S \, \sim \,  t_\nu(0,I)$ \ donde \ $I$ \ es la identidad  y \ $S$ \ es dicha
{\em Student-$t$ estandar}  y las caracter\'isticas de \ $X$  \ son v\'inculadas a
las  de  \ $S$  \  (y vice-versa)  por  transformaci\'on  lineal (ver  secciones
anteriores).


La densidad de probabilidad Student-$t$ estandar y la funci\'on de repartici\'on
en el caso escalar  son representadas en la figura Fig.~\ref{Fig:MP:Student}-(a)
y    (b)   y    una   densidad    en   un    contexto    bi-dimensional   figura
Fig.~\ref{Fig:MP:Student}(c).
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Student} \end{center}
% 
\leyenda{Ilustraci\'on  de  una  densidad  de probabilidad  Student-$t$  escalar
  estandar (a),  y la funci\'on  de repartici\'on asociada  (b) con \ $\nu  = 1$
  (linea llena), \ $\nu = 3$ (linea  guionada), \ $\nu = 7$ (linea punteada) \ y
  \ $\nu \to +\infty$ (linea llena fina; ver m\'as adelante) grados de libertad,
  as\'i que una densidad de probabilidad Student-$t$ bi-dimensional con \ $\nu =
  1$ \  grado de libertad,  centrada, y de  matriz caracter\'istica \  $\Sigma =
  R(\theta) \Delta^2  R(\theta)^t$ \ con \  $R(\theta) = \protect\begin{bmatrix}
    \cos\theta    &     -    \sin\theta\\[2mm]    \sin\theta     &    \cos\theta
    \protect\end{bmatrix}$   \   matriz   de    rotaci\'on   y   \   $\Delta   =
  \diag\left(\protect\begin{bmatrix}  1   &  a\protect\end{bmatrix}  \right)$  \
  matriz  de   cambio  de  escala,   y  sus  marginales   \  $X_1  \,   \sim  \,
  t_\nu\left(0,\cos^2\theta +  a^2 \sin^2\theta \right)$ \  y \ $X_2  \, \sim \,
  t_\nu\left(0,\sin^2\theta   +   a^2  \cos^2\theta   \right)$   \  (ver   m\'as
  adelante). En la figura, $a = \frac13$ \ y \ $\theta = \frac{\pi}{6}$.}
\label{Fig:MP:Student}
\end{figure}

Nota: el caso  \ $\nu = 1$ \  es conocido como distribuci\'on de  {\em Cauchy} o
{\em Cauchy  Breit-Wigner}~\cite{SamTaq91, toto,  titi}.  Es un  caso particular
tambi\'en de distrubuci\'on $\alpha$-estables~\cite{SamTaq91}.

Contrariamente al caso gaussiano, de la forma de la densidad de probabilidad, es
claro que si la matriz \ $\Sigma$  \ es diagona, la densidad no factoriza, as\'i
que  las componentes  del vector  no  son independientes.  Este ejemplo  muestra
claramente que  la reciproca del lema~\ref{Lem:MP:IndependenciaCov}  es falsa en
general.

Sin embargo, las distribuciones Student-$t$ tienen varias propiedades notables.

\begin{lema}[Stabilidad por transformaci\'on lineal]
\label{Lem:MP:StabilidadLineal}
%
  Sea \ $X  \, \sim \, t_\nu(m,\Sigma)$,  \ $A$ \ matriz de  \ $\Rset^{d' \times
    d}$ \ con \ $d' \le d$, y de rango lleno y \ $b \in \Rset^{d'}$. Entonces
  %
  \[
  A X + b\, \sim \, t_\nu( A m + b , A \Sigma A^t)
  \]
  %
  En particular los componentes de \ $X$ \ son student-$t$,
  %
  \[
  X_i \, \sim \, t_\nu(m_i , \Sigma_{i,i} )
  \]
\end{lema}
\begin{proof}
  La prueba es inmediata usando  la funci\'on caracter\'istica y sus propiedades
  por  transformaci\'on lineal.  La condici\'on  sobre \  $A$ \  es  necesaria y
  suficiente para que \ $A \Sigma A^t \in P_{d'}^+(\Rset)$.
\end{proof}

\begin{lema}[V\'inculo con las distribuciones Gamma y Gausiana (mezcla Gaussiana de escala)]
\label{Lem:MP:MezclaGaussianaEscalaStudent}
%
  Sea \ $V \sim \G\left( \frac{\nu}{2} \, ,  \, \frac{\nu}{2} \right)$ \ y \ $G \, \sim \,
  \N(0,I)$ \ independientes. Entonces
  %
  \[
  \frac{G}{\sqrt{V}} \, \sim \, t_\nu( 0 , I )
  \]
  %
  Dicho de  otra manera, se  puede escribir \  $X \, \sim \,  t_\nu(m,\Sigma)$ \
  esticasticamente   bajo   la    forme   \   $X   \egald   \sqrt{\frac{\nu}{V}}
  \Sigma^{\frac12} G + m $ \ donde  \ $\egald$ \ significa que la igualdad es en
  distribuci\'on.
\end{lema}
\begin{proof}
  Lo  m\'as  simple es  de  salir  de la  formula  de  probabilidad total  vista
  pagina~\pageref{:MP:}, notando que condicionalmente a \ $V=v$ \ la variable es
  gausiana de covarianza $\frac{1}{\sqrt{v}} I$,
%
\begin{eqnarray*}
p_X(x) & = & \int_\Rset p_{X|V=v}(x) \, p_V(v) \, dv\\[2mm]
%
& \propto & \int_0^{+\infty} v^{\frac{d}{2}} e^{-\frac{v}{2} x^t x}
v^{\frac{\nu}{2}-1} e^{-\frac{\nu}{2} v} \, dv\\[2mm]
%
& \propto & \left( 1 + \frac{x^t x}{\nu} \right)^{- \frac{d+\nu}{2}}
\int_0^{+\infty} u^{\frac{d+\nu}{2}-1} e^{-u} \, du\\[2mm]
%
& \propto & \left( 1 + \frac{x^t x}{\nu} \right)^{- \frac{d+\nu}{2}}
\end{eqnarray*}
%
con  \ $\propto$  \ significando  ``proporcional a''  (el coeficiente  es  lo de
normalizaci\'on) y  el cambio de  variables $v  = \frac{2 \,  u}{\nu + x^t  x} =
\frac{\frac{2}{\nu}}{1 + \frac{x^t x}{\nu}} \, u$.
\end{proof}
%
Nota: este  lema permite tambi\'en  probar el lema~\ref{Lem:MP:StabilidadLineal}
escribiendo \ $A X + b \egald  \sqrt{\frac{\nu}{V}} A \Sigma^{\frac12} G + A m +
b$.

\begin{lema}[L\'imite Gausiana]
\label{Lem:MP:LimiteGaussiana}
%
  Sea \ $X_\nu \, \sim \, t_\nu(m,\Sigma)$ \ vector Student-$t$ parametizado por
  \ $\nu$ \ sus grados de libertad. Entonces
  %
  \[
  X_\nu \, \limitd{\nu \to \infty} \, X \, \sim \, \N(m,\Sigma)
  \]
  %
  donde  \  $\displaystyle  \limitd{}$  \   significa  que  el  l\'imite  es  en
  distribuci\'on.
\end{lema}
\begin{proof}
  La prueba  es inmediata tomando el  logaritmo de la  densidad de probabilidad,
  usando   la  formula   de   Stirling~\footnote{De  hecho,   esta  formula   es
    probablemente debida previamente a  A.  De Moivre~\cite{Dem56, Pea24, Cam86,
      Dut91}, y fue mejorada por  Stirling m\'as tarde. Fue mejorada a\'un m\'as
    por         el        famoso        matem\'atico         S.        Ramanujan
    recientemente~\cite[\S~4.1]{AndBer13}}  para \ $\log\Gamma(z)  = \left(  z -
    \frac12 \right)  \log z  - z +  \frac12 \log(2 \pi)  + o(1)$  \ en \  $z \to
  +\infty$~\cite{Sti30, AbrSte70, GraRyz15} \ y \ $-\frac{d+\nu}{2} \log\left( 1
    +  \frac{(x-m)^t \Sigma^{-1} (x-m)}{\nu}  \right) =  -\frac{d+\nu}{2} \left(
    \frac{(x-m)^t \Sigma^{-1} (x-m)}{\nu} + o\left( \nu^{-1} \right) \right) = -
  \frac{(x-m)^t \Sigma^{-1} (x-m)}{2} + o(1)$.
\end{proof}


\SZ{
%corolario suma dis independientes... via condicionalmente a...

Sampling distribution, Gosset, Fisher 25.

Applications
}

\SZ{

% --------------------------------- Laplace
\subsubseccion{Distribuci\'on de Laplace multivariada}
\label{Sssec:MP:Laplace}

} 


\SZ{

% --------------------------------- Familia exponencial
\subsubseccion{Familia  exponencial}
\cite{Dar35, Koo36,  And70,  Kay93, LehCas98,  Rob07}.;
Muchas de estas leyes entran en una familia que juega un rol particular en problema de maximizacion de entropie (ver cap 2): es la familia exponencial...
}

\SZ{
% --------------------------------- Familia eliptica
\subsubseccion{Familia eliptica}
Invariante por rotacion... GSM
}


\SZ{Teorema del l\'imite central, y relajando la independencia, y versiones con leyes diferentes pero uniformamente acotadas.}

\SZ{hablar de  simulaci\'on? Metoto inverso,  mezcla, rejeccion, a traves  de la
  condicional para el caso vectorial?}

