\seccion{Esperanza, momentos, identidades y desigualdades}%funciones generadoras}
\label{Sec:MP:esperanzamomento}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\modif{Como lo hemos introducido, la  noci\'on formal de probabilidad naci\'o en
  el  contexto  de  juego (cartas,  dados),  bajo  entre  otros el  impulso  del
  matem\'atico  italiano y  jugador de  dados y  cartas Gerolamo  Cardano  en el
  siglo~{XVI},  y a\'un  m\'as bajo  el trabajo  muy profundo  de  la dinast\'ia
  Bernoulli,   y  en   particular  Jacob   Bernoulli~\cite[en  lat\'in]{Ber1713}
  o~\cite{Ber13:2}.   En particular,  Bernoulli  se interes\'o  no solamente  al
  resultado de un  sorteo, impredictible, pero en lo que pasa  cuando se hace un
  gran   num\'ero   de  sorteos.   As\'i   sali\'o   la   idea  de   ``resultado
  promedio''. \SZ{De  hecho, la noci\'on  de promedio es probablemente  debido a
    B.    Pascal    dentro    de    una    carta    mandada    a    Fermat    en
    1654~\cite{DavEdw01}.  Adem\'as,}  el  trabajo  de Bernoulli  ha  ido  m\'as
  adelante: prob\'o la ley de gran  n\'umero que vamos a ver m\'as adelante. Sin
  ir tan lejos, en el caso de  una variable $X$ aleatoria discreta (ej. un dado,
  que puede tomar valores en $\{ 1 \; \ldots \; 6 \}$), haciendo varios sorteos,
  el promedio de  estes sorteos va a ser $\sum_i  \widetilde{p}_i x_i$ con $x_i$
  los valores que puede tomar la variable y $\widetilde{p}_i$ la proporci\'on de
  $x_i$ que  se obtuvo en el  sorteo.  De hecho, intuitivamente  (es la visi\'on
  frecuencista),  $\widetilde{p}_i$ va  a tender  a $p_i  = P(X=x_i)$  cuando el
  n\'umero de sorteo tiende al  infinito.  La definici\'on del valor promedio, o
  media, de una variable aleatoria  cualquiera se formaliza mas rigorosamente en
  en marco de la teoria de la medida, pero coincide con la intuici\'on.}


% ================================= Media

\subseccion{Media de un vector aleatorio}
\label{Ssec:MP:VectorMedio}


Una  variable aleatoria  $X$  tiene asociado  un  {\it promedio}  o {\it  media}
(tambi\'en  llamado   {\it  valor  esperado  o  de   expectaci\'on  o  esperanza
  matem\'atica})  que se  obtiene pesando  cada valor  de $X$  con la  medida de
probabilidad asociada a ese valor~\cite{AshDol99, AthLah06},
%
\begin{definicion}[Media o valor/vector medio]
\label{Def:MP:ValorMedio}
%
  Formalmente, la media de  una variable aleatoria $X$ \underline{integrable} es
  definida por
  %
  \[
  \Esp[X] = \int_\Omega X(\omega) \, dP(\omega).
  \]
  %
  Por    el    teorema    de    la    medida    imagen~\ref{Teo:MP:MedidaImagen},
  pagina~\pageref{Teo:MP:MedidaImagen}, esta media  se escribe tambi\'en a partir
  de la medida de probabilidad $P_X$ como
  %
  \[
  \Esp[X] = \int_\Rset x \, dP_X(x).
  \]
  %
  En  el caso vectorial  $d$-dimensional, hay  que entender  la media,  o vector
  medio, como  un vector de componentes  $i$-\'esima la media  $\Esp[X_i]$ de la
  componente $i$-\'esima $X_i$ de $X$, dando
  %
  \[
  \Esp[X] = \int_{\Rset^d} x \, dP_X(x).
  \]
  %
  A veces,  se encuentra  tambi\'en la notaci\'on  \ $\langle  x \rangle$ \  o \
  $\langle  x  \rangle_{p_X}$  \  para  el  valor  medio,  especialmente  en  la
  literatura de f\'isica.
\end{definicion}
%
La secunda formulaci\'on  del valor medio se proba  sencillamente, empezando por
$X =  \un_A$ para unos $A \in  \B(\Rset)$.  Entonces $P_X =  (1-P(A)) \delta_0 +
P(A)  \delta_1$.  Luego  $\displaystyle \int_\Omega  \un_A(\omega)  dP(\omega) =
P(A) = (1-P(A)) \times 0 + P(A)  \times 1 = \int_\Rset x dP_X(x)$.  Se cierra la
prueba  con  el  teorema~\ref{Teo:MP:MedibleLimite}  dando  cualquier  funci\'on
medible    como     limite    de    funciones    escalonadas,     y    por    la
definici\'on~\ref{Def:MP:IntegracionReal} de la  integral de cualquier funci\'on
medible.

Luego,   de    la   distribuci\'on   marginal    $\displaystyle   P_{X_i}(B)   =
\int_{\Rset^{i-1}   \times   B   \times   \Rset^{d-i}}  dP_X(x)$,   se   obtiene
$\displaystyle  \Esp[X_i] = \int_{\Rset^d}  x_i \,  dP_X(x)$, dando  la \'ultima
formulaci\'on en el caso vectorial.

Una variable  aleatoria $X$ se dice  integrable cuando $E[|X|] <  \infty$. De la
misma  manera, un  vector aleatorio  admite  una media  si y  solamente si  cada
componente  es   integrable.  Veremos  m\'as  adelante   que  existen  variables
aleatorias que no admiten una media.

M\'as  all\'a  de  la formulaci\'on  matem\'atica  de  la  media \  $\Esp[X]$  \
representa la posici\'on alrededor de la cual se ``distribuye las probabilidades
de  occurencia''. Es  el equivalente  probabil\'istico de  centro de  gravedad o
barycentro en mec\'anica.

En el  caso de variables  aleatorias discretas, de soporte  \modif{$\X$ discreto
  finito o numerable,} inmediatamente
%
\[
\Esp[X] = \modif{\sum_{x \in \X} x \, P(X = x) = \sum_{x \in \X} x \, p_X(x).}
\]
%
\noindent  Fijense de  que $\Esp[X]$  no partenece  necesariamente a  $\X$:
%
\begin{ejemplo}
\label{Ej:MP:Uniforme3Estados}
%
  Sea \  $X$ \  uniforme sobre \  $\X = \{  1 \; 3  \; 7 \}$,  \ie \
  $\forall \,  i \in \X, \quad  P(X = i) =  \frac13$. \ Se calcula  $\Esp[X] = 1
  \times \frac13  + 3 \times \frac13  + 7 \times \frac13  = \frac{11}{3} \not\in
  \X$.  Tampoco es el promedio de los valores extremos.
\end{ejemplo}
%
\noindent Cuando \ $|\X| = +\infty$, \ $X$ \ no es necesariamente integrable:
%
\begin{ejemplo}
\label{Ej:MP:DiscretaSinMedia}
%
Sea  \ $\X  = \Nset^*$  \ con  \modif{$P(X =  x) =  \frac{6}{\pi^2 \,  x^2}$}. \
Claramente, \modif{$\sum_x\frac{6}{\pi^2 \, x}$} diverge, as\'i que $X$ no tiene
una media.
\end{ejemplo}

En el caso de vectores  aleatorios continuos, obtenemos la expresi\'on siguiente
de la media (o vector medio):
%
\[
\Esp[X] = \int_{\Rset^d} x \, p_X(x) \, dx.
\]
%
\noindent Las mismas observaciones que  hicimos en el caso discreto se encuentra
en el caso continuo:
%
\begin{ejemplo}
\label{Ej:MP:MediaNoEnX}
%
Sea \ $X$ \ de densidad de  probabilidad \ $p_X(x) = \frac12 \un_{[0 \; 1)}(x) +
\frac{3   \sqrt{x-2}}{4}   \un_{[2   \;   3)}(x)$  \   como   ilustrado   figura
Fig.~\ref{Fig:MP:ProbaContinua},    pagina~\pageref{Fig:MP:ProbaContinua}.    Se
calcula \ $\Esp[X] = \frac{31}{20} \not\in \X = [0 \; 1] \cup [2 \; 3]$.
\end{ejemplo}
%
\begin{ejemplo}
\label{Ej:MP:VariableCauchySinMedia}
%
  Un ejemplo  de vector aleatorio no  teniendo media es  dado en el caso  de una
  distribuci\'on de Cauchy-Lorentz (ver  m\'as adelante) \ $\displaystyle p_X(x)
  = \frac{\alpha}{\left( 1 + x^t x \right)^{\frac{d+1}{2}}}$ \ donde $\alpha$ es
  un factor de normalizaci\'on.
\end{ejemplo}

En el caso general, para calcular  la media, hay que pasar por la distribuci\'on
$P_X$, como en el ejemplo~\ref{Ej:MP:Mixta}  pagina~\pageref{Ej:MP:Mixta}:
%
\begin{ejemplo}[Continuaci\'on del ejemplo~\ref{Ej:MP:Mixta}]
\label{Ej:MP:EspMixta}
%
Sea $X =  V \, \un_{U <  \frac12} + \un_{U \ge \frac12}$  \ con \ $U$ \  y \ $V$
variables aleatorias  independientes de distribuci\'on uniformas sobre  \ $[0 \;
1)$, \ie  $p_U(x) = \un_{[0 \;  1)}(x)$. \ De \  $(X \in B)  \: \Leftrightarrow \:
\left( \left( U < \frac12 \right) \cap \left( V \in B \right) \right) \, \cup \,
\left( \left( U \ge \frac12 \right) \cap  \left( 1 \in B \right) \right)$, \ del
hecho de que  los eventos de la uni\'on son incompatibles  y de la independencia
de $U$ y $V$ (o saliendo de la funci\'on de repartici\'on), se obtiene $P_X(B) =
\frac12  P_V(B)  + \frac12  \delta_1(B)$.   A  continuaci\'on, \  $\displaystyle
\Esp[X] = \frac12 \int_\Rset dP_V(x) + \frac12 \int_\Rset d\delta_1(x) = \frac12
\int_\Rset p_V(x)  \, dx + \frac12  \times 1 =  \frac12 \int_0^1 dx +  \frac12 =
\frac34$.
\end{ejemplo}

\

Una nota intersante es  de que, en el caso escalar, si \  $X \ge 0$ \ admitiendo
una media, se obtiene
%
\[
\Esp[X]  = \int_{\Rset_+} P(X  > t)  \, dt  = \int_{\Rset_+}  \left( 1  - F_X(t)
\right) \, dt.
\]
%
Se proba saliendo  de \ $\displaystyle x = \int_0^x  dt = \int_{\Rset_+} \un_{(t
  \; +\infty)}(x)  \, dt$ \ dando  \ $\displaystyle \Esp[X]  = \int_\Rset \left(
  \int_{\Rset_+}   \un_{(t  \;  +\infty)}(x)   \,  dt   \right)  \,   dP_X(x)  =
\int_{\Rset_+} \left(  \int_\Rset \un_{(t \; +\infty)}(x) \,  dP_X(x) \right) \,
dt$     \     por    el     teorema     de    Fubini     Th.~\ref{Teo:MP:Fubini}
pagina~\pageref{Teo:MP:Fubini}. Se  cierra la prueba observando  que la integral
interior es nada  m\'as que $P(X > t)$.   En el caso discreto con  $\X = \Nset$,
viene inmediatamente \  \modif{$\displaystyle \sum_{t \in \Nset} P(X  > t)$} que
podemos probar  directamente saliendo de  \modif{$P(X =  t) = P(X  > t) -  P(X >
  t-1)$}. En el  caso de variable admitiendo una  densidad, se \modif{lo} obtiene tambi\'en
haciendo  una integraci\'on por  partes~\footnote{El el  casi discreto,  hay que
  tener precauciones  separando la series de  una diferencia de  terminos. En el
  caso  $X$  continuo  admitiendo  una   densidad,  hay  que  estudiar  bien  el
  coomportamiento de $t \mapsto t (1-F_X(t))$ al infinito.}

Esta f\'ormula se aplica al ejemplo~\ref{Ej:MP:Mixta} que tratamos:
%
\begin{ejemplo}[Continuaci\'on del ejemplo~\ref{Ej:MP:Mixta}]
\label{Ej:MP:EspMixtaPositiva}
%
Sea $X =  V \, \un_{U <  \frac12} + \un_{U \ge \frac12}$  \ con \ $U$ \  y \ $V$
variables aleatorias  independientes de distribuci\'on uniformas sobre  \ $[0 \;
1)$.  Obtuvimos pagina~\pageref{Ej:MP:Mixta} \  $F_X(x) = \frac{x}{2} \un_{[0 \;
  1)}(x)  +   \un_{[1  \;   +\infty)}(x)$.   A  continuaci\'on,   reobtenemos  \
$\displaystyle  \Esp[X]  = \int_0^1  \left(  1 -  \frac{x}{2}  \right)  \, dx  =
\frac34$.
\end{ejemplo}.

\

Terminamos  esta  secci\'on con  la  propiedad  de  linealidad de  la  esperanza
matem\'atica $\Esp$,  como consecuencia de  la linealidad de la  integraci\'on y
definici\'on de la distribuci\'on  marginal: para cualquier conjunto de vectores
aleatorios \ $\{  X_i \}$ \ integrables  y cualquieras matrices \ $\{  C_i \}$ \
dadas  de  dimensiones compatibles  con  las  de \  $X$  \  (incluyendo el  caso
escalar),
%
\[
\Esp\left[ \sum_i C_i X_i \right] = \sum_i C_i \Esp\left[ X_i \right]
\]
%
(la integrabilidad de la suma se proba a partir de la desigualdad triangular).



% ================================= Momentos

\subseccion{Momentos de un vector aleatorio}
\label{Ssec:MP:Momentos}

Si $X$ es una variable aleatoria, para cualquier funci\'on medible $f$, \ $f(X)$
tambi\'en lo es.   Se puede entonces definir su valor medio,  si existe. A pesar
de necesitar evaluar  la distribuci\'on de probabilidad de $Y  = f(X)$, el valor
medio se calcula a partir del de $X$:
%
\begin{teorema}[Teorema de transferencia]
\label{Teo:MP:Transferencia}
%
  Sea  \ $X$  \ un  vector  aleatorio $d$-dimensional  y \  $f: \Rset^d  \mapsto
  \Rset^{d'}$ \ una funci\'on medible tal que $f(X)$ sea integrable. Entonces
  %
  \[
  \Esp\left[   f(X)  \right]   =  \int_\Omega   f(X(\omega))  \,   dP(\omega)  =
  \int_{\Rset^{d'}} f(x) \, dP_X(x).
  \]
  %
  En particular, en el caso $\X = X(\Omega)$ discreto se obtiene
  %
  \[
  \Esp\left[ f(X) \right] = \modif{\sum_{x \in \X} f(x) \, P(X = x)}
  \]
  %
  y para $X$ continuo admitiendo una densidad de probabilidad
  %
  \[
  \Esp\left[ f(X) \right] = \int_{\Rset^d} f(x) \, p_X(x) \, dx.
  \]
\end{teorema}
%
\begin{proof}
  Sea $B \in \B(\Rset^d)$ \ y consideramos \ $f(x) = \un_B(x)$. Entonces, $\Y
  = \{  0 \;  1 \}$ y inmediatamente
  %
  \[
  P_Y = P_X(B) \, \delta_1 + (1-P_X(B)) \, \delta_0.
  \]
  %
  Entonces
  %
  \[
  \Esp[f(X)]  =  \int_\Rset  P_X(B)  \,  d\delta_1 +  \int_\Rset  (1-P_X(B))  \,
  d\delta_0 = P_X(B) = \int_{\Rset^d} \un_B(x) \, dP_X(x).
  \]
  %
  En el caso  $d' = 1$, para $f  \ge 0$, se cierra entonces la  prueba usando el
  teorema~\ref{Teo:MP:MedibleLimite}         pagina~\pageref{Teo:MP:MedibleLimite},
  escribiendo \  $f$ \  como l\'imite creciente  de una sucesi\'on  de funciones
  escalonadas,  y   la  definici\'on  Def.~\ref{Def:MP:IntegracionReal}   de  la
  integraci\'on real. El  caso $d' > 1$ es  nada mas que $d' =  1$, componente a
  componente.
\end{proof}

De  manera general,  estas medias  son llamadas  {\it momentos}  de  la variable
aleatoria $X$. Los momentos relevantes usuales son los siguientes:
%
\begin{itemize}
\item    para    el    ``monomio''    $f(x)   =    x^{\otimes    K}$    producto
  \modif{externo}~\footnote{Recuerdense  de  que $x  \otimes  x$  es una  matriz
    teniendo como componentes  $x_i x_j$; entonces $x^{\otimes K}$  en un tensor
    $K$-dimensional teniendo como  componentes $ \displaystyle \left[ x^{\otimes
        K}  \right]_{i_1,\ldots,i_K} =  \prod_{j=1}^K x_{i_j}$.}
   de $x$  \ $K$
  veces \ siendo \ $K \in \Nset^*$, se obtiene \modif{el tensor conteniendo todo
    los} $K${\it-\'esimo momentos (ordinarios)} de $X$:
  %
  \[
  m_K \equiv \Esp\left[  X^{\otimes K} \right] = \int_{\Rset^d}  x^{\otimes K} \
  dP_X(x)
  \]
  %
  que tiene unidades de $\prod_j X_{i_j}$  (de $X_i^K$ si los componentes de $X$
  tienen la  misma ``unidad'').  Se escriben  tambi\'en los momentos  de orden \
  $K$ \ \modif{bajo la escritura (con el mismo $m$, por abuso)}
  %
  \[
  m_{k_1,\ldots,k_d}   =  \Esp\left[   \prod_{i=1}^d  X_i^{k_i}   \right]  \quad
  \mbox{con}  \quad \modif{\begin{bmatrix}  k_1 &  \cdots &  k_d \end{bmatrix}^t
    \in \Part{K,d} }
%\sum_i k_i = K.
  \]
  %
  \modif{que son los componentes de $m_K$.}   Se puede incluir el caso $K=0$ con
  la  convenci\'on $x^{\otimes  0} =  1$, que  corresponde a  la  condici\'on de
  normalizaci\'on: \ $\displaystyle m_0 =  \int_\Rset dP_X(x) = 1$.  La media es
  el  primer  momento: $m_1  =  \Esp[X]  =  m_X$.  T\'ipicamente,  los  primeros
  momentos  son  m\'as  relevantes  que   los  de  \'ordenes  mayores,  para  la
  caracterizaci\'on de una distribuci\'on.  Para $K = 2$, en el caso escalar, el
  momento  de   orden  2  es  el   an\'alogo  del  momento  de   inercia  de  la
  mec\'anica.\newline  Por ejemplo,  para la  distribuci\'on uniforme  $p_X(x) =
  \frac{1}{b-a}$   en   el   intervalo   $[   a   \;   b]$,   resulta   $m_K   =
  \frac{b^{K+1}-a^{K+1}}{(K+1)(b-a)}$.   En particular,  $m_1  = \frac{a+b}{2}$,
  valor  medio del  intervalo.\newline  Fijense  de que  $X^{\otimes  K}$ no  es
  siempre integrable,  por ejemplo, en el  caso con densidad,  si $p_X(x)$ tiene
  soporte  (semi)infinito, necesariamente  la funci\'on  $p_X$ debe  tender  a 0
  cuando \modif{$\|  x \|  \rightarrow \infty$.}  Si  $p_X(x)$ es {\it  de largo
    alcance}, en el  sentido de que no cae a 0  suficientemente r\'apido con $x$
  para  $x$  grandes, algunos  momentos  pueden  no  existir.  Por  ejemplo,  la
  distribuci\'on   de   probabilidad   de   Cauchy--Lorentz  (o   funci\'on   de
  Breit--Wigner), dada  por \ $p_X(x)  = \modif{\frac{\Gamma\left( \frac{d+1}{2}
      \right)}{\pi^{\frac{d+1}{2}} \, \left| R \right|^{\frac12}}} \, \left( 1 +
    (x-x_0)^t R^{-1}  (x-x_0) \right)^{-  \, \frac{d+1}{2}}$ \  sobre $\Rset^d$,
  con  \ \modif{$R \in  P_d^+(\Rset), \:  x_0 \in  \Rset^d$}, no  tiene momentos
  finitos de orden $K \geq 1$.
%
\item Frecuentemente (especialmente en el  caso de variables discretas $X$ sobre
  $\X  =  \Nset$),  resulta   \'util  introducir  los  $K$-\'esimo  {\it  momento
    factorial}   de   $X,  \quad   k   =   \begin{bmatrix}   k_1  &   \cdots   &
    k_d \end{bmatrix}^t \in \Part{K,d}$
%\Nset^d, \quad K = \sum_{i=1}^d k_i$
 \ mediante \modif{el tensor de orden $K$}
  %
  \[
  \modif{f_K \quad  \mbox{de componentes} \quad  f_{k_1,\ldots,k_d} = \Esp\left[
      \prod_{i=1}^d \PocD{X_i}{k_i} \right]}
  %  \equiv \Esp\left[  \prod_{k=0}^{r-1} (X-k)  \right] =  \sum_{n  = r}^\infty
  % \frac{n!}{(n-r)!} \, P(X = n).
 \]
 %
 \modif{con \ $\PocD{x}{n}$ \ factorial decreciente (ver notaciones).}  Se puede
 ver que cuando \ $\X  = \{ 0 \; \ldots \; n \}, \: n \in  \Nset$, \ $f_K = 0$ \
 cuando \ $K > n$.
%
%  o {\it cumulantes}
\item Los {\it  momentos centrales} se definen alrededor  de la media $\Esp[X]$,
  \ie, como  el tensor de  los $K$-\'esimo momentos  de la {\it  desviaci\'on} \
  $\Delta X \equiv X-\Esp[X]$:
  %
  \[
  \zeta_K \equiv \Esp\left[  \left( X - \Esp[X] \right)^{\otimes K} \right].
  \]
  %
  Se escribe tambi\'en
  %
  \[
  \zeta_{k_1,\ldots,k_d}   =  \Esp\left[   \prod_{i=1}^d   \left(  X_i-\Esp[X_i]
    \right)^{k_i}  \right] \quad \mbox{con}  \quad \modif{\begin{bmatrix}  k_1 &
      \cdots & k_d \end{bmatrix}^t \in \Part{K,d}}
%\sum_i k_i = K.
  \]
  %
  Se deduce que si la  distribuci\'on de probabilidad satisface a una simetr\'ia
  central  con respecto a  la media,  \ie \  $X-m_X \egald  -(X-m_X)$ \  donde \
  $\egald$ \ significa que los vectores aleatorios tiene la misma distribuci\'on
  de probabilidad, entonces todos los momentos centrales impares son nulos.  Los
  momentos     (centrales)    brindan     medidas     que    caracterizan     la
  distribuci\'on. \modif{Cuando existen:}
  %
  \begin{enumerate}
  \item El primer momento, o media:
   %
   \[
    m_X = \Esp[X].
   \]
   %
 \item El segundo momento central se conoce como {\it matriz de covarianza}.  En
   el caso escalar, hablamos de {\it varianza}, o {\it dispersi\'on} o tambi\'en
   {\it desviaci\'on cuadr\'atica media}.
  %
  \[
  \Sigma_X \equiv  \Cov[X] \equiv  \zeta_2 = \Esp\left[  \left( X -  m_X \right)
    \left( X - m_X \right)^t \right].
  \]
  %
  \SZ{Se  conoce  tambi\'en  la  matriz  inversa \  $\Sigma_X^{-1}$  \  bajo  la
    denominaci\'ion {\em matriz de precisi\'on}.}
  %
  En el caso escalar, la varianza se escribe en general
  %
  \[
  \Var[X] \equiv \sigma_X^2 = \Esp\left[ \left( X  - m_X \right)^2 \right]
  \]
  %
  y  es  una  medida  del  cuadrado  del  ancho  efectivo  de  una  densidad  de
  probabilidad (o vector de probabilidad). \modif{Para dos vectores aleatorios \
    $X$  \  e  \ $Y$  \  respectivamente  $d$  y  $d'$-dimensonal (con  $d'$  no
    necesariamente igual a  $d$), hablamos de {\it covarianza  entre $X$ e $Y$},
    que se escribe
    %
    \[
    \Sigma_{X,Y} = \equiv \Cov[X,Y] = \Esp\left[ \left( X - m_X \right) \left( Y
        - m_Y \right)^t \right].
    \]
    %
    Esta  matriz   partenece  a  \  $\M_{d,d'}(\Rset)$  \   y  \[\Sigma_{X,Y}  =
    \Sigma_{Y,X}^t.\].\newline Se  puede notar que, para dos  componentes $i \ne
    j$ de  $X$, $\Sigma_X$ tiene  como $(i,j)$-\'esima componente  la covarianza
    $\Cov[X_i,X_j]  = \Esp\left[  \left(  X_i  - m_{X_i}  \right)  \left( X_j  -
        m_{X_j} \right) \right]$  entre las variables $X_i$ y  $X_j$ y tiene las
    varianzas  de los  $X_i$ en  su diagonal.  Adem\'as, es  sencillo ver  que \
    $\Sigma_X  \in   P_d(\Rset)$,  \ie  \   $\Cov[X]$  \  es   sim\'etrica,  por
    construcci\'on, y \ $\Cov[X] \ge 0$  ($\forall \, \mu \in \Rset, \quad \mu^t
    \Sigma_X \mu  \ge 0$; en el caso  escalar la varianza es  no negativa),} con
  igualdad  s\'olo cuando  $P_X =  \delta_{x_0}$ para  un $x_0$  dado,  esto es,
  cuando  no   hay  incerteza  sobre   el  resultado.   De  la   desigualdad  de
  Cauchy-Bunyakovsky-Schwarz           (ver          corolario~\ref{Cor:MP:CBS},
  pagina~\pageref{Cor:MP:CBS}) se proba sencillamente de que
  %
  \[
  \left| \Cov[X_i,X_j] \right|^2 \le \sigma_{X_i}^2 \sigma_{X_j}^2,
  \]
  %
  as\'i que  se define  tambi\'en el {\it  coeficiente de correlaci\'on}  que es
  adimensional   y   toma    valores   entre   $-1$   (variables   completamente
  anticorrelacionadas) y 1 (variables completamente correlacionadas) como:
  %
  \[
  \rho_{i,j} = \rho_{j,i} \equiv \frac{\Cov[X_i,X_j]}{\sigma_{X_i} \sigma_{X_j}}.
  \]
  %
  Como ejemplo, dadas $X_1$ y $X_2 = a X_1 + b$ que fluct\'uan en fase ($a>0$) o
  al rev\'es ($a<0$), se tiene \modif{la relaci\'on entre desviaciones \ $\Delta
    X_2  = a  \Delta X_1$,  conduciendo a}  \ $\rho_{1,2}  = \frac{a}{|a|}  = \pm
  1$.\newline Tambi\'en,  \modif{se puede ver que
  %
    \[
    \Var\left[ \| \Delta X \| \right] = \Tr \Sigma_X.
    \]}
   %
  La covarianza  est\'a bien definida si \  \modif{$\| X \|$} \  es una variable
  aleatoria de cuadrado integrable, esto  es, cuando \ \modif{$E\left[ \| X \|^2
    \right]  <   \infty$}.   Se  proba   sencillamente  \modif{(desallorando  el
    ``producto'' y usando la linealidad de la esperanza) que
  %
  \[
  \Cov[X,Y] = \Esp\left[ X Y^t \right] - m_X m_Y^t
  \]
  %
} conocido como  {\it teorema de K\"onig-Huygens}.  En  el caso escalar \modif{y
  $X  =  Y$},  es  el  equivalente  del teorema  de  Huygens  de  la  mec\'anica
relacionando  el momento  de  inertia de  un  solido con  respeto  al origen  en
funci\'on  del momento  de  inertia con  respeto  al centro  de masa.  \modif{Adem\'as,
inmediatamente,
  %
  \[
  \forall \: A  \in \M_{n,d}(\Rset), \: B \in  \M_{n',d'}(\Rset), \: a \in
  \Rset^n, \: b \in \Rset^{n'}, \quad \Cov[A X + a , B Y + b] = A \Cov[X,Y]
  B^t.
  \]
}
  %
  En el caso escalar, $d = 1$,  lo que es conocido tambi\'en como el {\it ancho}
  de una distribuci\'on est\'a dado por la {\it desviaci\'on est\'andar}
  %
  \[
  \sigma_X = \sqrt{\Var[X]}
  \]
  %
  tiene  las mismas  unidades de  $X$,  y se  usa para  normalizar los  momentos
  centrales  de orden  superior.  El  {\it ancho  relativo} es  otra  medida que
  caracteriza    la   distribuci\'on,    dado   por    $\frac{\sigma_X}{m_X}   =
  \sqrt{\frac{\Esp\left[  X^2 \right]}{m_X^2}-1}$ cuando  $m_X \ne  0$. \newline
  Dado un vector aleatorio $X$, teniendo en cuenta que los dos primeros momentos
  dan  las   caracter\'isticas  m\'as   importantes  de  la   distribuci\'on  de
  probabilidad,  puede  resultar   conveniente  hacer  una  transformaci\'on  de
  variable  aleatoria  a  la   llamada  {\it  variable  est\'andar}:  $Y  \equiv
  \Sigma_X^{-\frac12} \left(  X - m_X \right)$, donde  $\Sigma^{-\frac12}$ es la
  \'unica matriz  sim\'etrica definida positiva tal  que su cuadrado  es igual a
  $\Sigma^{-1}$~\cite{HorJoh13, MagNeu99}  que entonces tiene media igual  a 0 y
  una  matriz  de  covarianza  igual  al  identidad $I$  (en  el  caso  escalar,
  desviaci\'on est\'andar igual a 1).
  %
\item  En  el  caso  escalar,  el  tercer momento  central  permite  definir  el
  \modif{{\it   coeficiente   de  asimetr\'ia}   o   m\'as  sencillamente   {\it
      asimetr\'ia}} (o skewness en ingles)~\cite{Spi76, Pea05}:
  %
  \[
  \Asim[X]  \equiv \gamma_X  \equiv \Esp\left[  \left( \frac{X  - m_X}{\sigma_X}
    \right)^3 \right] = \frac{\zeta_3}{\sigma_X^3},
  \]
  %
  momento de orden  3 de la variable estandar, que  resulta adimensional y puede
  tener  signo positivo  o negativo,  anul\'andose para  distribuciones  que son
  sim\'etricas respecto  del valor medio.   \modif{En el contexto  multivariado, la
    extensi\'on  natural es  el tensor  de orden  3 de  los momentos  del vector
    centrado   y   estandardizado    $\Sigma_X^{-\frac12}   \left(   X   -   m_X
    \right)$~\cite{MorRoh94}:
  %
  \[
  \Asim[X] \equiv \gamma_X \equiv \Esp\left[ \left( \Sigma_X^{-\frac12} \left( X
        - m_X \right) \right)^{\otimes 3} \right].
  \]
  %
  Sin  embargo, se  puede querer  un  resumen de  la asimetr\'ia  a trav\'es  un
  n\'umero  escalar  o  un  vector.   Se  encuentran  en  la  literatura  varias
  proposiciones  en   esta  direcci\'on~\cite{Mar70,  MalAfi73,   Iso82,  Sri84,
    MorRoh94, BalBri07, Kol08, BalSca12}.  La mayor\'ia de estas extensiones se
  relacionan con $\gamma_X$ (ej. suma de los terminos cuadrados en~\cite{Mar70},
  vector  de  $i$-\'esima   componente  la  suma  en  $j$   de  las  componentes
  $(j,j,i)$-\'esima de $\gamma_X$ en~\cite{MorRho94}  o el vector de $i$-\'esima
  componente la suma en $j,k$ de las componentes $(j,k,i)$-\'esima de $\gamma_X$
  en~\cite{Kol08}).  Todas coinciden  con $\gamma_X$  o su  cuadrado en  el caso
  escalar y cada  medida es invariente por transformaci\'on $X \mapsto  A X + b$
  con $A \in \M_{d,d}(\Rset)$ invertible y $b \in \Rset^d$.}
%  Por ejemplo,  en~\cite{Mar70} (ver  tambi\'en~\cite{BalSca12, Seb04})  como \
%  $\displaystyle  \gamma_X^{\mathrm{mar}}  = \sum_{i,j,k=1}^d  \left(  \gamma_X
% \right)_{i,j,k}^2$.  Esta medida est  estrictamente positiva desde que hay una
% asimetr\'ia en una direcci\'on,  pero no da ninguna direcci\'on de asimetr\'ia
% por  ejemplo.  En el caso escalar,  $\gamma_X^{\mathrm{m}}$ coincide con\ldots
%   $\gamma_X^2$.   Una   otra   propuesta  es   dada  en~\cite{MorRho94}   (ver
%  tambi\'en~\cite{BalSca12})  como   \  $\gamma_X^{\mathrm{mrs}}  =  \Esp\left[
%   \left\|   \Sigma_X^{-\frac12}   \left(    X   -   m_X   \right)   \right\|^2
%   \Sigma_X^{-\frac12} \left( X - m_X \right) \right]$.  Es un vector que tiene
% una direcci\'on y que se relaciona con $\gamma_X$ bajo la forma $\displaystyle
%  \left(  \gamma_X^{\mathrm{mrs}}  \right)_i  =  \sum_{j=1}^d  \left(  \gamma_X
% \right)_{j,j,i}$.   Sin embargo, la  significaci\'on de la direcci\'on  no queda
% clara~\cite{BalSca12}. Adem\'as no toma  en cuenta todos los momentos de orden
% 3, lo que dio lugar  a una extensi\'on propuesta en~\cite{Kol08} bajo la forma
% de un vector $d$-dimensional de componente $i$-\'esima \ $\displaystyle \left(
%   \gamma_X^{\mathrm{k}}    \right)_i   =   \sum_{j,k=1}^d    \left(   \gamma_X
%  \right)_{j,k,i}$.   Esta definici\'on  coincide  con  la  asimetr\'ia del  caso
% escalar,  pero la significaci\'on  de la direcci\'on  de este vector  no queda
% claro tampoco.   Adem\'as, en el caso $d  > 1$, puede ser cero a  pesar de que
%  haya  asimetr\'ia   (terminos  positivos  y  negativos  en   la  suma  pueden
% compensarse).  Mencionamos el trabajo  de~\cite{MalAfi73} donde la idea  es de
% proyectar \ $X-m_X$  \ sobre un vector \ $u \in \Sset_d$  \ y de considerar la
%  asim\'etria  de  este  escalar, $\gamma_X^{\mathrm{ma}}  (u)  =  \frac{\left(
%     \Esp\left[   \left(  u^t   (X-m_X)  \right)^3   \right]  \right)^2}{\left(
%     \Var\left[ u^t (X-m_X) \right] \right)^3}$ \ y \ $\gamma_X^{\mathrm{ma}} =
%  \max{u \in \Sset_d}  \gamma_X^{\mathrm{ma}}(u)$.  So  notara que  se podr\'ia
%  definir tales  medidas con  el vector  estandardizado  \ $\Sigma_X^{-\frac12}
% (X-m_X)$  \ dando \ $\gamma_{\Sigma_X^{-\frac12}  (X-m_X)}^{\mathrm{ma}} (u) =
%    \gamma_X^{\mathrm{ma}}    \left(    \frac{\Sigma_X^{-\frac12}    u}{\left\|
%       \Sigma_X^{-\frac12} u \right\|}  \right)$, \ $\gamma_X^{\mathrm{ma}} (v)
%      =      \gamma_{\Sigma_X^{-\frac12}     (X-m_X)}^{\mathrm{ma}}      \left(
%   \frac{\Sigma_X^{\frac12} v}{\left\|  \Sigma_X^{\frac12} v \right\|} \right)$
%    \    y     \    $\gamma_X^{\mathrm{ma}}    =    \gamma_{\Sigma_X^{-\frac12}
%   (X-m_X)}^{\mathrm{ma}}$. Esta medida  fue modificada en~\cite{BalBri07} como
%     \      $\displaystyle     \gamma_X^{\mathrm{bbq}}     =     \int_{\Sset_d}
% \gamma_{\Sigma_X^{-\frac12} (X-m_X)}^{\mathrm{ma}} (u)  d\mu(u)$ \ con \ $\mu$
% \ medida de Haar sobre $\Sset_d$.  Estas \'ultimas medidas no se relacionan de
% manera obvia al tensor \ $\gamma_X$, pero coinciden con \ $\gamma_X^2$ \ en el
% caso escalar..}
  %
\item  En  el caso  escalar,  el  cuarto momento  central  da  lugar  a la  {\it
    curtosis}~\cite{Pea05, Wes14}:
  %
  \[
  \Curt[X]  \equiv \kappa_X  \equiv \Esp\left[  \left( \frac{X  - m_X}{\sigma_X}
    \right)^4 \right] = \frac{\zeta_4}{\sigma_X^4},
  \]
  % 
  momento de orden  4 de la variable estandar,  que posibilita diferenciar entre
  distribuciones  altas y  angostas.   Veremos  m\'as adelante  de  que para  la
  densidad  Gausiana  $p_X(x)  =  \frac{1}{\sqrt{2  \pi}  \sigma}  \exp\left(  -
    \frac{(x-m)^2}{2 \, \sigma^2} \right)$, \ $m_X = m, \: \sigma_X = \sigma, \:
  \gamma_X =  0, \: \kappa_X  = 3$. Se  dice de que $p_X$  es alta y  angosta, o
  sub-gausiana,  o {\it con  colas livianas}  o tambi\'en  platic\'urtica cuando
  $\kappa_X < 3$,  y se dice bajas  y anchas o sobre-gausiana, o  {\it con colas
    pesadas} o tambi\'en leptoc\'urtica cuando  $\kappa_X > 3$ (para $\kappa_X =
  3$  la distribuci\'on es  a veces  dicha mesoc\'urtica).   A veces,  se define
  entonces la {\it curtosis por exceso}
  %
  \[
  \Excurt[X]  \equiv \widebar{\kappa}_X  = \Curt[X]  - 3  =  \Esp\left[ \left(
      \frac{X     -     m_X}{\sigma_X}    \right)^4     \right]     -    3     =
  \frac{\zeta_4}{\sigma_X^4} - 3.
  \]
  %
  M\'as que  el pico de distribuci\'on,  la curtosis (por  excesso) describe las
  colas de una distribuci\'on (pesadas para el curtosis por exceso o livianas en
  el caso contrario)~\cite{Wes14}. \modif{Como  para la asimetr\'ia, la extensi\'on
    natural multivariada de la curtosis es el tensor de orden 4
  %
    \[
    \Curt[X] \equiv  \kappa_X \equiv \Esp\left[ \left(  \Sigma_X^{-\frac12} (X -
        m_X) \right)^{\otimes 4} \right]
    \]
  %
    Se muestra  de nuevo que en  el contexto gausiano multivariado  $p_X(x) = (2
    \pi)^{-\frac{d}{2}}   |\Sigma|^{-\frac12}    \exp\left(   -\frac12   (x-m)^t
      \Sigma^{-1} (x-m) \right)$ (ver m\'as adelante) que , \ $\displaystyle m_X
    = m,  \: \Sigma_X =  \Sigma, \: \gamma_X  = 0, \: \kappa_X  = \sum_{i,j=1}^d
    \Big( \left(  \un_i \un_i^t \right)  \!\otimes\! \left( \un_j \un_j^t  \right) +
    \left( \un_i \un_j^t  \right) \!\otimes\! \left( \un_i \un_j^t  \right) + \left(
      \un_i \un_j^t \right) \!\otimes\! \left(  \un_j \un_i^t \right) \Big)$ \ as\'i
    que se  puede definir una  curtosis multivariada por  exceso (sub-entendido,
    con respeto a la gausiana) como:
  %
  \[
  \Excurt[X] \equiv \widebar{\kappa}_X =  \Curt[X] - \sum_{i,j=1}^d \Big( \left(
    \un_i \un_i^t  \right) \!\otimes\! \left(  \un_j \un_j^t \right) +  \left( \un_i
    \un_j^t \right) \!\otimes\! \left( \un_i  \un_j^t \right) + \left( \un_i \un_j^t
  \right) \!\otimes\! \left( \un_j \un_i^t \right) \Big).
  \]
  %
  Se encuentren en la literatura varias medidas aletrnativas dando un resumen de
  la curtosis a trav\'es un n\'umero escalar o una matriz~\cite{Mar70, MalAfi73,
    Sri84,  MorRoh94,  Kol08, Seb04}.   La  mayor\'ia  de  estas extensiones  se
  relacionan  con  $\kappa_X$  (ej.  suma  de  las  componentes  $(i,i,j,j)$  de
  $\kappa_X$ en~\cite{Mar70},  matriz de  $(i,j)$-\'esima componente la  suma en
  $k$ de las componentes  $(i,k,k,j)$-\'esima de $\kappa_X$ en~\cite{MorRho94} o
  la matriz  de $(i,j)$-\'esima componente la  suma en $k,l$  de las componentes
  $(k,l,i,j)$-\'esima  de  $\gamma_X$  en~\cite{Kol08}).   Todas  coinciden  con
  $\kappa_X$ (resp. $\overline{\kappa}_X$)  en el caso escalar y  cada medida es
  invariente   por  transformaci\'on  $X   \mapsto  A   X  +   b$  con   $A  \in
  \M_{d,d}(\Rset)$ invertible y $b \in \Rset^d$.}
%  se  encuentra  en~\cite[\S~2.7]{Seb04}  una extensi\'on  multivariada  de  la
% curtosis cuando $\Sigma_X \in P_d^+(\Rset)$ (no degenerada, \ie invertible):
%    %
%    \[
%    \Curt[X] \equiv \kappa_X = \Esp\left[  \left( \left( X - m_X \right)^t
%          \Sigma_X^{-1} \left( X - m_X \right) \right)^2 \right]
%    \]
%    %
%    Se muestra  de nuevo que en  el contexto gausiano multivariado  $p_X(x) = (2
%%    \pi)^{-\frac{d}{2}}   |\Sigma|^{-\frac12}    \exp\left(   -\frac12   (x-m)^t
%      \Sigma^{-1}  (x-m) \right)$  (ver m\'as  adelante) que  , \  $m_X =  m, \:
%    \sigma_X = \sigma, \: \gamma_X = 0, \: \kappa_X = d (d+2)$ y se usa la misma
%    terminologia  describiendo las  colas  de distribuciones  con  respeto a  la
%    gausiana.  As\'i,  se puede de  nuedo definir una curtosis  multivariada por
%    exceso como:
%  %
%  \[
%  \Excurt[X] \equiv \widebar{\kappa}_X = \Curt[X] - d (d+2).
%  \]
  \end{enumerate}
  %
  Fijense  de   que,  en  el  contexto   escalar  $d  =  1$,   se  vinculan  los
  \modif{momentos  centrales}  y los  momentos  ordinarios  directamente de  las
  definiciones:
  %
  \[
  \zeta_k = \sum_{l=0}^k \binom{k}{l} \left( - m_X \right)^{k-l} m_l
  \]
  %
  para cualquier  $k \in  \Nset$, siendo $m_0  = \zeta_0  = 1$.  Por  ejemplo, \
  $\zeta_2  =  m_2  -  m_1^2$  \   que  es  nada  m\'as  que  la  relaci\'on  de
  K\"onig-Huyggens, mientras que \ $\zeta_3 = m_3 - 3 m_1 m_2 + 2 m_1^3$.  En el
  contexto  multivariado,  la relaci\'on  momentos--momentos  centrales toma  la
  expresi\'on
  %
  \[
  \zeta_{k_1,\ldots,k_d}  = \sum_{l_1  =  0}^{k_1} \cdots  \sum_{l_d =  0}^{k_d}
  \left(  \prod_{i=1}^d  \binom{k_i}{l_i}  \left(  -  m_{X_i}  \right)^{k_i-l_i}
  \right) \, m_{l_1,\ldots,l_d}.
  \]
\end{itemize}


% ================================= Independencia, identidades y desigualdades

\subseccion{Independencia, identidades y desigualdades}
\label{Ssec:MP:MomentosDesigualdades}

Una primera relaci\'on interesante concierna el caso de variables independientes
y como se comporta la covarianza de estas:
%
\begin{lema}[Independencia y covarianza]
\label{Lem:MP:IndependenciaCov}
%
  Sean  \  $X$  \ e  \  $Y$  \  dos  vectores  aleatorios integrables.   Si  son
  independientes, entonces
  %
  \[
  \Esp[X Y^t] = \Esp[X] \Esp[Y]^t \qquad \mbox{\ie} \qquad \Cov[X,Y] = 0.
  \]
  %
  En  particular, para  $X$  con componentes  independientes,  $\Cov[X]$ es  una
  matriz diagonal.
\end{lema}
%
\begin{proof}
  Sean \ $X = \sum_j x_j \un_{A_j}$  \ e \ $Y = \sum_k y_k \un_{B_k}$ \
  dos variables escalonadas. Entonces, \ $A_j =  (X = x_j)$ \ y \ $B_k = (Y
  = y_k)$. Luego
  %
  \begin{eqnarray*}
  \Esp[X Y] & = & \sum_{j,k} x_j y_k \Esp\left[ \un_{A_j} \un_{B_k} \right]\\[2.5mm]
  %
  & = & \sum_{j,k} x_j y_k \Esp\left[ \un_{A_j \cap B_k} \right]\\[2.5mm]
  %
  & = & \sum_{j,k} x_j y_k P(A_j \cap B_k)\\[2.5mm]
  %
  & = & \sum_{j,k} x_j y_k P(X = x_j) P(Y = y_k) \quad \mbox{(de la independencia)}
  \end{eqnarray*}
  %
  dando  el resultado  para  variables  escalonadas. Se  cierra  la prueba  para
  variables positivas  como l\'imite de  crecientes de funciones  escalonadas, y
  variables  reales tratando  las partes  positivas y  negativas aparte.  El caso
  vectorial se deduce trabajando con pares de componentes.
\end{proof}
%
Fijense  de que  la  reciproca es  falsa en  general:
%
\begin{ejemplo}[Uniforme sobre el disco unitario]
\label{Ej:MP:UniformeDiscoUnitario}
%
Sea \ $X = (X_1,X_2)$ \  uniforme sobre el disco unitario \modif{o bola unitaria
  2-dimensional  $\Bset_2$  (ver  notaciones)},  i.e., $p_X(x)  =  \frac{1}{\pi}
\un_{\Sset^2}(x)$.   Claramente, los \  $X_i$ no  pueden ser  independientes del
hecho  que \  $\X_i  =  [-1 \;  1]$  \ y  \  $\X  \ne \X_1  \times  \X_2$ \  (es
estrictamente incluido  en el producto  cartesiano).  Por simetr\'ia  central de
$p_X$, es sencillo ver  que $\Esp[X_1 X_2] = 0$ \ y  similarmente \ $\Esp[X_i] =
0$: a pesar de que los \ $X_i$ \ no sean independientes, $\Cov[X_1,X_2] = 0$.
\end{ejemplo}

\modif{La  consecuencia  de  la  independencia  sobre  la  covarianza}  facilita
frecuentemente los calculos de media.  Volviendo al ejemplo~\ref{Ej:MP:Mixta} de
la pagina~\pageref{Ej:MP:Mixta}:
%
\begin{ejemplo}[Continuaci\'on del ejemplo~\ref{Ej:MP:Mixta}]
\label{Ej:EspMixtaInd}
%
Tratando de la media  de \ $X = V \, \un_{U < \frac12}  + \un_{U \ge \frac12}$ \
con \ $U$ \ y \ $V$ \ variables independientes de distribuci\'on uniformas sobre
$(0 \; 1)$, se calcula gracia a la linealidad y a la independencia, \ $\Esp[X] =
\Esp[V] \Esp\left[  \un_{U < \frac12}  \right] + \Esp\left[ \un_{U  \ge \frac12}
\right] = \frac12  \times \frac12 + \frac12 = \frac34$ \  como lo hemos obtenido
usando  $P_X$  en la  pagina~\pageref{Ej:MP:EspMixta}  o  la  positividad en  la
pagina~\pageref{Ej:MP:EspMixtaPositiva}.
\end{ejemplo}

Una otra  consecuencia de  esta proposici\'on trata  de un conjunto  de vectores
aleatorios \ $\{ X_i \}$ \ y un conjunto de matrices de dimensiones adecuadas,
%
\[
\Cov\left[ \sum_i A_i X_i + B\right] =  \sum_i A_i \Sigma_{X_i} A_i^t + \sum_{j \ne
  i} A_i \Cov[X_i,X_j] A_j^t.
\]
%
En particular, en el caso escalar,
%
\[
\Cov\left[ \sum_i A_i X_i + B \right]  = \sum_i A_i^2 \Var[X_i] + \sum_{j \ne i}
A_i A_j \Cov[X_i,X_j].
\]
%
\underline{Si}   los   $X_i$   son  independientes,   \underline{entonces}   las
covarianzas conjuntas son nulas as\'i que, respectivamente,
%
\[
\Cov\left[ \sum_i  A_i X_i +  B \right] =  \sum_i A_i \Sigma_{X_i}  A_i^t \qquad
\mbox{y}  \qquad  \Cov\left[  \sum_i  A_i   X_i  +  B  \right]  =  \sum_i  A_i^2
\sigma_{X_i}^2.
\]

Si el  teorema da  una implicaci\'on  de la independencia,  de hecho  existe una
reciproca que toma la forma siguiente:
%
\begin{teorema}[Independencia y momentos]
\label{Teo:MP:IndependenciaMomentos}
%
  Sean \ $X$ \ e \ $Y$ \ dos vectores aleatorios. Son independientes si y s\'olo
  si $E[f(X) g(Y)]=E[f(X)] E[g(Y)]$ para todo par  de funciones \ $f$ \ y \ $g$,
  medibles y acotadas de dimensiones adecuadas.
\end{teorema}
%
\begin{proof}
  Se puede referirse a~\cite{Fel71, JacPro03} para unas pruebas rigurosa.  En el
  caso escalar, el  principio consiste a ver \  $f$ \ y \ $g$ \  como limites de
  funciones escalonadas. Para  \ $f(x) = \sum_i a_i \un_{A_i}(x)$ \  y \ $g(y) =
  \sum_j b_j \un_{B_j}(y)$ se obtiene $E[f(X) g(Y)]=E[f(X)] E[g(Y)]$ si y s\'olo
  si $\sum_{i,j} a_i b_j \left( P( (X \in  A_i) \cap (Y \in B_j)) - P(X \in A_i)
    P(Y \in  B_j) \right) = 0$.  B\'asicamente, eso debe  valer para cualquieras
  $A_i, B_j$  y $a_i,  b_j$, as\'i  que el t\'ermino  entre parentesis  debe ser
  cero, lo que es  nada m\'as de la definici\'on de la  independencia de \ $X$ \
  e\ $Y$.  El caso vectorial se entiende por pares de componentes.
\end{proof}

\

Relaciones  tamb\'ien muy  \'utiles  son conocidas  como  {\it Desigualdades  de
  Chebyshev}~\cite{Bie53,  Tch67,  Mar84,  OlkPra58,  Fer82,  Nav13,  StePar17}.
Estas desigualdades dan una cota superior  a la probabilidad de que una cantidad
que  fluct\'ua aleatoriamente  exceda  cierto valor  umbral,  a\'un sin  conocer
detalladamente la forma de la distribuci\'on de probabilidad.
%
\begin{teorema}[Desigualdades de Chebyshev]
\label{Teo:MP:Chebyshev}
%
  Sea un vector aleatorio $d$-dimensional \  $X$ \ y una funci\'on \ $g: \Rset^d
  \mapsto \Rset_+$ \ medible tal que \ $g(X)$ \ sea integrable. Entonces,
  %
  \[
  \forall \: a > 0, \quad P(g(X) \ge a) \: \le \: \frac{\Esp[g(X)]}{a}.
  \]
  %
\end{teorema}
%
\begin{proof}
  Sea \ $\D_a = \{ x \in \X \tq  g(x) \ge a \} \subset \X$. Entonces, $g$ siendo
  non negativa,
  %
  \[
  \Esp[g(X)]  = \int_\X  g(x) \,  dP_X(x) \ge  \int_{\D_a} g(x)  \,  dP_X(x) \ge
  \int_{\D_a} a \, dP_X(x) = a P(X \in \D_a).
  \]
  %
  Se cierra la prueba notando de que \ $(X \in \D_a) \, = \, (g(X) \ge a)$.
\end{proof}
%
Existen varias  formas similares, que son  de hecho casos  particulares de estas
desigualdades.
%
\begin{corolario}[Bienaym\'e--Chebyshev]
\label{Cor:MP:BienaymeChebyshev}
%
  Sea \  $X$ \  un vector aleatorio  $d$-dimensional admitiendo una  esperanza \
  $m_X$ \ y una covarianza \ $\Sigma_X$. Entonces,
  %
  \[
  \forall \:  \varepsilon > 0,  \qquad \modif{P\left( \left\| \Sigma_X^{-\frac12}  (X -
      m_X) \right\| > \varepsilon \right)} \: \le \: \frac{d}{\varepsilon^2}.
 \]
\end{corolario}
%
Viene del teorema incial aplicado a \ $\Sigma_X^{-\frac12} (X - m_X)$, \ $g(x)
= \modif{\| x\ |^2}$ \ y \ $a = \varepsilon^2$.
%
\begin{corolario}[Markov]
\label{Cor:MP:Markov}
%
  Sea \ $X$ \ un vector aleatorio y \ $\varphi \ge 0$ \ una funci\'on no decreciente
  tal que \ \modif{$\varphi(\|X\|)$} \ sea integrable. Entonces,
  %
  \[
  \forall \: \varepsilon \ge  0, \quad \mbox{tal que} \quad \varphi(\varepsilon)
  \ne  0,  \qquad P\left(  \modif{\|  X  \|} >  \varepsilon  \right)  \: \le  \:
  \frac{\Esp\left[      \varphi\left(      \modif{\|      X     \|}      \right)
    \right]}{\varphi(\varepsilon)}.
 \]
\end{corolario}
%
La versi\'on  inicial des  esta desigualdad trataba  de funciones  $\varphi(u) =
u^r, \: r  > 0$. Viene del teorema  incial aplicado a \ \  $g(x) = \varphi\left(
  \modif{\| x  \|} \right)$ \ y \  $a = \varphi(\varepsilon)$, notando  de que \
$\left( \varphi\left( \modif{\|X\|}  \right) \ge \varphi(\varepsilon) \right) \,
= \,  \left( \modif{\| X \|} \ge  \varepsilon \right)$ por la  no decrecencia de
$\varphi$. El caso anterior (una vez  la variable centrada) es nada m\'as que un
caso especial.

Estas  relaciones  afirman que  cuanto  m\'as chica  es  la  varianza, m\'as  se
concentra la variable en torno a su media. Ambas cotas son en general d\'ebiles,
como se lo puede ver en el ejemplo siguiente
%
\begin{ejemplo}
\label{Ej:MP:BienaymeChebyshev}
%
  La  desigualdad  de  Bienaym\'e--Chebyshev   indica  que  la  probabilidad  de
  encontrar  una  fluctuaci\'on superior  a  $\varepsilon  =  3 \sigma_X$,  tres
  desviaciones est\'andar alrededor  de la media, est\'a por  debajo de \ $1/9$;
  el c\'alculo para  una distribuci\'on t\'ipica como la  Gaussiana, \ $p_X(x) =
  \frac{1}{\sqrt{2  \pi} \sigma_X}  \exp\left( -  \frac{(x-m_X)^2}{2}  \right)$ \
  ajusta dicha probabilidad por debajo de $0.003$.
\end{ejemplo}


Una  desigualdad muy  importante que  usaremos frecuentemente  en  el c\'apitulo
siguiente, trata de funciones convexa, y del efecto sobre la media de una vector
aleatorio.
%
\begin{definicion}[Funci\'on convexa]
\label{Def:MP:convexa}
%
  Por definici\'on, una  funci\'on \ $\phi: \X \subset  \Rset^d \mapsto \Rset$ \
  con \ $\X$ un convexo, es convexa si  para cualquier $\pi_1 \in [0 \; 1],
  \pi_2 = 1-\pi_1$ \ y \ $x_1, x_2 \in \Rset^d$,
  %
  \[
  \phi(\pi_1 x_1 + \pi_2 x_2) \le \pi_1 \phi(x_1) + \pi_2 \phi(x_2).
  \]
  %
  $\phi$ es dicha estrictamente convexa  si la desigualdad es estricta, salvo si
  $x_2  =  x_1$.\newline Se  puede  ver  de  que si  \  $\phi$  \ es  dos  veces
  diferenciable, su matriz Hessiana \modif{es simetrica no negativa, $\Hess \phi
    \in  P_d(\Rset)$}.\newline  \modif{Se  muestra  por  recurrencia  au},  para
  cualquier conjunto \ $\{ x_i \}_i$ numerable de elementos de \ $\X$ \ y reales
  positivos \ $\{ \pi_i \}_i$ \ tales que $\sum_i \pi_i = 1$,
  %
  \[
  \phi\left( \sum_i \pi_i x_i \right) \le \sum_i \pi_i \phi(x_i).
  \]
  %
  Dicho con  palabras, la funcci\'on  del barycentro (combinaci\'on  convexa) de
  los $x_i$  es debajo del  barycentro de los  $\phi(x_i)$. Eso es ilustrado  el la
  figura Fig.~\ref{Fig:MP:Convexa}.
\end{definicion}
%
\begin{figure}[h!]
%
\begin{center} \input{TIKZ_MP/Convexidad} \end{center}
%
\leyenda{Ejemplo  de  funci\'on  $\phi$  convexa:  la  cuerda,  conteniendo  los
  barycentros de  $\{ \phi(x_1) \; \phi(x_2) \}$,  es siempre arriba  de la curba,
  \ie funci\'on de los barycentros de $\{ x_1 \; x_2 \}$.}
%
\label{Fig:MP:Convexa}
\end{figure}
%
Intuitivamente, la  media teniendo un sabor  de barycentro, se intuye  de que la
media  de $\phi(X)$ va  a ser  arriva de  la funci\'on  de la  media de  $X$. Es
precisamente el  teorema de Jensen~\footnote{En~\cite{Jen06} se trata  del en el
  caso discreto  y integral; en~\cite{Hol89,  Had93} se encuentran  las primeras
  semillas de  esta desigualdad,  y entre otros~\cite{Jes31:I,  Jes31:II, Per74,
    Rud91}  para versiones  m\'as generales.\label{Foot:SZ:Jensen}}~\cite{Jen06,
 Fel71, Bre88, AshDol99, AthLah06, Coh13}:
%
\begin{teorema}[Desigualdad de Jensen]
\label{Teo:MP:Jensen}
%
  Sea \ $X$ \ integrable y definida  sobre $\X \subset \Rset^d$, convexo y \ $f:
  \X \mapsto \Rset$. Entonces
  %
  \[
  \Esp[\phi(X)] \ge \phi\left( \Esp[X] \right).
  \]
  %
  Si $\phi$ es  estrictamente convexa, la igualdad se alcanza  si y solamente si
  $X$ es determinista casi siempre.
\end{teorema}
%
\begin{proof}
  Sea \ $X = \sum_i x_i  \un_{A_i}$ \ variable escalonada. Entonces \ $\phi(X) =
  \sum_i \phi(x_i) \un_{A_i}$, dando
  %
  \[
  \Esp[\phi(X)]  = \sum_i  P(A_i)  \phi(x_i) \ge  \phi\left(  \sum_i P(A_i)  x_i
  \right) = \phi\left( \Esp[X] \right)
  \]
  %
  con igualdad  (cuando la convexidad es  estricta) si y solamente  si todos los
  $x_i$  son iguales.  Se  cierra la  prueba tomando  $X \ge  0$ como  limite de
  sucesi\'on   de   funciones  escalonadas   (teorema~\ref{Teo:MP:MedibleLimite},
  pagina~\pageref{Teo:MP:MedibleLimite}),  y cualquier $X$  tratando de  la parte
  positiva  y  negativa  (ver  pagina~\pageref{Teo:MP:MedibleLimite}).   El  caso
  vectorial  se   trata  componente  a   componente  para  $X$  en   termino  de
  limite. Tomando  el limite, la  condici\'on $x_i$ todos iguales  vuelve ``casi
  todos'' los $x_i$ deben ser iguales, \ie $X$ debe ser constante casi siempre.
\end{proof}

Terminamos esta secci\'on  con una desigualdad tambi\'en muy  \'util, y conocida
en   los   espacios   de    Hilbert,   conocida   como   {\it   desigualdad   de
  H\"older}~\cite{Hol89, Sho29}:
%
\begin{teorema}[Desigualdad de H\"older]
\label{Teo:MP:Holder}
%
Sean \ $X$ \  e \ $Y$ \ dos vectores aleatorios $d$-dimensionales y  \ $r > 1$ \
real. $r^* > 1$ \ tal que  \ $\frac1r + \frac1{r^*}$ \ es llamado {\it conjugado
  de H\"older} de \ $r$, y
  %
  \[
  \left|  \Esp\left[  X^t Y  \right]  \right| \:  \le  \:  \Esp\left[ \left\|  X
    \right\|_r^r   \right]^{\frac1r}  \:  \Esp\left[   \left\|  Y
    \right\|_{r^*}^{r^*} \right]^{\frac1{r^*}}.
  \]
  %
  Se obtiene la igualda si y solamente si existe un \ $\lambda$ \ tal que \ $X =
  \lambda Y$ \ casi siempre.
\end{teorema}
%
\begin{proof}
  Obviamente, $\left| \Esp\left[ X^t Y \right] \right| \le \Esp\left[ \left| X^t
      Y \right| \right]$. \ Luego, de  la convexidad de la funci\'on \ $-\log$ \
  se obtiene  la desigualdad \ $\log(|a  b|) = \frac1r \log  |a|^r + \frac1{r^*}
  \log |b|^{r^*} \le \log\left( \frac{|a|^r}{r} + \frac{|b|^{r^*}}{r^*} \right)$
  \ con igualdad si y solamente si \ $a$ \ es proporcional a \ $b$. \ Aplicado a
  las componentes de dos vectores \ $a$ \ y \ $b$ \ se obtiene la desigualdad de
  Young    \    $\left|    a^t    b   \right|    \le    \frac{\|a\|_r^r}{r}    +
  \frac{\|b\|_{r^*}^{r^*}}{r^*}$ \  con igualdad si y solamente  si los vectores
  son proporcional. A continuaci\'on, denotando
  %
  \[
  \widetilde{X}         =         \frac{X}{\Esp\left[        \|X\|_r^r
    \right]^{\frac1r}}    \qquad    \mbox{y}    \qquad   \widetilde{Y}    =
  \frac{Y}{\Esp\left[ \|Y\|_{r^*}^{r^*} \right]^{\frac1{r^*}}}
  \]
  %
  tenemos
  %
  \[
  \Esp\left[   \left|   X^t  Y   \right|   \right]   =   \Esp\left[  \left\|   X
    \right\|_r^r   \right]^{\frac1r}  \:  \Esp\left[   \left\|  Y
    \right\|_{r^*}^{r^*}   \right]^{\frac1{r^*}}   \:  \Esp\left[
    \left| \widetilde{X}^t \widetilde{Y} \right| \right].
  \]
  %
  De la desigualdad de Young, se obtiene entonces
  %
  \[
  \Esp\left[   \left|   \widetilde{X}^t   \widetilde{Y}  \right|   \right]   \le
  \frac{\Esp\left[ \left\| \widetilde{X} \right\|_r^r \right]}{r}
  +   \frac{\Esp\left[   \left\|  \widetilde{Y}   \right\|_{r^*}^{r^*}
    \right]}{r^*} = \frac1r + \frac1{r^*} = 1,
  \]
  %
  lo que cierra la prueba.
\end{proof}

Un       corolario       es       conocido       como       desigualdad       de
Cauchy-Bunyakovsky-Schwarz~\footnote{Esta  desigualdad, fue  probada  por Cauchy
  para  sumas   en  1821~\cite{Cau21},   para  integrales  por   Bunyakovsky  en
  1859~\cite{Bou59} y  m\'as elegamente por  Schwarz en 1888~\cite{Sch88}  en un
  enfoque m\'as general. Ver tambi\'en~\cite{Ste04}.} para $p = \frac12$:
%
\begin{corolario}[Desiguladad de Cauchy-Bunyakovsky-Schwarz]
\label{Cor:MP:DesigualdadCauchySchwarz}
%
  Sean \  $X$ \ e \  $Y$ \ dos vectores  aleatorios $d$-dimensionales. Entonces
  %
  \[
  \left|  \Esp\left[  X^t Y  \right]  \right|^2 \:  \le  \:  \Esp\left[ \modif{\left\|  X
    \right\|^2}   \right]  \:  \Esp\left[   \modif{\left\|  Y
    \right\|^2} \right]\modif{.}
  \]
  %
  Se obtiene la igualda si y solamente si existe un \ $\lambda$ \ tal que \ $X =
  \lambda Y$ \ casi siempre.
\end{corolario}
%
Nota:  se puede probar  esta desigualdad  considerando el  polinomio $\Esp\left[
  \modif{\left\| \lambda X + Y \right\|^2}  \right] \ge 0$, del secundo orden en
$\lambda$. Siendo no negativa para cualquier $\lambda$ el discriminente debe ser
no positivo, conduciendo a la desigualdad.

De hecho,  se puede ver $\Esp[X^t  Y]$ como un producto  escalar entre variables
aleatorias. La sola subtileza es que  \modif{$\Esp\left[ \| X \|^2 \right] = 0$}
conduce a $X = 0$ casi siempre, \ie  se puede tener $X \ne 0$ pero con medida de
probabilidad  igual a  cero (ej.  puntos  $\omega$ ``aislados''  en el  contexto
continuo).

\modif{  Un   otro  corolario  de   la  desiguldad  de  H\"older   concierna  el
  comportamiento   de    \   $s   \mapsto    \Esp\left[   \left\| X  \right\|_s^s
  \right]^{\frac{1}{s}}$ \ dado \ $X$:
%
  \begin{corolario}[Crecencia   de   $s   \mapsto  \Esp\big\lbrack   \left\|   X
    \right\|_s^s \big\rbrack^{\frac{1}{s}}$ ]
\label{Cor:MP:CrecenciaSToEspS}
%
  Sean \  $X$ \ vectores  aleatorios $d$-dimensionales. Entonces
  %
  \[
  s \mapsto \Esp\Big[ \left\|  X \right\|_s^s \Big]^{\frac{1}{s}} \quad \mbox{es
    creciente}
  \]
  %
\end{corolario}
\begin{proof}
  Aplicando la desigualdad de H\"older a $\|X\|_s^s$ \ y \ $1$ \ se obtiene
  %
  \[
  \forall \: r  > 1, \quad \Esp\Big[ \|X\|_s^s  \Big] \le \Esp\Big[ \|X\|_{r
      s}^{r s} \Big]^{\frac1r}
  \]
  %
  Se cierra la prueba elevando la  desigualdad a la potencia $\frac1s$ y notando
  que $t = r s > s$.
\end{proof}

Varias  otras desigualdades  se encuentran  en  la literatura  (ver por  ejemplo
en~\cite[y notas  debidas a  Pearson]{Sho29} para unas  de las  m\'as antiguas),
as\'i  que  no  se  puede  ser  exaustivo. Presentamos  en  esta  secci\'on  las
principales.}


%  $\kappa \ge \gamma^2+1$ Shohat
