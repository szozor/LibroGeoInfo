\seccion{Esperanza, momentos, identidades y desigualdades}%funciones generadoras}
\label{Sec:MP:esperanzamomento}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\aver{\emph{introducci\'on...}} %%


% ================================= Media

\subseccion{Media de un vector aleatorio}
\label{Ssec:MP:VectorMedio}


Una  variable aleatoria  $X$  tiene asociado  un  {\it promedio}  o {\it  media}
(tambi\'en  llamado   {\it  valor  esperado  o  de   expectaci\'on  o  esperanza
  matem\'atica})  que se  obtiene pesando  cada valor  de $X$  con la  medida de
probabilidad asociada a ese valor,
%
\begin{definicion}[Media o valor/vector medio]
  Formalmente, la media de  una variable aleatoria $X$ \underline{integrable} es
  definida por
  %
  \[
  \Esp[X] = \int_\Omega X(\omega) \, dP(\omega).
  \]
  %
  Por    el    teorema    de    la    medida    imagen~\ref{Th:MP:MedidaImagen},
  pagina~\pageref{Th:MP:MedidaImagen}, esta media  se escribe tambi\'en a partir
  de la medida de probabilidad $P_X$ como
  %
  \[
  \Esp[X] = \int_\Rset x \, dP_X(x).
  \]
  %
  En  el caso vectorial  $d$-dimensional, hay  que entender  la media,  o vector
  medio, como  un vector de componentes  $i$-\'esima la media  $\Esp[X_i]$ de la
  componente $i$-\'esima $X_i$ de $X$, dando
  %
  \[
  \Esp[X] = \int_{\Rset^d} x \, dP_X(x).
  \]
  %
  A veces,  se encuentra  tambi\'en la notaci\'on  \ $\langle  x \rangle$ \  o \
  $\langle  x  \rangle_{p_X}$  \  para  el  valor  medio,  especialmente  en  la
  literatura de f\'isica.
\end{definicion}
%
La secunda formulaci\'on  del valor medio se proba  sencillamente, empezando por
$X = \un_A$ para unos $A$.   Entonces $P_X = (1-P(A)) \delta_0 + P(A) \delta_1$.
Luego  $\displaystyle \int_\Omega  \un_A(\omega)  dP(\omega) =  P(A) =  (1-P(A))
\times 0 +  P(A) \times 1 = \int_\Rset  x dP_X(x)$.  Se cierra la  prueba con el
teorema~\ref{Th:MP:MedibleLimite} dando cualquier  funci\'on medible como limite
de funciones escalonadas,  y por la definici\'on~\ref{Def:MP:IntegracionReal} de
la integral de cualquier funci\'on medible.

Luego,   de    la   distribuci\'on   marginal    $\displaystyle   P_{X_i}(B)   =
\int_{\Rset^{i-1}   \times   B   \times   \Rset^{d-i}}  dP_X(x)$,   se   obtiene
$\displaystyle  \Esp[X_i] = \int_{\Rset^d}  x_i \,  dP_X(x)$, dando  la \'ultima
formulaci\'on en el caso vectorial.

%$p(x)\,dx$, e integrando sobre el rango permitido de $x$:
%$$
%E[X] = \langle x\rangle = \int_{\Omega} x \ p(x)\,dx \equiv \mu
%$$
%si  la  integral  existe.  La  \emph{esperanza} de  la  variable  aleatoria  $X$
%representa  el valor  medio  que puede  tomar  entre todos  los  eventos de  una
%prueba. 
Una variable  aleatoria $X$ se dice  integrable cuando $E[|X|] <  \infty$. De la
misma  manera, un  vector aleatorio  admite  una media  si y  solamente si  cada
componente  es   integrable.  Veremos  m\'as  adelante   que  existen  variables
aleatorias que no admiten una media.

M\'as  all\'a  de  la formulaci\'on  matem\'atica  de  la  media \  $\Esp[X]$  \
representa la posici\'on alrededor de la cual se ``distribuye las probabilidades
de  occurencia''. Es  el equivalente  probabil\'istico de  centro de  gravedad o
barycentro en mec\'anica.

En  el caso de  variables aleatorias  discretas, de  soporte $\X  = \{  x_i \}$,
inmediatamente
%
\[
\Esp[X] = \sum_i x_i P(X = x_i) = \sum_i x_i p_X(x_i).
\]
%
\noindent  Fijense de  que $\Esp[X]$  no partenece  necesariamente a  $\X$:
%
\begin{ejemplo}
  Sea \  $X$ \  uniforme sobre \  $\X = \{  1 \,  , \, 3  \, ,  \, 7 \}$,  \ie \
  $\forall \,  i \in \X, \quad  P(X = i) =  \frac13$. \ Se calcula  $\Esp[X] = 1
  \times \frac13  + 3 \times \frac13  + 7 \times \frac13  = \frac{11}{3} \not\in
  \X$.  Tampoco es el promedio de los valores extremos.
\end{ejemplo}
%
\noindent Cuando \ $|\X| = +\infty$, \ $X$ \ no es necesariamente integrable:
%
\begin{ejemplo}
  Sea \ $\X = \Nset^*$ \ con  $P(X = n) = \frac{6}{\pi^2 \, n^2}$. \ Claramente,
  $\sum_n\frac{6}{\pi^2 \, n}$ diverge, as\'i que $X$ no tiene una media.
\end{ejemplo}

En el caso de vectores  aleatorios continuos, obtenemos la expresi\'on siguiente
de la media (o vector medio):
%
\[
\Esp[X] = \int_{\Rset^d} x \, p_X(x) \, dx.
\]
%
\noindent Las mismas observaciones que  hicimos en el caso discreto se encuentra
en el caso continuo:
%
\begin{ejemplo}
  Sea \ $X$  \ de densidad de probabilidad  \ $p_X(x) = \frac12 \un_{[0  \, ; \,
    1)}(x) +  \frac{3 \sqrt{x-2}}{4}  \un_{[2 \, ;  \, 3)}(x)$ \  como ilustrado
  figura                                         Fig.~\ref{Fig:MP:ProbaContinua},
  pagina~\pageref{Fig:MP:ProbaContinua}. Se  calcula \ $\Esp[X]  = \frac{31}{20}
  \not\in \X = [0 \, ; \, 1] \cup [2 \, ; \, 3]$.
\end{ejemplo}
%
\begin{ejemplo}
  Un ejemplo  de vector aleatorio no  teniendo media es  dado en el caso  de una
  distribuci\'on de Cauchy-Lorentz (ver  m\'as adelante) \ $\displaystyle p_X(x)
  = \frac{\alpha}{\left( 1 + x^t x \right)^{\frac{d+1}{2}}}$ \ donde $\alpha$ es
  un factor de normalizaci\'on.
\end{ejemplo}

En el caso general, para calcular  la media, hay que pasar por la distribuci\'on
$P_X$, como en el ejemplo~\ref{Ej:MP:Mixta}  pagina~\pageref{Ej:MP:Mixta}:
%
\begin{ejemplo}[Continuaci\'on del ejemplo~\ref{Ej:MP:Mixta}]\label{Ej:MP:EspMixta}
  Sea $X = V \,  \un_{U < \frac12} + \un_{U \ge \frac12}$ \ con  \ $U$ \ y \ $V$
  variables aleatorias independientes de distribuci\'on uniformas sobre \ $[0 \,
  ;  \, 1)$,  \ie  $p_U(x) =  \un_{[0  \, ;  \,  1)}(x)$. \  De \  $X  \in B  \:
  \Leftrightarrow  \: \left(  \left( U  < \frac12  \right) \cap  \left( V  \in B
    \right) \right) \, \cup \, \left( \left( U \ge \frac12 \right) \cap \left( 1
      \in B \right) \right)$,  \ del hecho de que los eventos  de la uni\'on son
  incompatibles y de  la independencia de $U$ y $V$ (o  saliendo de la funci\'on
  de repartici\'on), se obtiene $P_X(B) = \frac12 P_V(B) + \frac12 \delta_1(x)$.
  A  continuaci\'on, \  $\displaystyle \Esp[X]  = \frac12  \int_\Rset  dP_V(x) +
  \frac12 \int_\Rset  d\delta_1(x) = \frac12  \int_\Rset p_V(x) \, dx  + \frac12
  \times 1 = \frac12 \int_0^1 dx + \frac12 = \frac34$.
\end{ejemplo}

\

Una nota intersante es  de que, en el caso escalar, si \  $X \ge 0$ \ admitiendo
una media, se obtiene
%
\[
\Esp[X]  = \int_{\Rset_+} P(X  > t)  \, dt  = \int_{\Rset_+}  \left( 1  - F_X(t)
\right) \, dt.
\]
%
Se proba saliendo  de \ $\displaystyle x = \int_0^x  dt = \int_{\Rset_+} \un_{(t
  \, ;  \, +\infty)}(x)  \, dt$  \ dando \  $\displaystyle \Esp[X]  = \int_\Rset
\left( \int_{\Rset_+}  \un_{(t \, ; \,  +\infty)}(x) \, dt \right)  \, dP_X(x) =
\int_{\Rset_+} \left( \int_\Rset \un_{(t \, ; \, +\infty)}(x) \, dP_X(x) \right)
\,    dt$    \    por    el    teorema    de    Fubini    Th.~\ref{Th:MP:Fubini}
pagina~\pageref{Th:MP:Fubini}. Se  cierra la  prueba observando que  la integral
interior es nada  m\'as que $P(X > t)$.   En el caso discreto con  $\X = \Nset$,
viene  inmediatamente \ $\displaystyle  \sum_{n \in  \Nset} P(X>n)$  que podemos
probar directamente  saliendo de $P(X  = n) =  P(X>n)- P(X>n-1)$. En el  caso de
variable   admitiendo  una   densidad,   se  obtiene   tambi\'en  haciendo   una
integraci\'on  por   partes~\footnote{El  el   casi  discreto,  hay   que  tener
  precauciones separando la series de una diferencia de terminos. En el caso $X$
  continuo admitiendo una densidad, hay  que estudiar bien el coomportamiento de
  $t \mapsto t (1-F_X(t))$ al infinito.}

Esta f\'ormula se aplica al ejemplo~\ref{Ej:MP:Mixta} que tratamos:
%
\begin{ejemplo}[Continuaci\'on del ejemplo~\ref{Ej:MP:Mixta}]\label{Ej:MP:EspMixtaPositiva}
  Sea $X = V \,  \un_{U < \frac12} + \un_{U \ge \frac12}$ \ con  \ $U$ \ y \ $V$
  variables aleatorias independientes de distribuci\'on uniformas sobre \ $[0 \,
  ;  \,  1)$. Obtuvimos  pagina~\pageref{Ej:MP:Mixta}  \  $F_X(x) =  \frac{x}{2}
  \un_{[0 \,  ; \, 1)}(x)  + \un_{[1 \,  ; \, +\infty)}(x)$.   A continuaci\'on,
  reobtenemos \ $\displaystyle \Esp[X] = \int_0^1 \left( 1 - \frac{x}{2} \right)
  \, dx = \frac34$.
\end{ejemplo}.

\

Terminamos  esta  secci\'on con  la  propiedad  de  linealidad de  la  esperanza
matem\'atica $\Esp$,  como consecuencia de  la linealidad de la  integraci\'on y
definici\'on de la distribuci\'on  marginal: para cualquier conjunto de vectores
aleatorios \ $\{  X_i \}$ \ integrables  y cualquieras matrices \ $\{  C_i \}$ \
dadas  de  dimensiones compatibles  con  las  de \  $X$  \  (incluyendo el  caso
escalar),
%
\[
\Esp\left[ \sum_i C_i X_i \right] = \sum_i C_i \Esp\left[ X_i \right]
\]
%
(la integrabilidad de la suma se proba a partir de la desigualdad triangular).



% ================================= Momentos

\subseccion{Momentos de un vector aleatorio}
\label{Ssec:MP:Momentos}

Si $X$ es una variable aleatoria, para cualquier funci\'on medible $f$, \ $f(X)$
tambi\'en lo es.   Se puede entonces definir su valor medio,  si existe. A pesar
de necesitar evaluar  la distribuci\'on de probabilidad de $Y  = f(X)$, el valor
medio se calcula a partir del de $X$:
%
\begin{teorema}[Teorema de transferencia]
  Sea  \ $X$  \ un  vector  aleatorio $d$-dimensional  y \  $f: \Rset^d  \mapsto
  \Rset^{d'}$ \ una funci\'on medible tal que $f(X)$ sea integrable. Entonces
  %
  \[
  \Esp\left[   f(X)  \right]   =  \int_\Omega   f(X(\omega))  \,   dP(\omega)  =
  \int_{\Rset^{d'}} f(x) \, dP_X(x).
  \]
  %
  En particular, en el caso $\X = X(\Omega)$ discreto se obtiene
  %
  \[
  \Esp\left[ f(X) \right] = \sum_i f(x_i) P(X = x_i)
  \]
  %
  y para $X$ continuo admitiendo una densidad de probabilidad
  %
  \[
  \Esp\left[ f(X) \right] = \int_{\Rset^d} f(x) \, p_X(x) \, dx.
  \]
\end{teorema}
%
\begin{proof}
  Sea $B \in \B(\Rset^d)$ \ y consideramos \ $f(x) = \un_B(x)$. Entonces, $\Y
  = \{  0 \, , \,  1 \}$ y inmediatamente  \[P_Y = P_X(B) \, \delta_1 + (1-P_X(B))
  \, \delta_0.\] Entonces
  %
  \[
  \Esp[f(X)]  =  \int_\Rset  P_X(B)  \,  d\delta_1 +  \int_\Rset  (1-P_X(B))  \,
  d\delta_0 = P_X(B) = \int_{\Rset^d} \un_B(x) \, dP_X(x).
  \]
  %
  En el caso  $d' = 1$, para $f  \ge 0$, se cierra entonces la  prueba usando el
  teorema~\ref{Th:MP:MedibleLimite}         pagina~\pageref{Th:MP:MedibleLimite},
  escribiendo \  $f$ \  como l\'imite creciente  de una sucesi\'on  de funciones
  escalonadas,  y   la  definici\'on  Def.~\ref{Def:MP:IntegracionReal}   de  la
  integraci\'on real. El  caso $d' > 1$ es  nada mas que $d' =  1$, componente a
  componente.
\end{proof}

De  manera general,  estas medias  son llamadas  {\it momentos}  de  la variable
aleatoria $X$. Los momentos relevantes usuales
%\underline{en el caso escalar} ($d = 1$) 
 son los siguientes:
%
\begin{itemize}
\item para  el ``monomio'' $f(x) =  x^{\otimes r}$ producto tensiorial  de $x$ \
  $r$ veces~\footnote{Recuerdense  de que $x  \otimes x$ es una  matriz teniendo
    como  componentes   $x_i  x_j$;  entonces  $x^{\otimes  r}$   en  un  tensor
    $r$-dimensionale teniendo como componentes $ \displaystyle \left[ x^{\otimes
        r}  \right]_{i_1,\ldots,i_r} =  \prod_j x_{i_j}$.}   \ siendo  \  $r \in
  \Nset^*$, se  obtiene el tensor de los  $r${\it-\'esimo momentos (ordinarios)}
  de $X$:
  %
  \[
  m_r \equiv \Esp\left[  X^{\otimes r} \right] = \int_{\Rset^d}  x^{\otimes r} \
  dP_X(x)
  \]
  %
  que tiene unidades de $\prod_j X_{i_j}$  (de $X_i^r$ si los componentes de $X$
  tienen la misma ``unidad'').  Se escribe tambi\'en
  %
  \[
  m_{r_1,\ldots,r_d}  =  \Esp\left[   \prod_{i=1}^d  X_i^{r_i}  \right]  \quad
  \mbox{con} \quad \sum_i r_i = r.
  \]
  %
  Se puede  incluir el caso $r=0$ con  la convenci\'on $x^{\otimes 0}  = 1$, que
  corresponde  a  la condici\'on  de  normalizaci\'on:  \  $\displaystyle m_0  =
  \int_\Rset dP_X(x)  = 1$.   La media es  el primer  momento: $m_1 =  \Esp[X] =
  m_X$.  T\'ipicamente,  los primeros momentos  son m\'as relevantes que  los de
  \'ordenes mayores, para la caracterizaci\'on de una distribuci\'on.  Para $r =
  2$, en el caso  escalar, el momento de orden 2 es  el an\'alogo del momento de
  inercia de la mec\'anica.\newline Por ejemplo, para la distribuci\'on uniforme
  $p_X(x)    =    \frac{1}{b-a}$    en    el    intervalo    $[a,b]$,    resulta
  $m_r=\frac{b^{r+1}-a^{r+1}}{(r+1)(b-a)}$.      En    particular,     $m_1    =
  \frac{a+b}{2}$, valor medio del  intervalo.\newline Fijense de que $X^{\otimes
    r}$  no es  siempre integrable,  por ejemplo,  el el  caso con  densidad, si
  $p_X(x)$ tiene soporte (semi)infinito,  necesariamente la funci\'on $p_X$ debe
  tender  a 0  cuando $|x|\rightarrow\infty$,  donde $|\cdot|$  denota  la norma
  euclideana.  Si  $p_X(x)$ es {\it de largo  alcance}, en el sentido  de que no
  cae a  0 suficientemente r\'apido con  $x$ para $x$  grandes, algunos momentos
  pueden  no  existir.   Por  ejemplo,  la  distribuci\'on  de  probabilidad  de
  Cauchy--Lorentz   (o  funci\'on   de  Breit--Wigner),   dada  por   $p_X(x)  =
  \frac{\alpha}{\left(  1 +  (x-x_0)^t R^{-1}  (x-x_0) \right)^{\frac{d+1}{2}}}$
  sobre $\Rset^d$, con  la matriz cuadrada \ $R  > 0, \: x_0 \in \Rset^d$  \ y \
  $\alpha >  0$ \ coeficiente de  normalizaci\'on, no tiene  momentos finitos de
  orden $r \geq 1$.
%
\item En el  caso de variables discretas $X$ sobre $\X  = \Nset$, resulta \'util
  introducir el $r$-\'esimo {\it momento factorial} de $X$ ($r \in \Nset$) mediante
  %
  \[
  \Esp\left[ X^{(r)} \right] \equiv  \Esp\left[ \prod_{k=0}^{r-1} (X-k) \right]
  = \sum_{n = r}^\infty \frac{n!}{(n-r)!} \, P(X = n).
 \]
 %
 (usamos la convenci\'on usual \ $\prod_0^{-1} = 1$).
%
\item Los {\it momentos centrales} o {\it cumulantes} se definen alrededor de la
  media $\Esp[X]$,  \ie, como el tensor  de los $r$-\'esimo momentos  de la {\it
    desviaci\'on} \ $\Delta X \equiv X-\Esp[X]$:
  %
  \[
  \zeta_r \equiv \Esp\left[  \left( X - \Esp[X] \right)^{\otimes r} \right].
  \]
  %
  Se escribe tambi\'en
  %
  \[
  \zeta_{r_1,\ldots,r_d}   =   \Esp\left[   \prod_{i=1}^d   \left(   X_i-\Esp[X_i]
    \right)^{r_i} \right] \quad \mbox{con} \quad \sum_i r_i = r.
  \]
  %
  Se deduce que si la  distribuci\'on de probabilidad satisface a una simetr\'ia
  central  con respecto a  la media,  \ie \  $X-m_X \egald  -(X-m_X)$ \  donde \
  $\egald$ \ significa que los vectores aleatorios tiene la misma distribuci\'on
  de probabilidad, entonces todos los momentos centrales impares son nulos.  Los
  momentos (centrales) brindan medidas que caracterizan la distribuci\'on.
  %
  \begin{enumerate}
  \item El primer momento, o media:
   \[
    m_X = \Esp[X].
   \]
  %
 \item El segundo momento central se conoce como {\it matriz de covarianza}.  En
   el caso escalar, hablamos de {\it varianza}, o {\it dispersi\'on} o tambi\'en
   {\it desviaci\'on cuadr\'atica media}.
  %
  \[
  \Sigma_X \equiv  \Cov[X] \equiv  \zeta_2 = \Esp\left[  \left( X -  m_X \right)
    \left( X - m_X \right)^t \right].
  \]
  %
  En el caso escalar, la varianza se escribe en general
  %
  \[
  \Var[X] = \Esp\left[ \left( X  - m_X \right)^2 \right]
  \]
  %
  y  es  una  medida  del  cuadrado  del  ancho  efectivo  de  una  densidad  de
  probabilidad  (o vector  de  probabilidad).  Para dos  componentes  $i \ne  j$
  hablamos de {\it covarianza entre variables}, y escribimos
  %
  \[
  \Cov[X_i,X_j]  =  \Esp\left[ \left(  X_i  -  m_{X_i}  \right) \left(  X_j  -
      m_{X_j} \right) \right].
  \]
  %
  La matriz de covarianza tiene las varianzas de los $X_i$ en su diagonal, y las
  covarianzas entre  componentes en las componentes no  diagonales.  Es sencillo
  ver de que \ $\Cov[X]$ es sim\'etrica, por construcci\'on, y de que \ $\Cov[X]
  \ge 0$ \ donde $A \ge 0$  significa que la matriz es, definida no negativa (en
  el caso escalar la varianza es no negativa), con igualdad s\'olo cuando $P_X =
  \delta_{x_0}$ para  un $x_0$ dado, esto  es, cuando no hay  incerteza sobre el
  resultado.    De    la   desigualdad   de    Cauchy-Bunyakovsky-Schwarz   (ver
  corolario~\ref{Cor:MP:CBS},      pagina~\pageref{Cor:MP:CBS})     se     proba
  sencillamente  de  que
  %
  \[
  \left| \Cov[X_i,X_j] \right|^2 \le \sigma_{X_i}^2 \sigma_{X_j}^2,
  \]
  %
  as\'i que  se define  tambi\'en el {\it  coeficiente de correlaci\'on}  que es
  adimensional   y   toma    valores   entre   $-1$   (variables   completamente
  anticorrelacionadas) y 1 (variables completamente correlacionadas) como:
  %
  \[
  \rho_{ij} = \rho_{ji} \equiv \frac{\Cov[X_i,X_j]}{\sigma_{X_i} \sigma_{X_j}}.
  \]
  %
  Como ejemplo, dadas $X_1$ y $X_2 = a X_1 + b$ que fluct\'uan en fase ($a>0$) o
  al rev\'es ($a<0$),  se tiene $\Delta X_2 = a \Delta  X_1$, luego $\rho_{12} =
  \frac{a}{|a|} = \pm  1$.\newline Tambi\'en, se puede ver  de que \[\Var[|X|] =
  \Tr \Sigma_X,\] $\Tr$  siendo la traza y \ $|\cdot|$ \  la norma euclideana de
  un  vector.  La  covarianza  est\'a bien  definida  si $|X|$  es una  variable
  aleatoria de  cuadrado integrable,  esto es, cuando  $E[|X|^2] <  \infty$.  Se
  proba sencillamente (desallorando el ``cuadrado'' y usando la linealidad de la
  esperanza) de que
  %
  \[
  \Cov[X] = \Esp\left[ X X^t \right] - m_X m_X^t
  \]
  %
  conocido como  {\it teorema  de K\"onig-Huygens}.  En  el caso escalar,  es el
  equivalente del teorema de Huygens de la mec\'anica relacionando el momento de
  inertia de un solido con respeto al origen en funci\'on del momento de inertia
  con respeto al centro de masa. Adem\'as, inmediatamente,
  %
  \[
  \forall \: A \in \Rset^{d' \times d}, \:  b \in \Rset^d, \quad \Cov[A X + b] =
  A \Cov[X] A^t.
  \]
  %
  En el caso escalar, $d = 1$,  lo que es conocido tambi\'en como el {\it ancho}
  de una distribuci\'on est\'a dado por la {\it desviaci\'on est\'andar}
  %
  \[
  \sigma_X = \sqrt{\Var[X]}
  \]
  %
  tiene  las mismas  unidades de  $X$,  y se  usa para  normalizar los  momentos
  centrales  de orden  superior.  El  {\it ancho  relativo} es  otra  medida que
  caracteriza    la   distribuci\'on,    dado   por    $\frac{\sigma_X}{m_X}   =
  \sqrt{\frac{\Esp\left[  X^2 \right]}{m_X^2}-1}$ cuando  $m_X \ne  0$. \newline
  Dado un vector aleatorio $X$, teniendo en cuenta que los dos primeros momentos
  dan  las   caracter\'isticas  m\'as   importantes  de  la   distrubuci\'on  de
  probabilidad,  puede  resultar   conveniente  hacer  una  transformaci\'on  de
  variable  aleatoria  a  la   llamada  {\it  variable  est\'andar}:  $Y  \equiv
  \Sigma_X^{-\frac12} \left(  X - m_X \right)$, donde  $\Sigma^{-\frac12}$ es la
  \'unica matriz  sim\'etrica definida positiva tal  que su cuadrado  es igual a
  $\Sigma^{-1}$~\cite{HorJoh13, MagNeu99}  que entonces tiene media igual  a 0 y
  una  matriz  de  covarianza  igual  al  identidad $I$  (en  el  caso  escalar,
  desviaci\'on est\'andar igual a 1).
  %
\item En  el caso  escalar, el  tercer momento central  permite definir  el {\it
    coeficiente de asimetr\'ia} (o skewness en ingles)~\cite{Pea05}:
  \[
  \Asim[X]  \equiv \gamma_X  \equiv \Esp\left[  \left( \frac{X  - m_X}{\sigma_X}
    \right)^3 \right] = \frac{\zeta_3}{\sigma_X^3},
  \]
  %
  momento de orden  3 de la variable estandar, que  resulta adimensional y puede
  tener  signo positivo  o negativo,  anul\'andose para  distribuciones  que son
  sim\'etricas respecto del valor medio.
  %
\item  En  el caso  escalar,  el  cuarto momento  central  da  lugar  a la  {\it
    curtosis}~\cite{Pea05, Wes14}:
  \[
  \Curt[X]  \equiv \kappa_X  \equiv \Esp\left[  \left( \frac{X  - m_X}{\sigma_X}
    \right)^4 \right] = \frac{\zeta_4}{\sigma_X^4},
  \]
  % 
  momento de orden  4 de la variable estandar,  que posibilita diferenciar entre
  distribuciones  altas y  angostas.   Veremos  m\'as adelante  de  que para  la
  densidad  Gausiana  $p_X(x)  =  \frac{1}{\sqrt{2  \pi}  \sigma}  \exp\left(  -
    \frac{(x-m)^2}{2 \, \sigma^2} \right)$, \ $m_X = m, \: \sigma_X = \sigma, \:
  \gamma_X =  0, \: \kappa_X  = 3$. Se  dice de que $p_X$  es alta y  angosta, o
  sub-gausiana,  o {\it con  colas livianas}  o tambi\'en  platic\'urtica cuando
  $\kappa_X < 3$,  y se dice bajas  y anchas o sobre-gausiana, o  {\it con colas
    pesadas} o tambi\'en leptoc\'urtica cuando  $\kappa_X > 3$ (para $\kappa_X =
  3$  la distribuci\'on es  a veces  dicha mesoc\'urtica).   A veces,  se define
  entonces la {\it curtosis por exceso} \  $\Curt[X] - 3$.  M\'as que el pico de
  distribuci\'on, la curtosis describe  las colas de una distribuci\'on (pesadas
  o livianas)~\cite{Wes14}.
  \end{enumerate}
  %
  Fijense de que, en  el contexto escalar $d = 1$, se  vinculan los cumulantes y
  los momentos ordinarios directamente de las definiciones:
  %
  \[
  \zeta_r = \sum_{s=0}^r \binom{r}{s} \left( - m_X \right)^{r-s} m_s
  \]
  %
  para cualquier  $r \in  \Nset$, siendo $m_0  = \zeta_0  = 1$.  Por  ejemplo, \
  $\zeta_2  =  m_2  -  m_1^2$  \   que  es  nada  m\'as  que  la  relaci\'on  de
  K\"onig-Huyggens, mientras que \ $\zeta_3 = m_3 - 3 m_1 m_2 + 2 m_1^3$.  En el
  contexto multivariado, la relaci\'on momentos-cumulantes toma la expresi\'on
  %
  \[
  \zeta_{r_1,\ldots,r_d}  = \sum_{s_1  =  0}^{r_1} \cdots  \sum_{s_d =  0}^{r_d}
  \left(  \prod_{i=1}^d  \binom{r_i}{s_i}  \left(  -  m_{X_i}  \right)^{r_i-s_i}
  \right) \, m_{s_1,\ldots,s_d}.
  \]
\end{itemize}

Tratando de covarianza, m\'as generalmente, para dos vectores aleatorios \ $X$ \
e \ $Y$, se define la matriz de covarianza conjunta como
%
\[
\Sigma_{X,Y} \equiv  \Cov[X,Y] =  \Esp\left[ \left( X  - m_X\right) \left(  Y -
    m_Y \right)^t \right] = \Esp\left[ X Y^t \right] - m_X m_Y^t.
\]
%
Esta matriz contiene las covarianzas $\Cov[X_i,Y_j]$.


% ================================= Independencia, identidades y desigualdades

\subseccion{Independencia, identidades y desigualdades}
\label{Ssec:MP:MomentosDesigualdades}

Una primera relaci\'on interesante concierna el caso de variables independientes
y como se comporta la covarianza de estas:
%
\begin{propuesta}
  Sean  \  $X$  \ e  \  $Y$  \  dos  vectores  aleatorios integrables.   Si  son
  independientes, entonces
  %
  \[
  \Esp[X Y^t] = \Esp[X] \Esp[Y]^t \qquad \mbox{\ie} \qquad \Cov[X,Y] = 0.
  \]
  %
  En  particular, para  $X$  con componentes  independientes,  $\Cov[X]$ es  una
  matriz diagonal.
\end{propuesta}
%
\begin{proof}
  Sean \ $X = \sum_j x_j \un_{A_j}$  \ e \ $Y = \sum_k y_k \un_{B_k}$ \
  dos variables escalonadas. Entonces, \ $A_j =  (X = x_j)$ \ y \ $B_k = (Y
  = y_k)$. Luego
  %
  \begin{eqnarray*}
  \Esp[X Y] & = & \sum_{j,k} x_j y_k \Esp[\un_{A_j} \un_{B_k}]\\[2.5mm]
  %
  & = & \sum_{j,k} x_j y_k \Esp[\un_{A_j \cap B_k}]\\[2.5mm]
  %
  & = & \sum_{j,k} x_j y_k P(A_j \cap B_k)\\[2.5mm]
  %
  & = & \sum_{j,k} x_j y_k P(X = x_j) P(Y = y_k) \quad \mbox{(de la independencia)}
  \end{eqnarray*}
  %
  dando  el resultado  para  variables  escalonadas. Se  cierra  la prueba  para
  variables positivas  como l\'imite de  crecientes de funciones  escalonadas, y
  variables  reales tratando  las partes  positivas y  negativas aparte.  El caso
  vectorial se deduce trabajando con pares de componentes.
\end{proof}
%
Fijense  de que  la  reciproca es  falsa en  general:
%
\begin{ejemplo}[Uniforme sobre el disco unitario]
  Sea \  $X =  (X_1,X_2)$ \ uniforme  sobre el  disco unitario, i.e.,  $p_X(x) =
  \frac{1}{\pi} \un_{\Sset^2}(x)$ \  con \ $\Sset^2 = \{  (x_1,x_2) \in \Rset^2:
  \:  x_1^2  +  x_2^2  \le  1  \}$.  Claramente,  los  \  $X_i$  no  pueden  ser
  independientes del hecho de que  \ $\X_i = [-1 \, , \, 1]$ \  y \ $\X \ne \X_1
  \times \X_2$  \ (es  estrictamente incluido en  el producto  cartesiano).  Por
  simetr\'ia central de  $p_X$, es sencillo ver  de que $\Esp[X_1 X_2] =  0$ \ y
  similarmente  \ $\Esp[X_i]  =  0$:  a pesar  de  que los  \  $X_i$  \ no  sean
  independientes, $\Cov[X_1,X_2] = 0$.
\end{ejemplo}

Esta implicaci\'on facilita frecuentemente  los calculos de media.  Volviendo al
ejemplo~\ref{Ej:MP:Mixta}  de la  pagina~\pageref{Ej:MP:Mixta}:
%
\begin{ejemplo}[Continuaci\'on del ejemplo~\ref{Ej:MP:Mixta}]\label{Ej:EspMixtaInd}
  Tratando de la media de \ $X = V \, \un_{U < \frac12} + \un_{U \ge \frac12}$ \
  con \  $U$ \ y  \ $V$ \  variables independientes de  distribuci\'on uniformas
  sobre $(0 \, ; \, 1)$, se calcula gracia a la linealidad y a la independencia,
  \ $\Esp[X]  = \Esp[V]  \Esp[\un_{U < \frac12}]  + \Esp[\un_{U \ge  \frac12}] =
  \frac12 \times  \frac12 + \frac12 =  \frac34$ \ como lo  hemos obtenido usando
  $P_X$   en  la   pagina~\pageref{Ej:MP:EspMixta}  o   la  positividad   en  la
  pagina~\pageref{Ej:MP:EspMixtaPositiva}.
\end{ejemplo}

Una otra  consecuencia de  esta proposici\'on trata  de un conjunto  de vectores
aleatorios \ $\{ X_i \}$ \ y un conjunto de matrices de dimensiones adecuadas,
%
\[
\Cov\left[ \sum_i A_i X_i + B\right] =  \sum_i A_i \Sigma_{X_i} A_i^t + \sum_{j \ne
  i} A_i \Cov[X_i,X_j] A_j^t.
\]
%
En particular, en el caso escalar,
%
\[
\Cov\left[ \sum_i A_i X_i + B \right]  = \sum_i A_i^2 \Var[X_i] + \sum_{j \ne i}
A_i A_j \Cov[X_i,X_j].
\]
%
\underline{Si}   los   $X_i$   son  independientes,   \underline{entonces}   las
covarianzas conjuntas son nulas as\'i que, respectivamente,
\[
\Cov\left[ \sum_i  A_i X_i +  B \right] =  \sum_i A_i \Sigma_{X_i}  A_i^t \qquad
\mbox{y}  \qquad  \Cov\left[  \sum_i  A_i   X_i  +  B  \right]  =  \sum_i  A_i^2
\sigma_{X_i}^2.
\]

Si el  teorema da  una implocaci\'on  de la independencia,  de hecho  existe una
reciproca que toma la forma siguiente:
%
\begin{teorema}
  Sean \ $X$ \ e \ $Y$ \ dos vectores aleatorios. Son independientes si y s\'olo
  si $E[f(X) g(Y)]=E[f(X)] E[g(Y)]$ para todo par  de funciones \ $f$ \ y \ $g$,
  medibles y acotadas de dimensiones adecuadas.
\end{teorema}
%
\begin{proof}
  Se puede referirse a~\cite{Fel71, JacPro03} para unas pruebas rigurosa.  En el
  caso escalar, el  principio consiste a ver \  $f$ \ y \ $g$ \  como limites de
  funciones escalonadas. Para  \ $f(x) = \sum_i a_i \un_{A_i}(x)$ \  y \ $g(y) =
  \sum_j b_j \un_{B_j}(y)$ se obtiene $E[f(X) g(Y)]=E[f(X)] E[g(Y)]$ si y s\'olo
  si $\sum_{i,j} a_i b_j \left( P( (X \in  A_i) \cap (Y \in B_j)) - P(X \in A_i)
    P(Y \in  B_j) \right) = 0$.  B\'asicamente, eso debe  valer para cualquieras
  $A_i, B_j$  y $a_i,  b_j$, as\'i  que el t\'ermino  entre parentesis  debe ser
  cero, lo que es  nada m\'as de la definici\'on de la  independencia de \ $X$ \
  e\ $Y$.  El caso vectorial se entiende por pares de componentes.
\end{proof}

\

Relaciones  tamb\'ien muy  \'utiles  son conocidas  como  {\it Desigualdades  de
  Chebyshev}~\cite{Bie53,  Tch67,  Mar84,  OlkPra58,  Fer82,  Nav13,  StePar17}.
Estas desigualdades dan una cota superior  a la probabilidad de que una cantidad
que  fluct\'ua aleatoriamente  exceda  cierto valor  umbral,  a\'un sin  conocer
detalladamente la forma de la distribuci\'on de probabilidad.
%
\begin{teorema}[Desigualdades de Chebyshev]
  Sea un vector aleatorio $d$-dimensional \  $X$ \ y una funci\'on \ $g: \Rset^d
  \mapsto \Rset_+$ \ medible tal que \ $g(X)$ \ sea integrable. Entonces,
  %
  \[
  \forall \: a > 0, \quad P(g(X) \ge a) \: \le \: \frac{\Esp[g(X)]}{a}.
  \]
  %
\end{teorema}
%
\begin{proof}
  Sea \ $\D_a = \{ x \in \X: \: g(x) \ge a \} \subset \X$. Entonces, $g$ siendo non negativa,
  %
  \[
  \Esp[g(X)]  = \int_\X  g(x) \,  dP_X(x) \ge  \int_{\D_a} g(x)  \,  dP_X(x) \ge
  \int_{\D_a} a \, dP_X(x) = a P(X \in \D_a).
  \]
  %
  Se cierra la prueba notando de que \ $(X \in \D_a) \, = \, (g(X) \ge a)$.
\end{proof}
%
Existen varias  formas similares, que son  de hecho casos  particulares de estas
desigualdades.
%
\begin{corolario}[Bienaym\'e--Chebyshev]
  Sea \  $X$ \  un vector aleatorio  $d$-dimensional admitiendo una  esperanza \
  $m_X$ \ y una covarianza \ $\Sigma_X$. Entonces,
  %
  \[
  \forall \:  \varepsilon > 0,  \qquad P\left( \left| \Sigma_X^{-\frac12}  (X -
      m_X) \right| \right) > \varepsilon \: \le \: \frac{d}{\varepsilon^2}.
 \]
\end{corolario}
%
Viene del teorema incial aplicado a \ $\Sigma_X^{-\frac12} (X - m_X)$, \ $g(x)
= |x|^2$ \ y \ $a = \varepsilon^2$.
% , notando de que \  $\left( \left\| \Sigma_X^{-\frac12} (X - m_X) \right\|^2
%   \ge \varepsilon^2 \right) \, = \, (\|X\| \ge \varepsilon)$.
%
%
\begin{corolario}[Markov]
  Sea \ $X$ \ un vector aleatorio y $\varphi \ge 0$ una funci\'on no decreciente
  tal que $\varphi(|X|)$ sea integrable. Entonces,
  %
  \[
  \forall \: \varepsilon \ge  0, \quad \mbox{tal que} \quad \varphi(\varepsilon)
  \ne     0,     \qquad     P(|X|     >     \varepsilon)     \:     \le     \:
  \frac{\Esp[\varphi(|X|)]}{\varphi(\varepsilon)}.
 \]
\end{corolario}
%
La versi\'on  inicial des  esta desigualdad trataba  de funciones  $\varphi(u) =
u^r, \: r > 0$. Viene del teorema  incial aplicado a \ \ $g(x) = \varphi(|x|)$ \
y  \   $a  =  \varphi(\varepsilon)$,   notando  de  que  \   $(\varphi(|X|)  \ge
\varphi(\varepsilon)) \,  = \, (|X| \ge  \varepsilon)$ por la  no decrecencia de
$\varphi$. El caso anterior (una vez  la variable centrada) es nada m\'as que un
caso especial.

Estas  relaciones  afirman que  cuanto  m\'as chica  es  la  varianza, m\'as  se
concentra la variable en torno a su media. Ambas cotas son en general d\'ebiles,
como se lo puede ver en el ejemplo siguiente
%
\begin{ejemplo}
  La  desigualdad  de  Bienaym\'e--Chebyshev   indica  que  la  probabilidad  de
  encontrar  una  fluctuaci\'on superior  a  $\varepsilon  =  3 \sigma_X$,  tres
  desviaciones est\'andar alrededor  de la media, est\'a por  debajo de \ $1/9$;
  el c\'alculo para  una distribuci\'on t\'ipica como la  Gaussiana, \ $p_X(x) =
  \frac{1}{\sqrt{2  \pi} \sigma_X}  \exp\left( -  \frac{x-m_X)^2}{2}  \right)$ \
  ajusta dicha probabilidad por debajo de $0.003$.
\end{ejemplo}


Una  desigualdad muy  importante que  usaremos frecuentemente  en  el c\'apitulo
siguiente, trata de funciones convexa, y del efecto sobre la media de una vector
aleatorio.
%
\begin{definicion}[Funci\'on convexa]\label{Def:MP:convexa}
  Por definici\'on, una  funci\'on \ $\phi: \X \subset  \Rset^d \mapsto \Rset$ \
  con \ $\X$ un convexo, es convexa si  para cualquier $\pi_1 \in [0 \, , \, 1],
  \pi_2 = 1-\pi_1$ \ y \ $x_1, x_2 \in \Rset^d$,
  %
  \[
  \phi(\pi_1 x_1 + \pi_2 x_2) \le \pi_1 \phi(x_1) + \pi_2 \phi(x_2).
  \]
  %
  $\phi$ es dicha estrictamente convexa  si la desigualdad es estricta, salvo si
  $x_2  =  x_1$.\newline Se  puede  ver  de  que si  \  $\phi$  \ es  dos  veces
  diferenciable, su matriz Hessiana, $\Hess \phi \ge 0$ donde las componentes de
  la Hessiana son las  derivadas partiales secundas de $\phi$, $\frac{\partial^2
    \phi}{\partial x_i  \partial x_j}$.\newline Por  recurrencia, para cualquier
  conjunto \ $\{ x_i \}_i$ numerable de elementos de \ $\X$ \ y reales positivos
  \ $\{ \pi_i \}_i$ \ tales que $\sum_i \pi_i = 1$,
  %
  \[
  \phi\left( \sum_i \pi_i x_i \right) \le \sum_i \pi_i \phi(x_i).
  \]
  %
  Dicho con  palabras, la funcci\'on  del barycentro (combinaci\'on  convexa) de
  los $x_i$  es debajo del  barycentro de los  $\phi(x_i)$. Eso es ilustrado  el la
  figura Fig.~\ref{Fig:MP:Convexa}.
\end{definicion}
%
\begin{figure}[h!]
%
\begin{center} \input{TIKZ_MP/Convexidad} \end{center}
%
\leyenda{Ejemplo  de  funci\'on  $\phi$  convexa:  la  cuerda,  conteniendo  los
  barycentros de  $\{ \phi(x_1), \phi(x_2) \}$,  es siempre arriba  de la curba,
  \ie funci\'on de los barycentros de $\{ x_1 , x_2 \}$.}
%
\label{Fig:MP:Convexa}
\end{figure}
%
Intuitivamente, la  media teniendo un sabor  de barycentro, se intuye  de que la
media  de $\phi(X)$ va  a ser  arriva de  la funci\'on  de la  media de  $X$. Es
precisamente el  teorema de Jensen~\footnote{En~\cite{Jen06} se trata  del en el
  caso discreto  y integral; en~\cite{Hol89,  Had93} se encuentran  las primeras
  semillas de  esta desigualdad,  y entre otros~\cite{Jes31:I,  Jes31:II, Per74,
    Rud91}  para versiones  m\'as generales.\label{Foot:SZ:Jensen}}~\cite{Jen06,
 Fel71, Bre88, AthLah06, Coh13}:
%
\begin{teorema}[Desigualdad de Jensen]\label{Th:MP:Jensen}
  Sea \ $X$ \ integrable y definida  sobre $\X \subset \Rset^d$, convexo y \ $f:
  \X \mapsto \Rset$. Entonces
  %
  \[
  \Esp[\phi(X)] \ge \phi\left( \Esp[X] \right).
  \]
  %
  Si $\phi$ es  estrictamente convexa, la igualdad se alcanza  si y solamente si
  $X$ es determinista casi siempre.
\end{teorema}
%
\begin{proof}
  Sea \ $X = \sum_i x_i  \un_{A_i}$ \ variable escalonada. Entonces \ $\phi(X) =
  \sum_i \phi(x_i) \un_{A_i}$, dando
  %
  \[
  \Esp[\phi(X)]  = \sum_i  P(A_i)  \phi(x_i) \ge  \phi\left(  \sum_i P(A_i)  x_i
  \right) = \phi\left( \Esp[X] \right)
  \]
  %
  con igualdad  (cuando la convexidad es  estricta) si y solamente  si todos los
  $x_i$  son iguales.  Se  cierra la  prueba tomando  $X \ge  0$ como  limite de
  sucesi\'on   de   funciones  escalonadas   (teorema~\ref{Th:MP:MedibleLimite},
  pagina~\pageref{Th:MP:MedibleLimite}),  y cualquier $X$  tratando de  la parte
  positiva  y  negativa  (ver  pagina~\pageref{Th:MP:MedibleLimite}).   El  caso
  vectorial  se   trata  componente  a   componente  para  $X$  en   termino  de
  limite. Tomando  el limite, la  condici\'on $x_i$ todos iguales  vuelve ``casi
  todos'' los $x_i$ deben ser iguales, \ie $X$ debe ser constante casi siempre.
\end{proof}

Terminamos esta secci\'on  con una desigualdad tambi\'en muy  \'util, y conocida
en   los   espacios   de    Hilbert,   conocida   como   {\it   desigualdad   de
  H\"older}~\cite{Hol98}:
%
\begin{teorema}[Desigualdad de H\"older]
  Sean \  $X$ \ e \  $Y$ \ dos vectores  aleatorios $d$-dimensionales y  $r > 1$
  real. $r^* >  1$ tal que $\frac1r + \frac1{r^*}$ es  llamado {\it conjugado de
    H\"older} de $r$, y
  %
  \[
  \left|  \Esp\left[  X^t Y  \right]  \right| \:  \le  \:  \Esp\left[ \left\|  X
    \right\|_r^r   \right]^{\frac1r}  \:  \Esp\left[   \left\|  Y
    \right\|_{r^*}^{r^*} \right]^{\frac1{r^*}}
  \]
  %
  donde \ $\| x \|_r =  \left( \sum_i x_i^r \right)^{\frac1r}$ \ denota la norma
  $r$ de un  vector ($\|\cdot\|_2 \equiv |\cdot|$).  Se obtiene  la igualda si y
  solamente si existe un \ $\lambda$ \ tal que \ $X = \lambda Y$ \ casi siempre.
\end{teorema}
%
\begin{proof}
  Obviamente, $\left| \Esp\left[ X^t Y \right] \right| \le \Esp\left[ \left| X^t
      Y \right| \right]$. \ Luego, de  la convexidad de la funci\'on \ $-\log$ \
  se  obtiene  la  desigualdad  \  $\log(|a  b|) =  \frac1r  \log  |a|^r  +
  \frac1{r^*} \log  |b|^{r^*} \le \log\left( \frac{|a|^r}{r}
    + \frac{|b|^{r^*}}{r^*} \right)$ \  con igualdad si y solamente si
  \ $a es proporcional a b$. \  Aplicado a las componentes de dos vectores \ $a$
  \ y  \ $b$ \ se  obtiene la desigualdad de  Young \ $\left| a^t  b \right| \le
  \frac{\|a\|_r^r}{r}                                            +
  \frac{\|b\|_{r^*}^{r^*}}{r^*}$ \ con igualdad si y solamente si
  los vectores son proporcional. A continuaci\'on, denotando
  %
  \[
  \widetilde{X}         =         \frac{X}{\Esp\left[        \|X\|_r^r
    \right]^{\frac1r}}    \qquad    \mbox{y}    \qquad   \widetilde{Y}    =
  \frac{Y}{\Esp\left[ \|Y\|_{r^*}^{r^*} \right]^{\frac1{r^*}}}
  \]
  %
  tenemos
  %
  \[
  \Esp\left[   \left|   X^t  Y   \right|   \right]   =   \Esp\left[  \left\|   X
    \right\|_r^r   \right]^{\frac1r}  \:  \Esp\left[   \left\|  Y
    \right\|_{r^*}^{r^*}   \right]^{\frac1{r^*}}   \:  \Esp\left[
    \left| \widetilde{X}^t \widetilde{Y} \right| \right].
  \]
  %
  De la desigualdad de Young, se obtiene entonces
  %
  \[
  \Esp\left[   \left|   \widetilde{X}^t   \widetilde{Y}  \right|   \right]   \le
  \frac{\Esp\left[ \left\| \widetilde{X} \right\|_r^r \right]}{r}
  +   \frac{\Esp\left[   \left\|  \widetilde{Y}   \right\|_{r^*}^{r^*}
    \right]}{r^*} = \frac1r + \frac1{r^*} = 1,
  \]
  %
  lo que cierra la prueba.
\end{proof}

Un       corolario       es       conocido       como       desigualdad       de
Cauchy-Bunyakovsky-Schwarz~\footnote{Esta  desigualdad, fue  probada  por Cauchy
  para  sumas   en  1821~\cite{Cau21},   para  integrales  por   Bunyakovsky  en
  1859~\cite{Bou59} y  m\'as elegamente por  Schwarz en 1888~\cite{Sch88}  en un
  enfoque m\'as general. Ver tambi\'en~\cite{Ste04}.} para $p = \frac12$:
%
\begin{corolario}[Desiguladad de Cauchy-Bunyakovsky-Schwarz]
  Sean \  $X$ \ e \  $Y$ \ dos vectores  aleatorios $d$-dimensionales. Entonces
  %
  \[
  \left|  \Esp\left[  X^t Y  \right]  \right|^2 \:  \le  \:  \Esp\left[ \left|  X
    \right|^2   \right]  \:  \Esp\left[   \left|  Y
    \right|^2 \right]
  \]
  %
  (recuersense de que \ $|\cdot| \equiv  \| \cdot \|_2$).  Se obtiene la igualda
  si y  solamente si existe un \  $\lambda$ \ tal que  \ $X = \lambda  Y$ \ casi
  siempre.
\end{corolario}
%
Nota:  se puede probar  esta desigualdad  considerando el  polinomio $\Esp\left[
  \left|  \lambda  X  +  Y  \right|^2  \right] \ge  0$,  del  secundo  orden  en
$\lambda$. Siendo no negativa para cualquier $\lambda$ el discriminente debe ser
no positivo, conduciendo a la desigualdad.

De hecho,  se puede ver $\Esp[X^t  Y]$ como un producto  escalar entre variables
aleatorias. La sola  subtileza es que $\Esp[|X|^2]  = 0$ conduce a $X  = 0$ casi
siempre, \ie se  puede tener $X \ne  0$ pero con medida de  probabilidad igual a
cero (ej. puntos $\omega$ ``aislados'' en el contexto continuo).