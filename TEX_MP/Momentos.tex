\seccion{Esperanza, momentos, identidades y desigualdades}%funciones generadoras}
\label{s:MP:esperanzamomento}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\aver{\emph{introducci\'on...}} %%


% ================================= Media

\subseccion{\modif{Media de un vector aleatorio}}
\label{sec:MP:VectorMedio}


Una  variable aleatoria  $X$  tiene asociado  un  {\it promedio}  o {\it  media}
(tambi\'en  llamado   {\it  valor  esperado  o  de   expectaci\'on  o  esperanza
  matem\'atica})  que  se obtiene  pesando  cada  valor  de \modif{$X$}  con  la
\modif{medida de} probabilidad asociada a ese valor,
%
\modif{
\begin{definicion}[Media o valor/vector medio]
  Formalmente, la media de  una variable aleatoria $X$ \underline{integrable} es
  definida por
  %
  \[
  \Esp[X] = \int_\Omega X(\omega) \, dP(\omega)
  \]
  %
  Por    el    teorema    de    la    medida    imagen~\ref{Th:MP:MedidaImagen},
  pagina~\pageref{Th:MP:MedidaImagen}, esta media  se escribe tambi\'en a partir
  de la medida de probabilidad $P_X$ como
  %
  \[
  \Esp[X] = \int_\Rset x \, dP_X(x)
  \]
  %
  En  el caso vectorial  $d$-dimensional, hay  que entender  la media,  o vector
  medio, como  un vector de componentes  $i$-\'esima la media  $\Esp[X_i]$ de la
  componente $i$-\'esima $X_i$ de $X$, dando
  %
  \[
  \Esp[X] = \int_{\Rset^d} x \, dP_X(x)
  \]
  %
  A veces,  se encuentra  tambi\'en la notaci\'on  \ $\langle  x \rangle$ \  o \
  $\langle  x  \rangle_{p_X}$  \  para  el  valor  medio,  especialmente  en  la
  literatura de f\'isica.
\end{definicion}
%
La secunda formulaci\'on  del valor medio se proba  sencillamente, empezando por
$X = \un_A$ para unos $A$.   Entonces $P_X = (1-P(A)) \delta_0 + P(A) \delta_1$.
Luego  $\displaystyle \int_\Omega  \un_A(\omega)  dP(\omega) =  P(A) =  (1-P(A))
\times 0 +  P(A) \times 1 = \int_\Rset  x dP_X(x)$.  Se cierra la  prueba con el
teorema~\ref{Th:MP:MedibleLimite} dando cualquier  funci\'on medible como limite
de funciones escalonadas,  y por la definici\'on~\ref{Def:MP:IntegracionReal} de
la integral de cualquier funci\'on medible.

Luego,   de    la   distribuci\'on   marginal    $\displaystyle   P_{X_i}(B)   =
\int_{\Rset^{i-1}   \times   B   \times   \Rset^{d-i}}  dP_X(x)$,   se   obtiene
$\displaystyle  \Esp[X_i] = \int_{\Rset^d}  x_i \,  dP_X(x)$, dando  la \'ultima
formulaci\'on en el caso vectorial.
}
%$p(x)\,dx$, e integrando sobre el rango permitido de $x$:
%$$
%E[X] = \langle x\rangle = \int_{\Omega} x \ p(x)\,dx \equiv \mu
%$$
%si  la  integral  existe.  La  \emph{esperanza} de  la  variable  aleatoria  $X$
%representa  el valor  medio  que puede  tomar  entre todos  los  eventos de  una
%prueba. 
Una   variable   aleatoria   $X$   se   dice   integrable   cuando   $E[|X|]   <
\infty$. \modif{De  la misma manera, un  vector aleatorio admite una  media si y
  solamente si cada componente es integrable. Veremos m\'as adelante que existen
  variables aleatorias que no admiten una media.

  M\'as  all\'a de  la  formulaci\'on matem\'atica  de  la media  \ $\Esp[X]$  \
  representa   la  posici\'on  alrededor   de  la   cual  se   ``distribuye  las
  probabilidades de  occurencia''. Es el equivalente  probabil\'istico de centro
  de gravedad o barycentro en mec\'anica.

  En el  caso de variables  aleatorias discretas, de  soporte $\X = \{  x_i \}$,
  inmediatamente
  %
  \[
  \Esp[X] = \sum_i x_i P(X = x_i) = \sum_i x_i p_X(x_i)
  \]
  %
\noindent  Fijense de  que $\Esp[X]$  no partenece  necesariamente a  $\X$:
%
\begin{ejemplo}
  Sea \  $X$ \  uniforme sobre \  $\X = \{  1 \,  , \, 3  \, ,  \, 7 \}$,  \ie \
  $\forall \,  i \in \X, \quad  P(X = i) =  \frac13$. \ Se calcula  $\Esp[X] = 1
  \times \frac13  + 3 \times \frac13  + 7 \times \frac13  = \frac{11}{3} \not\in
  \X$.  Tampoco es el promedio de los valores extremos.
\end{ejemplo}
%
\noindent Cuando \ $|\X| = +\infty$, \ $X$ \ no es necesariamente integrable:
%
\begin{ejemplo}
  Sea \ $\X = \Nset^*$ \ con  $P(X = n) = \frac{6}{\pi^2 \, n^2}$. \ Claramente,
  $\sum_n\frac{6}{\pi^2 \, n}$ diverge, as\'i que $X$ no tiene una media.
\end{ejemplo}

En el caso de vectores  aleatorios continuos, obtenemos la expresi\'on siguiente
de la media (o vector medio):
%
\[
\Esp[X] = \int_{\Rset^d} x \, p_X(x) \, dx
\]
%
\noindent Las mismas observaciones que  hicimos en el caso discreto se encuentra
en el caso continuo:
%
\begin{ejemplo}
  Sea \ $X$  \ de densidad de probabilidad  \ $p_X(x) = \frac12 \un_{[0  \, ; \,
    1)}(x) +  \frac{3 \sqrt{x-2}}{4}  \un_{[2 \, ;  \, 3)}(x)$ \  como ilustrado
  figura                                         Fig.~\ref{fig:MP:ProbaContinua},
  pagina~\pageref{fig:MP:ProbaContinua}. Se  calcula \ $\Esp[X]  = \frac{31}{20}
  \not\in \X = [0 \, ; \, 1] \cup [2 \, ; \, 3]$.
\end{ejemplo}
%
\begin{ejemplo}
  Un ejemplo  de vector aleatorio no  teniendo media es  dado en el caso  de una
  distribuci\'on de Cauchy-Lorentz (ver  m\'as adelante) \ $\displaystyle p_X(x)
  = \frac{\alpha}{\left( 1 + x^t x \right)^{\frac{d+1}{2}}}$ \ donde $\alpha$ es
  un factor de normalizaci\'on.
\end{ejemplo}

En el caso general, para calcular  la media, hay que pasar por la distribuci\'on
$P_X$, como en el ejemplo~\ref{Ej:MP:Mixta}  pagina~\pageref{Ej:MP:Mixta}:
%
\begin{ejemplo}[Continuaci\'on del ejemplo~\ref{Ej:MP:Mixta}]\label{Ej:MP:EspMixta}
  Sea $X = V \,  \un_{U < \frac12} + \un_{U \ge \frac12}$ \ con  \ $U$ \ y \ $V$
  variables aleatorias independientes de distribuci\'on uniformas sobre \ $[0 \,
  ;  \, 1)$,  \ie  $p_U(x) =  \un_{[0  \, ;  \,  1)}(x)$. \  De \  $X  \in B  \:
  \Leftrightarrow  \: \left(  \left( U  < \frac12  \right) \cap  \left( V  \in B
    \right) \right) \, \cup \, \left( \left( U \ge \frac12 \right) \cap \left( 1
      \in B \right) \right)$,  \ del hecho de que los eventos  de la uni\'on son
  incompatibles y de  la independencia de $U$ y $V$ (o  saliendo de la funci\'on
  de repartici\'on), se obtiene $P_X(B) = \frac12 P_V(B) + \frac12 \delta_1(x)$.
  A  continuaci\'on, \  $\displaystyle \Esp[X]  = \frac12  \int_\Rset  dP_V(x) +
  \frac12 \int_\Rset  d\delta_1(x) = \frac12  \int_\Rset p_V(x) \, dx  + \frac12
  \times 1 = \frac12 \int_0^1 dx + \frac12 = \frac34$.
\end{ejemplo}

\

Una nota intersante es  de que, en el caso escalar, si \  $X \ge 0$ \ admitiendo
una media, se obtiene
%
\[
\Esp[X]  = \int_{\Rset_+} P(X  > t)  \, dt  = \int_{\Rset_+}  \left( 1  - F_X(t)
\right) \, dt
\]
%
Se proba saliendo  de \ $\displaystyle x = \int_0^x  dt = \int_{\Rset_+} \un_{(t
  \, ;  \, +\infty)}(x)  \, dt$  \ dando \  $\displaystyle \Esp[X]  = \int_\Rset
\left( \int_{\Rset_+}  \un_{(t \, ; \,  +\infty)}(x) \, dt \right)  \, dP_X(x) =
\int_{\Rset_+} \left( \int_\Rset \un_{(t \, ; \, +\infty)}(x) \, dP_X(x) \right)
\,    dt$    \    por    el    teorema    de    Fubini    Th.~\ref{Th:MP:Fubini}
pagina~\pageref{Th:MP:Fubini}. Se  cierra la  prueba observando que  la integral
interior es nada  m\'as que $P(X > t)$.   En el caso discreto con  $\X = \Nset$,
viene  inmediatamente \ $\displaystyle  \sum_{n \in  \Nset} P(X>n)$  que podemos
probar directamente  saliendo de $P(X  = n) =  P(X>n)- P(X>n-1)$. En el  caso de
variable   admitiendo  una   densidad,   se  obtiene   tambi\'en  haciendo   una
integraci\'on  por   partes~\footnote{El  el   casi  discreto,  hay   que  tener
  precauciones separando la series de una diferencia de terminos. En el caso $X$
  continuo admitiendo una densidad, hay  que estudiar bien el coomportamiento de
  $t \mapsto t (1-F_X(t))$ al infinito.}

Esta formula se aplica al ejemplo~\ref{Ej:MP:Mixta} que tratamos:
%
\begin{ejemplo}[Continuaci\'on del ejemplo~\ref{Ej:MP:Mixta}]\label{Ej:MP:EspMixtaPositiva}
  Sea $X = V \,  \un_{U < \frac12} + \un_{U \ge \frac12}$ \ con  \ $U$ \ y \ $V$
  variables aleatorias independientes de distribuci\'on uniformas sobre \ $[0 \,
  ;  \,  1)$. Obtuvimos  pagina~\pageref{Ej:MP:Mixta}  \  $F_X(x) =  \frac{x}{2}
  \un_{[0 \,  ; \, 1)}(x)  + \un_{[1 \,  ; \, +\infty)}(x)$.   A continuaci\'on,
  reobtenemos \ $\displaystyle \Esp[X] = \int_0^1 \left( 1 - \frac{x}{2} \right)
  \, dx = \frac34$.
\end{ejemplo}.

\

Terminamos  esta  secci\'on con  la  propiedad  de  linealidad de  la  esperanza
matem\'atica $\Esp$,  como consecuencia de  la linealidad de la  integraci\'on y
definici\'on de la distribuci\'on  marginal: para cualquier conjunto de vectores
aleatorios \ $\{  X_i \}$ \ integrables  y cualquieras matrices \ $\{  C_i \}$ \
dadas  de  dimensiones compatibles  con  las  de \  $X$  \  (incluyendo el  caso
escalar),
%
\[
\Esp\left[ \sum_i C_i X_i \right] = \sum_i C_i \Esp\left[ X_i \right]
\]
%
(la integrabilidad de la suma se proba a partir de la desigualdad triangular).
}


% ================================= Momentos
\modif{
\subseccion{Momentos de un vector aleatorio}
\label{sec:MP:Momentos}
}

Si $X$ es una variable aleatoria, para cualquier funci\'on medible $f$, \ $f(X)$
tambi\'en  lo  es.  \modif{Se  puede   entonces  definir  su  valor  medio},  si
existe. \modif{A pesar de necesitar evaluar la distribuci\'on de probabilidad de
  $Y = f(X)$, el valor medio se calcula a partir del de $X$:
%
\begin{teorema}[Teorema de transferencia]
  Sea  \ $X$  \ un  vector  aleatorio $d$-dimensional  y \  $f: \Rset^d  \mapsto
  \Rset^{d'}$ \ una funci\'on medible tal que $f(X)$ sea integrable. Entonces
  %
  \[
  \Esp\left[   f(X)  \right]   =  \int_\Omega   f(X(\omega))  \,   dP(\omega)  =
  \int_{\Rset^{d'}} f(x) \, dP_X(x)
  \]
  %
  En particular, en el caso $\X = X(\Omega)$ discreto se obtiene
  %
  \[
  \Esp\left[ f(X) \right] = \sum_i f(x_i) P(X = x_i)
  \]
  %
  y para $X$ continuo admitiendo una densidad de probabilidad
  %
  \[
  \Esp\left[ f(X) \right] = \int_{\Rset^d} f(x) \, p_X(x) \, dx
  \]
\end{teorema}
%
\begin{proof}
  Sea $B \in \B(\Rset^d)$ \ y consideramos \ $f(x) = \un_B(x)$. Entonces, $\Y
  = \{  0 \, , \,  1 \}$ y inmediatamente  $$P_Y = P_X(B) \, \delta_1 + (1-P_X(B))
  \, \delta_0$$ Entonces
  %
  \[
  \Esp[f(X)]  =  \int_\Rset  P_X(B)  \,  d\delta_1 +  \int_\Rset  (1-P_X(B))  \,
  d\delta_0 = P_X(B) = \int_{\Rset^d} \un_B(x) \, dP_X(x)
  \]
  %
  En el caso  $d' = 1$, para $f  \ge 0$, se cierra entonces la  prueba usando el
  teorema~\ref{Th:MP:MedibleLimite}         pagina~\pageref{Th:MP:MedibleLimite},
  escribiendo \  $f$ \  como l\'imite creciente  de una sucesi\'on  de funciones
  escalonadas,  y   la  definici\'on  Def.~\ref{Def:MP:IntegracionReal}   de  la
  integraci\'on real. El  caso $d' > 1$ es  nada mas que $d' =  1$, componente a
  componente.
\end{proof}

De  manera general,  estas medias  son llamadas  {\it momentos}  de  la variable
aleatoria $X$. Los momentos relevantes usuales
%\underline{en el caso escalar} ($d = 1$) 
 son los siguientes:}
%
\begin{itemize}
\item para el  \modif{``monomio'' $f(x) = x^{\otimes r}$  producto tensiorial de
    $x$ \  $r$ veces~\footnote{Recuerdense  de que $x  \otimes x$ es  una matriz
      teniendo como componentes $x_i x_j$; entonces $x^{\otimes r}$ en un tensor
      $r$-dimensionale   teniendo  como   componentes  $   \displaystyle  \left[
        x^{\otimes r} \right]_{i_1,\ldots,i_r} = \prod_j x_{i_j}$.}}  \ siendo \
  $r \in  \Nset^*$, se obtiene  \modif{el tensor de los}  $r${\it-\'esimo momentos
    (ordinarios)} de $X$:
  %
  \[
  \nu_r  \equiv   \Esp\left[  X^{\otimes  r}   \right]  =  \modif{\int_{\Rset^d}
    x^{\otimes r} \ dP_X(x)}
  \]
  %
  que tiene \modif{unidades de $\prod_j  X_{i_j}$ ($X_i^r$ si los componentes de
    $X$ tienen la misma ``unidad'').  Se escribe tambi\'en
  %
  \[
  \nu_{r_1,\ldots,r_d}  =  \Esp\left[   \prod_{i=1}^d  X_i^{r_i}  \right]  \quad
  \mbox{con} \quad \sum_i r_i = r.
  \]
  %
} Se  puede incluir el  caso $r=0$ \modif{con  la convenci\'on $x^{\otimes  0} =
  1$},   que    corresponde   a    la   condici\'on   de    normalizaci\'on:   \
\modif{$\displaystyle \nu_0 =  \int_\Rset dP_X(x) = 1$}.  La  media es el primer
momento: $\nu_1  = \Esp[X] =  \mu_X$.  T\'ipicamente, los primeros  momentos son
m\'as relevantes que los de  \'ordenes mayores, para la caracterizaci\'on de una
distribuci\'on. \modif{Para $r  = 2$, en el caso escalar, el  momento de orden 2
  es el an\'alogo del momento de inercia de la mec\'anica.}\newline Por ejemplo,
para  la  distribuci\'on  uniforme  $p_X(x)  = \frac{1}{b-a}$  en  el  intervalo
$[a,b]$,  resulta  $\nu_r=\frac{b^{r+1}-a^{r+1}}{(r+1)(b-a)}$.   En  particular,
$\nu_1 =  \frac{a+b}{2}$, valor  medio del intervalo.\newline  \modif{Fijense de
  que $X^{\otimes  r}$ no  es siempre  integrable, por ejemplo,  el el  caso con
  densidad,  si}  $p_X(x)$   tiene  soporte  (semi)infinito,  necesariamente  la
funci\'on $p_X$ debe tender a 0 cuando $\|x\|\rightarrow\infty$.  Si $p_X(x)$ es
{\it de largo alcance}, en el sentido de que no cae a 0 suficientemente r\'apido
con $x$ para  $x$ grandes, algunos momentos pueden no  existir.  Por ejemplo, la
distribuci\'on   de   probabilidad    de   Cauchy--Lorentz   (o   funci\'on   de
Breit--Wigner),  dada por  \modif{$p_X(x) =  \frac{\alpha}{\left( 1  + (x-x_0)^t
      R^{-1}  (x-x_0) \right)^{\frac{d+1}{2}}}$ sobre  $\Rset^d$, con  la matriz
  cuadrada \  $R > 0, \: x_0  \in \Rset^d$ \ y  \ $\alpha > 0$  \ coeficiente de
  normalizaci\'on}, no tiene momentos finitos de orden $r \geq 1$.
%
\item En el  caso de variables discretas $X$ sobre $\X  = \Nset$, resulta \'util
  introducir el $r$-\'esimo {\it momento factorial} de $X$ ($r \in \Nset$) mediante
  %
  \[
  \Esp\left[ X^{(r)} \right] \equiv  \modif{\Esp\left[ \prod_{k=0}^{r-1} (X-k) \right]
  = \sum_{n = r}^\infty \frac{n!}{(n-r)!} \, P(X = n)}.
 \]
 %
 \modif{(usamos la convenci\'on usual \ $\prod_0^{-1} = 1$).}
%
\item  Los  {\it  momentos  centrales}  \modif{o {\it  cumulantes}}  se  definen
  alrededor de  \modif{la media  $\Esp[X]$, \ie,} como  el \modif{tensor  de los
    $r$-\'esimo momentos de la {\it desviaci\'on} \ $\Delta X \equiv X-\Esp[X]$}:
  %
  \[
  \mu_r \equiv \Esp\left[  \left( X - \Esp[X] \right)^{\otimes r} \right]
  \]
  %
  Se escribe tambi\'en
  %
  \[
  \mu_{r_1,\ldots,r_d}   =   \Esp\left[   \prod_{i=1}^d   \left(   X_i-\Esp[X_i]
    \right)^{r_i} \right] \quad \mbox{con} \quad \sum_i r_i = r
  \]
  %
  Se  deduce que  si la  \modif{distribuci\'on de  probabilidad satisface  a una
    simetr\'ia central}  con respecto a  la media, \modif{\ie \  $X-\mu_X \egald
    -(X-\mu_X)$ \ donde \ $\egald$ \ significa que los vectores aleatorios tiene
    la  misma  distribuci\'on  de  probabilidad}, entonces  todos  los  momentos
  centrales  impares son nulos.   Los momentos  (centrales) brindan  medidas que
  caracterizan la distribuci\'on.
  %
  \begin{enumerate}
  \item \modif{El primer momento, o media:
   \[
    \mu_X = \Esp[X].
   \]}
  %
\item  El  segundo  momento  central   se  conoce  como  \modif{{\it  matriz  de
      covarianza}.  En el  caso escalar,  hablamos  de {\it  varianza}, o}  {\it
    dispersi\'on} o tambi\'en {\it desviaci\'on cuadr\'atica media}.
  %
  \[
  \modif{\Sigma_X  \equiv \Cov[X]  \equiv \mu_2  = \Esp\left[  \left( X  - \mu_X
      \right) \left( X - \mu_X \right)^t \right]}
  \]
  %
  \modif{En el caso escalar, la varianza se escribe en general
  %
  \[
  \Var[X] = \Esp\left[ \left( X  - \mu_X \right)^2 \right]
  \]}  y es  una  medida del  cuadrado del  ancho  efectivo de  una densidad  de
probabilidad (o vector  de probabilidad). \modif{Para dos componentes  $i \ne j$
  hablamos de {\it covarianza entre variables}, y escribimos
  %
  \[
  \Cov[X_i,X_j]  =  \Esp\left[ \left(  X_i  -  \mu_{X_i}  \right) \left(  X_j  -
      \mu_{X_j} \right) \right]
  \]
  %
  La matriz de covarianza tiene las varianzas de los $X_i$ en su diagonal, y las
  covarianzas entre  componentes en las componentes no  diagonales.  Es sencillo
  ver de que \ $\Cov[X]$ es sim\'etrica, por construcci\'on, y de que \ $\Cov[X]
  \ge 0$ \ donde $A \ge 0$  significa que la matriz es, definida no negativa (en
  el  caso escalar  la varianza  es no  negativa), con  igualdad}  s\'olo cuando
\modif{$P_X  =  \delta_{x_0}$  para un  $x_0$  dado},  esto  es, cuando  no  hay
incerteza    sobre    el     resultado.     \modif{De    la    desigualdad    de
  Cauchy-Bunyakovsky-Schwarz           (ver          corolario~\ref{Cor:MP:CBS},
  pagina~\pageref{Cor:MP:CBS})}   se  proba   sencillamente   de  que   $$\left|
  \Cov[X_i,X_j] \right|^2  \le \sigma_{X_i}^2 \sigma_{X_j}^2$$  \modif{as\'i que
  s}e  define \modif{tambi\'en}  el {\it  coeficiente de  correlaci\'on}  que es
adimensional    y   toma   valores    entre   $-1$    (variables   completamente
anticorrelacionadas) y 1 (variables completamente correlacionadas) como:
  %
  \[
  \rho_{ij} = \rho_{ji} \equiv \frac{\Cov[X_i,X_j]}{\sigma_{X_i} \sigma_{X_j}}
  \]
  %
  Como ejemplo, dadas $X_1$ y $X_2 = a X_1 + b$ que fluct\'uan en fase ($a>0$) o
  al rev\'es ($a<0$),  se tiene $\Delta X_2 = a \Delta  X_1$, luego $\rho_{12} =
  \frac{a}{|a|}   =  \pm   1$.\newline   \modif{Tambi\'en,  se   puede  ver   de
    que $$\Var[\| X \|] = \Tr  \Sigma_X$$ $\Tr$ siendo la traza y $\|\cdot\|$ la
    norma euclideana de un  vector.}  La \modif{co}varianza est\'a bien definida
  si \modif{$\|X\|$} es una variable  aleatoria de cuadrado integrable, esto es,
  cuando   \modif{$E[\|X\|^2]  <   \infty$}.    \modif{Se  proba   sencillamente
    (desallorando el ``cuadrado'' y usando la linealidad de la esperanza) de que
  %
    \[
    \Cov[X] = \Esp\left[ X X^t \right] - \mu_X \mu_X^t
    \]
  %
    conocido como {\it  teorema de K\"onig-Huygens}.  En el  caso escalar, es el
    equivalente del teorema de Huygens  de la mec\'anica relacionando el momento
    de inertia  de un solido con respeto  al origen en funci\'on  del momento de
    inertia con respeto al centro de masa. Adem\'as, inmediatamente,
  %
    \[
    \forall \: A \in \Rset^{d' \times d},  \: b \in \Rset^d, \quad \Cov[A X + b]
    = A \Cov[X] A^t
    \]
  %
    En el  caso escalar, $d  = 1$,  lo que es  conocido tambi\'en como}  el {\it
    ancho}  de   una  distribuci\'on  est\'a  dado  por   la  {\it  desviaci\'on
    est\'andar}
  %
  \[
  \sigma_X = \sqrt{\Var[X]}
  \]
  %
  tiene  las mismas  unidades de  $X$,  y se  usa para  normalizar los  momentos
  centrales  de orden  superior.  El  {\it ancho  relativo} es  otra  medida que
  caracteriza   la   distribuci\'on,    dado   por   $\frac{\sigma_X}{\mu_X}   =
  \sqrt{\frac{\Esp\left[    X^2   \right]}{\mu_X^2}-1}$   cuando    $\mu_X   \ne
  0$. \newline Dad\modif{o un vector  aleatorio} $X$, teniendo en cuenta que los
  dos  primeros  momentos dan  las  caracter\'isticas  m\'as  importantes de  la
  \modif{distrubuci\'on de  probabilidad}, puede resultar  conveniente hacer una
  transformaci\'on  de  variable  aleatoria  a la  llamada  \modif{\it  variable
    est\'andar}: \modif{$Y \equiv \Sigma_X^{-\frac12} \left( X - \mu_X \right)$},
  \modif{donde  $\Sigma^{-\frac12}$ es  la \'unica  matriz  sim\'etrica definida
    positiva  tal  que  su  cuadrado es  igual  a  $\Sigma^{-1}$~\cite{HorJoh85,
      MagNeu99}}  que entonces  tiene media  igual a  0 y  \modif{una  matriz de
    covarianza  igual  al  identidad  $I$  (en el  caso  escalar,}  desviaci\'on
  est\'andar igual a 1\modif{)}.
  %
\item \modif{en el  caso escalar,} el tercer momento  central permite definir el
  {\it coeficiente de asimetr\'ia} \modif{(o skewness en ingles)}:
  \[
  \modif{\gamma_X} \modif{\equiv  \Esp\left[ \left( \frac{X - \mu_X}{\sigma_X}
      \right)^3 \right]} = \frac{\mu_3}{\sigma_X^3},
  \]
  %
  \modif{momento de orden 3 de la variable estandar,} que resulta adimensional y
  puede tener  signo positivo o  negativo, anul\'andose para  distribuciones que
  son sim\'etricas respecto del valor medio;
  %
\item  \modif{en el  caso escalar,}  el  cuarto momento  central da  lugar a  la
  \emph{curtosis}:
  \[
  \modif{\Curt[X]  \equiv \kappa_X}  \equiv \modif{\Esp\left[  \left(  \frac{X -
          \mu_X}{\sigma_X} \right)^4 \right]} = \frac{\mu_4}{\sigma_X^4},
  \]
  % 
  \modif{momento de orden 4 de la variable estandar,} que posibilita diferenciar
  entre distribuciones  altas y angostas.  \modif{Veremos m\'as  adelante de que
    para la densidad Gausiana $p_X(x) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(
      - \frac{(x-\mu)^2}{2 \, \sigma^2} \right)$,  \ $\mu_X = \mu, \: \sigma_X =
    \sigma, \: \gamma_X  = 0, \: \kappa_X =  3$. Se dice de que $p_X$  es alta y
    angosta,  o   sub-gausiana,  o  con   colas  livianas  cuando   o  tambi\'en
    platic\'urtica,}  $\kappa_X  <  3$,  y  de otras  bajas  y  anchas  \modif{o
    sobre-gausiana,  o con  colas  pesadas cuando  o tambi\'en  leptoc\'urtica,}
  $\kappa_X > 3$ \modif{(para $\kappa_X =  3$ la distribuci\'on es a veces dicha
    mesoc\'urtica)}.  A veces,  se define entonces la {\it  curtosis por exceso}
  $\Curt[X] - 3$.
  \end{enumerate}
  %
  \modif{Fijense  de que,  en  el contexto  escalar  $d =  1$,  se vinculan  los
    cumulantes y} los momentos ordinarios directamente de las definiciones:
  %
  \[
  \modif{\mu_r = \sum_{s=0}^r \binom{r}{s} \left( - \mu_X \right)^{r-s} \nu_s}
  \]
  %
  para cualquier \modif{$r \in \Nset$},  siendo $\modif{\mu_0 = } \nu_0=1$.  Por
  ejemplo, \ $\mu_2=\nu_2-\nu_1^2$ \ \modif{que  es nada m\'as que la relaci\'on
    de K\"onig-Huyggens}, mientras que \ $\mu_3=\nu_3-3\nu_1\nu_2+2\nu_1^3$.
\end{itemize}

\modif{Tratando de covarianza, m\'as  generalmente, para dos vectores aleatorios
  \ $X$ \ e \ $Y$, se define la matriz de covarianza conjunta como
%
\[
\Sigma{X,Y} \equiv  \Cov[X,Y] =  \Esp\left[ \left( X  - \mu_X\right) \left(  Y -
    \mu_Y \right)^t \right] = \Esp\left[ X Y^t \right] - \mu_X \mu_Y^t
\]
%
Esta matriz contiene las covarianzas $\Cov[X_i,Y_j]$.
}

% ================================= Momentos
\modif{
\subseccion{Independencia, identidades y desigualdades}
\label{sec:MP:MomentosDesigualdades}

Una primera relaci\'on interesante concierna el caso de variables independientes
y como se comporta la covarianza de estas:}
%
\begin{propuesta}
  Sean  \  $X$  \ e  \  $Y$  \  dos  vectores  aleatorios integrables.   Si  son
  independientes, entonces
  %
  \[
  \Esp[X Y^t] = \Esp[X] \Esp[Y]^t \qquad \mbox{\ie} \qquad \Cov[X,Y] = 0
  \]
  %
  En  particular, para  $X$  con componentes  independientes,  $\Cov[X]$ es  una
  matriz diagonal.
\end{propuesta}
\modif{
\begin{proof}
  Sean \ $X = \sum_j \alpha_j \un_{A_j}$  \ e \ $Y = \sum_k \beta_k \un_{B_k}$ \
  dos variables escalonadas. Entonces, \ $A_j =  (X = \alpha_j)$ \ y \ $B_k = (Y
  = \beta_k)$. Luego
  %
  \begin{eqnarray*}
  \Esp[X Y] & = & \sum_{j,k} \alpha_j \beta_k \Esp[\un_{A_j} \un_{B_k}]\\[2.5mm]
  %
  & = & \sum_{j,k} \alpha_j \beta_k \Esp[\un_{A_j \cap B_k}]\\[2.5mm]
  %
  & = & \sum_{j,k} \alpha_j \beta_k P(A_j \cap B_k)\\[2.5mm]
  %
  & = & \sum_{j,k} \alpha_j \beta_k P(X = \alpha_j) P(Y = \beta_k) \quad \mbox{(de la independencia)}
  \end{eqnarray*}
  %
  dando  el resultado  para  variables  escalonadas. Se  cierra  la prueba  para
  variables positivas  como l\'imite de  crecientes de funciones  escalonadas, y
  variables  reals tratando  las partes  positivas y  negativas aparte.  El caso
  vectorial se deduce trabajando con pares de componentes.
\end{proof}
%
Fijense  de que  la  reciproca es  falsa en  general.   Por ejemplo,  para $X  =
(X_1,X_2)$  uniforme sobre  el  disco unitario,  i.e.,  $p_X(x) =  \frac{1}{\pi}
\un_{\Sset^2}(x)$ \ con \ $\Sset^2 =  \{ (x_1,x_2) \in \Rset^2: \: x_1^2 + x_2^2
\le 1 \}$. Claramente, los \ $X_i$ no pueden ser independientes del hecho de que
\ $\X_i =  [-1 \, , \, 1]$ \  y \ $\X \ne \X_1 \times  \X_2$ \ (es estrictamente
incluido  en  el producto  cartesiano).  Por  simetr\'ia  central de  $p_X$,  es
sencillo ver de  que $\Esp[X_1 X_2] = 0$  \ y similarmente \ $\Esp[X_i]  = 0$: a
pesar de que los \ $X_i$ \ no sean independientes, $\Cov[X_1,X_2] = 0$.

Esta implicaci\'on facilita frecuentemente  los calculos de media.  Volviendo al
ejemplo~\ref{Ej:MP:Mixta}  de la  pagina~\pageref{Ej:MP:Mixta}:
%
\begin{ejemplo}[Continuaci\'on del ejemplo~\ref{Ej:MP:Mixta}]\label{Ej:EspMixtaInd}
  Tratando de la media de \ $X = V \, \un_{U < \frac12} + \un_{U \ge \frac12}$ \
  con \  $U$ \ y  \ $V$ \  variables independientes de  distribuci\'on uniformas
  sobre $(0 \, ; \, 1)$, se calcula gracia a la linealidad y a la independencia,
  \ $\Esp[X]  = \Esp[V]  \Esp[\un_{U < \frac12}]  + \Esp[\un_{U \ge  \frac12}] =
  \frac12 \times  \frac12 + \frac12 =  \frac34$ \ como lo  hemos obtenido usando
  $P_X$   en  la   pagina~\pageref{Ej:MP:EspMixta}  o   la  positividad   en  la
  pagina~\pageref{Ej:MP:EspMixtaPositiva}.
\end{ejemplo}

Una otra  consecuencia de  esta proposici\'on trata  de un conjunto  de vectores
aleatorios \ $\{ X_i \}$ \ y un conjunto de matrices de dimensiones adecuadas,
%
\[
\Cov\left[ \sum_i A_i X_i + B\right] =  \sum_i A_i \Sigma_{X_i} A_i^t + \sum_{j \ne
  i} A_i \Cov[X_i,X_j] A_j^t
\]
%
En particular, en el caso escalar,
%
\[
\Cov\left[ \sum_i A_i X_i + B \right]  = \sum_i A_i^2 \Var[X_i] + \sum_{j \ne i}
A_i A_j \Cov[X_i,X_j]
\]
%
\underline{Si}   los   $X_i$   son  independientes,   \underline{entonces}   las
covarianzas conjuntas son nulas as\'i que, respectivamente,
\[
\Cov\left[ \sum_i  A_i X_i +  B \right] =  \sum_i A_i \Sigma_{X_i}  A_i^t \qquad
\mbox{y}  \qquad  \Cov\left[  \sum_i  A_i   X_i  +  B  \right]  =  \sum_i  A_i^2
\sigma_{X_i}^2
\]

Si el  teorema da  una implocaci\'on  de la independencia,  de hecho  existe una
reciproca que toma la forma siguiente: }
%
\begin{teorema}
  Sean \ $X$ \ e \ $Y$ \ dos vectores aleatorios. Son independientes si y s\'olo
  si $E[f(X) g(Y)]=E[f(X)] E[g(Y)]$ para todo par  de funciones \ $f$ \ y \ $g$,
  medibles y acotadas \modif{de dimensiones adecuadas}.
\end{teorema}
%
\modif{
\begin{proof}
  Se puede referirse a~\cite{Fel71, JacPro03} para unas pruebas rigurosa.  En el
  caso escalar, el  principio consiste a ver \  $f$ \ y \ $g$ \  como limites de
  funciones  escalonadas. Para \  $f(x) =  \sum_i \alpha_i  \un_{A_i}(x)$ \  y \
  $g(y) = \sum_j \beta_j \un_{B_j}(y)$ se obtiene $E[f(X) g(Y)]=E[f(X)] E[g(Y)]$
  si y s\'olo si $\sum_{i,j} \alpha_i \beta_j  \left( P( (X \in A_i) \cap (Y \in
    B_j)) -  P(X \in  A_i) P(Y \in  B_j) \right)  = 0$. B\'asicamente,  eso debe
  valer  para  cualquieras  $A_i,  B_j$  y $\alpha_i,  \beta_j$,  as\'i  que  el
  t\'ermino  entre  parentesis  debe ser  cero,  lo  que  es  nada m\'as  de  la
  definici\'on de  la independencia  de \ $X$  \ e\  $Y$.  El caso  vectorial se
  entiende por pares de componentes.
\end{proof}
}

\

\modif{Relaciones tamb\'ien  muy \'utiles son conocidas  como {\it Desigualdades
    de Chebyshev}~\cite{Bie53, Tch67,  Mar84, OlkPra58, Fer82, Nav13, StePar17}.}
  Estas  desigualdades  dan una  cota  superior a  la  probabilidad  de que  una
  cantidad que  fluct\'ua aleatoriamente exceda  cierto valor umbral,  a\'un sin
  conocer detalladamente la forma de la distribuci\'on de probabilidad.  \modif{
%
\begin{teorema}[Desigualdades de Chebyshev]
  Sea un vector aleatorio $d$-dimensional \  $X$ \ y una funci\'on \ $g: \Rset^d
  \mapsto \Rset_+$ \ medible tal que \ $g(X)$ \ sea integrable. Entonces,
  %
  \[
  \forall \: a > 0, \quad P(g(X) \ge a) \: \le \: \frac{\Esp[g(X)]}{a}
  \]
  %
\end{teorema}
%
\begin{proof}
  Sea \ $\D_a = \{ x \in \X: \: g(x) \ge a \} \subset \X$. Entonces, $g$ siendo non negativa,
  %
  \[
  \Esp[g(X)]  = \int_\X  g(x) \,  dP_X(x) \ge  \int_{\D_a} g(x)  \,  dP_X(x) \ge
  \int_{\D_a} a \, dP_X(x) = a P(X \in \D_a)
  \]
  %
  Se cierra la prueba notando de que \ $(X \in \D_a) \, = \, (g(X) \ge a)$.
\end{proof}
%
Existen varias  formas similares, que son  de hecho casos  particulares de estas
desigualdades.
%
\begin{corolario}[Bienaym\'e--Chebyshev]
  Sea \  $X$ \  un vector aleatorio  $d$-dimensional admitiendo una  esperanza \
  $\mu_X$ \ y una covarianza \ $\Sigma_X$. Entonces,
  %
  \[
  \forall \:  \varepsilon > 0,  \qquad P\left( \left\| \Sigma_X^{-\frac12}  (X -
      \mu_X) \right\| \right) > \varepsilon \: \le \: \frac{d}{\varepsilon^2} .
 \]
\end{corolario}
%
Viene del teorema incial aplicado a \ $\Sigma_X^{-\frac12} (X - \mu_X)$, \ $g(x)
= \|x\|^2$ \ y \ $a = \varepsilon^2$.
% , notando de que \  $\left( \left\| \Sigma_X^{-\frac12} (X - \mu_X) \right\|^2
%   \ge \varepsilon^2 \right) \, = \, (\|X\| \ge \varepsilon)$.
%
%
\begin{corolario}[Markov]
  Sea \ $X$ \ un vector aleatorio y $\varphi \ge 0$ una funci\'on no decreciente
  tal que $\varphi(\|X\|)$ sea integrable. Entonces,
  %
  \[
  \forall \: \varepsilon \ge  0, \quad \mbox{tal que} \quad \varphi(\varepsilon)
  \ne     0,     \qquad     P(\|X\|     >     \varepsilon)     \:     \le     \:
  \frac{\Esp[\varphi(\|X\|)]}{\varphi(\varepsilon)} .
 \]
\end{corolario}
%
La versi\'on  inicial des  esta desigualdad trataba  de funciones  $\varphi(u) =
u^r, \: r > 0$. Viene del teorema incial aplicado a \ \ $g(x) = \varphi(\|x\|)$
\  y  \  $a =  \varphi(\varepsilon)$,  notando  de  que \  $(\varphi(\|X\|)  \ge
\varphi(\varepsilon)) \, = \, (\|X\|  \ge \varepsilon)$ por la no decrecencia de
$\varphi$. El caso anterior (una vez  la variable centrada) es nada m\'as que un
caso especial.}

Estas  relaciones  afirman que  cuanto  m\'as chica  es  la  varianza, m\'as  se
concentra la variable en torno a su media. Ambas cotas son en general d\'ebiles,
\modif{como se lo puede ver en el ejemplo siguiente}
%
\begin{ejemplo}
  La  desigualdad  de  Bienaym\'e--Chebyshev   indica  que  la  probabilidad  de
  encontrar  una  fluctuaci\'on superior  a  $\varepsilon  =  3 \sigma_X$,  tres
  desviaciones est\'andar alrededor  de la media, est\'a por  debajo de \ $1/9$;
  el  c\'alculo   para  una  distribuci\'on   t\'ipica  como  la   Gaussiana,  \
  \modif{$p_X(x)    =    \frac{1}{\sqrt{2    \pi}   \sigma_X}    \exp\left(    -
      \frac{x-\mu_X)^2}{2} \right)$}  \ ajusta dicha probabilidad  por debajo de
  $0.003$.
\end{ejemplo}


\modif{Una  desigualdad  muy  importante   que  usaremos  frecuentemente  en  el
  c\'apitulo siguiente, trata de funciones  convexa, y del efecto sobre la media
  de una vector aleatorio.
%
\begin{definicion}[Funci\'on convexa]
  Por definici\'on, una funci\'on \ $\phi:  \X \subset \Rset^d \mapsto \Rset$ \ con
  \ $\X$ un convexo, es convexa si para cualquier $\alpha \in [0 \, , \, 1]$ \ y
  \ $x_1, x_2 \in \Rset^d$,
  %
  \[
  \phi(\alpha x_1 + (1-\alpha) x_2) \le \alpha \phi(x_1) + (1-\alpha) \phi(x_2)
  \]
  %
  $\phi$ es dicha estrictamente convexa  si la desigualdad es estricta, salvo si
  $x_2  =  x_1$.\newline Se  puede  ver  de  que si  \  $\phi$  \ es  dos  veces
  diferenciable, su matriz Hessiana, $\Hess \phi \ge 0$ donde las componentes de
  la Hessiana son las  derivadas partiales secundas de $\phi$, $\frac{\partial^2
    \phi}{\partial x_i  \partial x_j}$.\newline Por  recurrencia, para cualquier
  conjunto \ $\{ x_i \}_i$ numerable de elementos de \ $\X$ \ y reales positivos
  \ $\{ \alpha_i \}_i$ \ tales que $\sum_i \alpha_i = 1$,
  %
  \[
  \phi\left( \sum_i \alpha_i x_i \right) \le \sum_i \alpha_i \phi(x_i)
  \]
  %
  Dicho con  palabras, la funcci\'on  del barycentro (combinaci\'on  convexa) de
  los $x_i$  es debajo del  barycentro de los  $\phi(x_i)$. Eso es ilustrado  el la
  figura Fig.~\ref{fig:MP:Convexa}.
\end{definicion}
%
\begin{figure}[h!]
%
\begin{center} \input{TIKZ_MP/Convexidad} \end{center}
%
\leyenda{\modif{Ejemplo  de   funci\'on  $\phi$   convexa:  la  cuerda,   conteniendo  los
  barycentros de  $\{ \phi(x_1),  \phi(x_2) \}$,  es siempre arriba  de la  curba, \ie
  funci\'on de los barycentros de $\{ x_1 , x_2 \}$.}}
%
\label{fig:MP:Convexa}
\end{figure}
%
Intuitivamente, la  media teniendo un sabor  de barycentro, se intuye  de que la
media  de $\phi(X)$ va  a ser  arriva de  la funci\'on  de la  media de  $X$. Es
precisamente el  teorema de Jensen~\footnote{En~\cite{Jen06} se trata  del en el
  caso discreto  y integral; en~\cite{Hol89,  Had93} se encuentran  las primeras
  semillas de  esta desigualdad,  y entre otros~\cite{Jes31:I,  Jes31:II, Per74,
    Rud91}  para versiones  m\'as generales.\label{foot:SZ:Jensen}}~\cite{Jen06,
 Fel71, Bre88, AthLah06, Coh13}:
%
\begin{teorema}[Desigualdad de Jensen]\label{Th:MP:Jensen}
  Sea \ $X$ \ integrable y definida  sobre $\X \subset \Rset^d$, convexo y \ $f:
  \X \mapsto \Rset$. Entonces
  %
  \[
  \Esp[\phi(X)] \ge \phi\left( \Esp[X] \right)
  \]
  %
  Si $\phi$ es  estrictamente convexa, la igualdad se alcanza  si y solamente si
  $X$ es determinista casi siempre.
\end{teorema}
%
\begin{proof}
  Sea \ $X = \sum_i x_i  \un_{A_i}$ \ variable escalonada. Entonces \ $\phi(X) =
  \sum_i \phi(x_i) \un_{A_i}$, dando
  %
  \[
  \Esp[\phi(X)]  = \sum_i  P(A_i)  \phi(x_i) \ge  \phi\left(  \sum_i P(A_i)  x_i
  \right) = \phi\left( \Esp[X] \right)
  \]
  %
  con igualdad  (cuando la convexidad es  estricta) si y solamente  si todos los
  $x_i$  son iguales.  Se  cierra la  prueba tomando  $X \ge  0$ como  limite de
  sucesi\'on   de   funciones  escalonadas   (teorema~\ref{Th:MP:MedibleLimite},
  pagina~\pageref{Th:MP:MedibleLimite}),  y cualquier $X$  tratando de  la parte
  positiva  y  negativa  (ver  pagina~\pageref{Th:MP:MedibleLimite}).   El  caso
  vectorial  se   trata  componente  a   componente  para  $X$  en   termino  de
  limite. Tomando  el limite, la  condici\'on $x_i$ todos iguales  vuelve ``casi
  todos'' los $x_i$ deben ser iguales, \ie $X$ debe ser constante casi siempre.
\end{proof}

Terminamos esta secci\'on  con una desigualdad tambi\'en muy  \'util, y conocida
en   los   espacios   de    Hilbert,   conocida   como   {\it   desigualdad   de
  H\"older}~\cite{Hol98}:
%
\begin{teorema}[Desigualdad de H\"older]
  Sean \ $X$ \  e \ $Y$ \ dos vectores aleatorios  $d$-dimensionales y $\alpha >
  1$. $\alpha^* >  1$ tal que $\frac1\alpha +  \frac1{\alpha^*}$ es llamado {\it
    conjugado de H\"older} de $\alpha$, y
  %
  \[
  \left|  \Esp\left[  X^t Y  \right]  \right| \:  \le  \:  \Esp\left[ \left\|  X
    \right\|_\alpha^\alpha   \right]^{\frac1\alpha}  \:  \Esp\left[   \left\|  Y
    \right\|_{\alpha^*}^{\alpha^*} \right]^{\frac1{\alpha^*}}
  \]
  %
  donde \ $\|  x \|_\alpha = \left( \sum_i  x_i^\alpha \right)^{\frac1\alpha}$ \
  denota la norma  $\alpha$ de un vector.  Se obtiene la  igualda si y solamente
  si existe un \ $\lambda$ \ tal que \ $X = \lambda Y$ \ casi siempre.
\end{teorema}
%
\begin{proof}
  Obviamente, $\left| \Esp\left[ X^t Y \right] \right| \le \Esp\left[ \left| X^t
      Y \right| \right]$. \ Luego, de  la convexidad de la funci\'on \ $-\log$ \
  se  obtiene  la  desigualdad  \  $\log(|a  b|) =  \frac1\alpha  \log  |a|^p  +
  \frac1{\alpha^*} \log  |b|^{\alpha^*} \le \log\left( \frac{|a|^\alpha}{\alpha}
    + \frac{|b|^{\alpha^*}}{\alpha^*} \right)$ \  con igualdad si y solamente si
  \ $a es proporcional a b$. \  Aplicado a las componentes de dos vectores \ $a$
  \ y  \ $b$ \ se  obtiene la desigualdad de  Young \ $\left| a^t  b \right| \le
  \frac{\|a\|_\alpha^\alpha}{\alpha}                                            +
  \frac{\|b\|_{\alpha^*}^{\alpha^*}}{\alpha^*}$ \ con igualdad si y solamente si
  los vectores son proporcional. A continuaci\'on, denotando
  %
  \[
  \widetilde{X}         =         \frac{X}{\Esp\left[        \|X\|_\alpha^\alpha
    \right]^{\frac1\alpha}}    \qquad    \mbox{y}    \qquad   \widetilde{Y}    =
  \frac{Y}{\Esp\left[ \|Y\|_{\alpha^*}^{\alpha^*} \right]^{\frac1{\alpha^*}}}
  \]
  %
  tenemos
  %
  \[
  \Esp\left[   \left|   X^t  Y   \right|   \right]   =   \Esp\left[  \left\|   X
    \right\|_\alpha^\alpha   \right]^{\frac1\alpha}  \:  \Esp\left[   \left\|  Y
    \right\|_{\alpha^*}^{\alpha^*}   \right]^{\frac1{\alpha^*}}   \:  \Esp\left[
    \left| \widetilde{X}^t \widetilde{Y} \right| \right]
  \]
  %
  De la desigualdad de Young, se obtiene entonces
  %
  \[
  \Esp\left[   \left|   \widetilde{X}^t   \widetilde{Y}  \right|   \right]   \le
  \frac{\Esp\left[ \left\| \widetilde{X} \right\|_\alpha^\alpha \right]}{\alpha}
  +   \frac{\Esp\left[   \left\|  \widetilde{Y}   \right\|_{\alpha^*}^{\alpha^*}
    \right]}{\alpha^*} = \frac1\alpha + \frac1{\alpha^*} = 1
  \]
  %
  lo que cierra la prueba.
\end{proof}

Un       corolario       es       conocido       como       desigualdad       de
Cauchy-Bunyakovsky-Schwarz~\footnote{Esta  desigualdad, fue  probada  por Cauchy
  para  sumas   en  1821~\cite{Cau21},   para  integrales  por   Bunyakovsky  en
  1859~\cite{Bou59} y  m\'as elegamente por  Schwarz en 1888~\cite{Sch88}  en un
  enfoque m\'as general. Ver tambi\'en~\cite{Ste04}.} para $p = \frac12$:
%
\begin{corolario}[Desiguladad de Cauchy-Bunyakovsky-Schwarz]
\end{corolario}
%
Nota:  se puede  probar esta  desigualdad  considerando el  polinomio $\Esp[  \|
\lambda X + Y \|_2^2] \ge 0$, del secundo orden en $\lambda$. Siendo no negativa
para cualquier $\lambda$ el discriminente debe ser no positivo, conduciendo a la
desigualdad.

De hecho,  se puede  ver $\Esp[X  Y]$ como un  producto escalar  entre variables
aleatorias. La  sola subtileza es  que $\Esp[X^2]  = 0$ conduce  a $X =  0$ casi
siempre, \ie se  puede tener $X \ne  0$ pero con medida de  probabilidad igual a
cero (ej. puntos $\omega$ ``aislados'' en el contexto continuo).

}