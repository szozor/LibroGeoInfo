\seccion{Funciones generadoras}
\label{Sec:MP:generatrices}

Como lo hemos visto, un vector aleatorio es completamente definida por su medida
de probabilidad $P$, o equivalemente por la medida imagen $P_X$, o a trav\'es de
la funci\'on de repartici\'on $F_X$. Sin  embargo, bajo el impulso de Laplace en
el  siglo XVII  (entre  otros), se  introdujo  caracterizaciones alternativas  a
trav\'es de transformaciones  de la medida de probabilidad,  conocidas como {\em
  funciones generadoras}  o {\em funciones  generatrices}~\footnote{De hecho, de
  manera general,  se introdujeron  tales funciones en  un marco  m\'as general,
  asociado   a   sucesiones  de   n\'umeros,   bajo   el   impulso  de   A.   de
  Moivre~\cite{Dem30};   ver   tambi\'en~\cite{Sti30,   Eul41,   Eul50,   Dem56}
  o~\cite[Sec.~1.2.9]{Knu97_v1}.        \label{Foot:MP:Generadora}}~\cite{Lap20}.
Existen varias funciones, cuyas tienes  propiedades particulares que vamos a ver
en las subsecciones siguientes. Entre  otros, estas funciones dadas como valores
de expectaci\'on  de funciones de  la variable aleatoria (discreta  o continua),
con un par\'ametro  real o complejo, permiten hallar  f\'acilmente los distintos
momentos de una distribuci\'on de probabilidad.


% ================================= Funcion generadora de probabilidad

\subseccion{Funci\'on generadora de probabilidad}
\label{Ssec:MP:GeneradoraProbabilidad}

De  manera  general,  siguiendo  el  enfoque  de A.   de  Moivre  (ver  nota  de
pie~\ref{Foot:MP:Generadora}) dada una sucesi\'on \ $a_n, \quad n \in \Nset$, se
define la  funci\'on generadora  dicha {\em ordinaria}  de la sucesi\'on  como \
$G(\{ a_n  \}_{n \in \Nset} \,  , \, z) =  \sum_{n \in \Nset} \,  z^n$. A veces,
esta serie es conocida como transformada en $z$ de la sucesi\'on \ $\{ a_n \}_{n
  \in \Nset}$. Tratando de variables aleatorias discretas sobre \ $\Nset$, con \
$p_n  = P_X(n)  = P(X  = n)$,  se puede  definir as\'i  la  funci\'on generadora
asociada a  la sucesi\'on \  $p_n$ y se  puede ver que no  es nada m\'as  que el
momento $\Esp\left[ z^X \right]$.  De manera general, la funci\'on generadora de
probabilidad  se define  de  la manera  siguiente~\cite{Fel68, JohKot97,  Muk00,
  AthLah06}:
%
\begin{definicion}[funci\'on   generadora   de   probabilidad  o   de   momentos
  factoriales]
\label{Def:MP:GeneradoraProbabilidadFactorial}
%
  Sea $X = \begin{bmatrix} X_1  & \cdots & X_d \end{bmatrix}^t$ vector aleatorio
  $d$ dimensional  definido sobre $\X  \subset \Rset^d$.  La  funci\'on definida
  por
  %
  \[
  G_X(z) =  \Esp\left[ \prod_{i=1}^d  z_i^{X_i} \right]\quad \mbox{con}  \quad z
  = \begin{bmatrix} z_1 & \cdots & z_d \end{bmatrix}^t \in \Cset^d
  \]
  %
  es conocida como  {\em funci\'on generadora de probabilidad}  o {\em funci\'on
    generadora de momentos factoriales} de $X$.
\end{definicion}
%
\SZ{hyperanulus ie entre deux spheres: (SZ estoy retomando lo que sigue);
 Cuando  los \ $X_i$ \  son todas variables  aleatorias positivas, esta
integral         converge        uniformamente        en         una        bola
$d$-dimensional~\footnote{Claramente,  en  $\Bset_d  \equiv \Bset_d(1)$  tenemos
  $\displaystyle  \left|  \int_{\Rset_+^d}  \prod_{i=1}^d z_i^{x_i}  \,  dP_X(x)
  \right| \le \int_{\Rset_+^d} \left| \prod_{i=1}^d z_i^{x_i} \right| \, dP_X(x)
  \le  \int_{\Rset_+^d}  dP_X(x)  =   1$.  Adem\'as,  si  la  integral  converge
  uniformamente para un  $z$ tal que $|z|  = r$, por el mismo  enfoque se prueba
  que converge en  $\Bset_d(r)$.} de rayo $r \ge 1$ $\Bset_d(r)  = \left\{ z \in
  \Cset^d: \: \| z \| \le r \right\}$  donde $\| z \|^2 = \sum_i |z_i|^2$.
% En el
%caso  contrario,  puede resultar  delicado  en  termino  de existencia  de  esta
%funci\'on  (convergencia de la  integral); se  puede por  ejemplo que,  para una
%variables, no existe ninguno dominio de existencia de esta funci\'on.
}

\

La denominaci\'on  {\em generadora de  probabilidad} (pgf para  {\em probability
  generating function} en ingles) se entiende sencillamente del hecho siguiente:
%
\begin{lema}
\label{Lem:MP:GeneracionProbabilidades}
%
  Cuando \ $\X = \Nset^d$ \ para cualquier \ $k = \begin{bmatrix} k_1 & \ldots &
    k_d \end{bmatrix}^t \in \Nset^d$, con $K = \sum_{i=1}^d k_i$
  %
  \[
  \frac1{\prod_{i=1}^d  k_i!} \, \left.\frac{\partial^K  G_X}{\partial z_1^{k_1}
      \ldots \partial z_d^{k_d}}\right|_{z=0} = P_X(k) = P(X = k)
  \]
\end{lema}
%
\begin{proof}
  Se puede escribir la funci\'on \ $G_X$ \ bajo su forma de generadora ordinaria
  \ $\displaystyle G_X(z) =  \sum_{n \in \Nset^d} \left( \prod_{i=1}^d z_i^{n_i}
  \right)  P(X   =  n)$  \   con  \  $n   =  \begin{bmatrix}  n_1  &   \cdots  &
    n_d  \end{bmatrix}^t$.  A  continuaci\'on,  se nota  que  la serie  converge
  uniformamente por  lo menos en  la bola $\Bset_d \equiv  \Bset_d(1)$, probando
  que $G_X$  es diferenciable en $\Bset_d$,  as\'i que se puede  ver esta series
  como el desarollo de Taylor de $G_X$ (o, equivalentemente, diferenciar bajo la
  suma y tomar la derivada en $z = 0$), lo que cierra la prueba.
\end{proof}

De este resultado,  se puede notar que, en el caso  discreto, hay una relaci\'on
uno-a-uno entre  la medida  de probabilidad $P_X$  y la funci\'on  generadora de
probabilidad $G_X$.  En el caso continuo  \SZ{(y m\'as general)},  veremos en la
subsecci\'on~\ref{Ssec:MP:FuncionCaracterisica} que para  $z_j$ de la forma $z_j
= e^{\imath u_j}$ \  con \ $u_j \in \Rset$ \ la  transformaci\'on se inversa, de
manera que se  puede recuperar la densidad de  probabilidad $p_X$ \SZ{(medida de
  probabilidad $P_X$)} a partir de $G_X$. Dicho de otra manera, como la medida \
$P_X$, \ la funci\'on \ $G_X$  \ caracteriza completamente el vector aleatorio \
$X$.

\

Aparece  que la  funci\'on generadora  \ $G_X$  \ se  vincula tambi\'en  con los
momentos factoriales, justificando su secunda denominaci\'on, {\em generadora de
  momentos factoriales}  (fmgf para {\em factorial  moments generating function}
en ingles):
%
\begin{lema}
\label{Lem:MP:GeneracionMomentosFactoriales}
%
  Para cualquier \  $k = \begin{bmatrix} k_1 & \cdots  & k_d \end{bmatrix}^t \in
  \Nset^d$ \ con \ $K =  \sum_{i=1}^d k_i$, derivando $G_X$ se proba que, cuando
  existen~\footnote{En el  caso extremo,  el rayo de  convergencia de  la serie
    dando $G_X$ es igual a 1, as\'i  que no hay garantia que las derivadas en $z
    = 1$ existen.}
  %
  \[
  \left.\frac{\partial^K     G_X}{\partial     z_1^{k_1}     \cdots     \partial
      z_d^{k_d}}\right|_{z=1}    =   \Esp\left[    \prod_{i=1}^d    \left(   X_i
    \right)_{k_i} \right]
  \]
  %
  momento factorial~\footnote{Recuerdense que  $(x)_n = \prod_{i=0}^{n-1} (x-i),
    \quad n > 0$ \ s\'imbolo de Pochhammer, con la convenci\'on $(x)_0 = 1$; ver
    pagina~\pageref{Foot:MP:Pochhammer}} de $X$.
\end{lema}

De  este resultado,  se ve  por ejemplo  que, cuando  existen, se  recuperan los
momentos de $X$ a trav\'es de las derivadas de $G_X$:
%
\begin{itemize}
\item $G_X(1) = 1$, condici\'on de normalizaci\'on.
%
\item $\nabla_z G_X(1) = \Esp[X]$ \ donde $\nabla_z$ indica el gradiente, \ie el
  vector de componente $i$-esima \ $\frac{\partial}{\partial z_i}$.
%
\item $\Hess_z G_X(1)  + \diag\left( \nabla_z G_X(1) \right)  = \Esp\left[ X X^t
  \right]$  \ donde  \ $\Hess_z$  \ es  la matrice  Hessiana, \ie  la  matriz de
  componente $(i,j)$-esima \ $\frac{\partial^2}{\partial z_i \partial z_j}$, y \
  $\diag(a)$ es  une matriz  diagonal de componentes  \ $(i,i)$-esima \  $a_i$ \
  (vector $a$ sobre la diagonal).  Entonces  la matriz de covarianza es dada por
  \ $\Cov[X] =  \Hess_z G_X(1) + \diag\left( \nabla_z  G_X(1) \right) - \nabla_z
  G_X(1) \nabla_z^t G_X(1)$.
\end{itemize}

\

La funci\'on \ $G_X$ \ tiene unas propiedades permitiendo por ejemplo de manejar
sencillamente  distribuciones  de probabilidades  de  combinaciones lineales  de
vectores aleatorios independientes,  como lo vamos a ver  a trav\'es del teorema
siguiente.

\begin{teorema}%[Funci\'on generadora de probabilidad de una ]
\label{Teo:MP:PropiedadesGeneradoraProbabilidad}
%
  Sean  \   $X$  \  e  \   $Y$  \  dos   vectores  aleatorios  $d$-dimensionales
  independientes, $a  = \begin{bmatrix} a_1  & \cdots & a_d  \end{bmatrix}^t \in
  \Rset^d$ \  y \ $b  = \begin{bmatrix} b_1  & \cdots & b_d  \end{bmatrix}^t \in
  \Rset^d$.  Entonces  para  cualquier  $z  = \begin{bmatrix}  z_1  &  \cdots  &
    z_d \end{bmatrix}\in \Cset^d$ \ (donde existen las funciones):
  %
  \[
  G_{\diag(a) X + b}(z) =  \prod_{i=1}^d z_i^{b_i} G_X\left( z_1^{a_1} , \ldots ,
    z_d^{a_d} \right),
  \]
  %
  \[
  G_{X+Y}(z) = G_X(z) \, G_Y(z)
  \]
  %
  y para $z \in \Cset$
  %
  \[
  G_X\left( z^{a_1} , \ldots , z^{a_d} \right) = G_{a^t X}(z)
  \]
\end{teorema}
%
\begin{proof}
  El  primer  resultado es  inmediato,  escribiendo \  $z_i^{a_i  X_i  + b_i}  =
  z_i^{b_i}   \left(  z_i^{a_i}   \right)^{X_i}$.    El  secundo   viene  de   \
  $z_i^{X_i+Y_i}    =    z_i^{X_i}    z^{Y_i}$    \   conjuntamente    con    el
  teorema~\ref{Teo:MP:IndependenciaMomentos}   con   \   $f(X)  =   \prod_{i=1}^d
  z_i^{X_i}$  \ y \  $g(Y) =  \prod_{i=1}^d z_i^{Y_i}$.  El tercer  resultado es
  consecuencia de $\prod_{i=1}^d  \left( z^{a_i} \right)^{X_i} = z^{\sum_{i=1}^d
    a_i X_i}$.
\end{proof}
%
Estos  resultados permiten manejar  sencillamente la  medida de  probabilidad de
combinaciones lineales  de vectores aleatorios independientes y  de marginales a
trav\'es esta funci\'on generadora.

\

De  la  tercera identidad,  se  puede  hacer un  paso  m\'as  tratando de  sumas
aleatorias de vectores aleatorios:
%
\begin{teorema}
\label{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}
%
  Sea  \ $X_n,  \quad n  \in  \Nset$ \,  una sucesi\'on  de vectores  aleatorios
  indepedientes de  misma distribuci\'on  (resp.  generadora de  probabilidad) \
  $P_X$ \  (resp. $G_X$)  \ y  \ $N$ \  una variable  definida sobre  \ $\Nset$,
  independiente de los  \ $X_n$. Sea el vector aleatorio \  $ S_N = \sum_{n=0}^N
  X_n$. Entonces
  %
  \[
  G_{S_N}(z) =  G_N \big( G_X(z) \big),
  \]
  %
\end{teorema}
%
\begin{proof}
  \SZ{Usando la formula de esperanza total ec.~\ref{Teo:MP:EsperanzaTotal}, se escribe}
  %
  \begin{eqnarray*}
  G_{S_N}(z) & = & \Esp\left[ z^{\sum_{n=0}^N X_n} \right]\\[2.5mm]
  %
  & = & \Esp\left[ \Esp\left[ \left. z^{\sum_{n=0}^N X_n} \right| N  \right] \right]\\[2.5mm]
  %
  & = & \Esp\left[ G_X(z)^N \right]
  \end{eqnarray*}
\end{proof}


% ================================= Funcion generadora de momentos

\subseccion{Funci\'on generadora de momentos}
\label{Ssec:MP:GeneradoraMomentos}


Como lo hemos visto, la  funci\'on generadora de probabilidad permite recucperar
los  momentos  de  un  vector  aleatorio  a trav\'es  de  combinaciones  de  sus
derivadas.   Con una pequa\~na  modificaci\'on, se  puede definir  una funci\'on
generada  permitiendo  recuperar  m\'as  directamente los  momentos,  de  manera
siguiente~\cite{Fel68, JohKot97, Muk00, AthLah06}:
%
\begin{definicion}[funci\'on generadora de momentos]
\label{Def:MP:GeneradoraMomentos}
%
  La {\em  funci\'on generadora  de momentos} (mgf  para {\em  moment generating
    function} en ingles) de un vector aleatorio $d$-dimensional se define como
  %
  \[
  M_X(u) = \Esp\left[ e^{z^t X} \right]
  \]
  %
  para $u \in \Cset^d$.
\end{definicion}
%
De esta definici\'on se nota inmediatamente que
%
\[
M_X(u) = G_X\left( e^u \right)  \qquad \mbox{donde} \qquad e^u = \begin{bmatrix}
  e^{u_1} & \cdots & e^{u_d} \end{bmatrix}^t
\]
%
Entonces, como \ $G_X$, la  generadora de los momentos caracteriza completamente
el  vector  aleatorio \  $X$.   \SZ{Producto cartesiano  de  strip  en el  plano
  complejos:  (SZ estoy  retomando lo  que  sigue); Adem\'as,  resuelta de  esta
  relaci\'on que cuando los \  $X_i$ \ son todas variables aleatorias positivas,
  $M_X$ es definida en un dominio de  \ $\Cset^d$ tal que \ $\real{u_i} \le v_i$
  \ donde \ $\real{\cdot}$ \ denota la  parte real de un n\'umero complejo y los
  \ $v_i$  \ son  positivos llamados  indices de convergencia.}   En el  caso de
variables escalares admitiendo una densidad de probabildad \ $p_X$, denotando $s
= -u$, esta funci\'on se  interpreta como la transformada (bilateral) de Laplace
de \ $p_X$.


La  generadora de  los momentos  permite recuperar  directamente los  momentos a
trav\'es de derivadas, sin hacer combinaciones:
%
\begin{lema}
\label{Lem:MP:GenracionMomentos}
%
  Para cualquier \  $k = \begin{bmatrix} k_1 & \cdots  & k_d \end{bmatrix}^t \in
  \Nset^d$ \ con \ $K =  \sum_{i=1}^d k_i$, derivando $M_X$ se proba que, cuando
  existen
  %
  \[
  \left.\frac{\partial^K     M_X}{\partial     u_1^{k_1}     \cdots     \partial
      u_d^{k_d}}\right|_{u=0}  = \Esp\left[  \prod_{i=1}^d  X_i^{k_i} \right]  =
  m_{k_1,\ldots,k_d}
  \]
  %
  momento de orden \ $k$ \ de \ $X$.
\end{lema}
%
En particular, se recuperan
%
\begin{itemize}
\item $M_X(0) = 1$, condici\'on de normalizaci\'on.
%
\item $\nabla_u M_X(0) = \Esp[X]$ \ promedio,
%
\item $\Hess_u M_X(0) = \Esp\left[ X X^t \right]$, \ie $\Cov[X] = \Hess_u M_X(0)
  - \nabla_u M_X(0) \nabla_u^t M_X(0)$ \ matriz de covarianza.
\end{itemize}

Como la funci\'on \ $G_X$, la  generadora de los momentos tiene unas propiedades
similares  a las  de  los teoremas~\ref{Teo:MP:PropiedadesGeneradoraProbabilidad}
y~\ref{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}:
%
\begin{teorema}%[Funci\'on generadora de probabilidad de una ]
\label{Teo:MP:PropiedadesGeneradoraMomentos}
%
  Sean  \   $X$  \  e  \   $Y$  \  dos   vectores  aleatorios  $d$-dimensionales
  independientes,  \ $A$  \ una  matriz de  \  $\Rset^{d' \times  d}$ \  y \  $b
  =   \begin{bmatrix}    b_1   &    \cdots   &   b_{d'}    \end{bmatrix}^t   \in
  \Rset^{d'}$.  Entonces para  cualquier $u  =  \begin{bmatrix} u_1  & \cdots  &
    u_{d'} \end{bmatrix}^t \in \Cset^{d'}$ \ (donde la funci\'on existe):
  %
  \[
  M_{A X + b}(u) =  e^{u^t b} M_X\left( A^t u \right),
  \]
  %
  y para cualquier  $u = \begin{bmatrix} u_1 & \cdots  & u_d \end{bmatrix}^t \in
  \Cset^d$ \ (donde la funci\'on existe):
  %
  \[
  M_{X+Y}(u) = M_X(u) \, M_Y(u)
  \]
  %
  Adem\'as,  para \  $X_n, \quad  n  \in \Nset$  \, una  sucesi\'on de  vectores
  aleatorios  indepedientes  de   misma  distribuci\'on  (resp.   generadora  de
  momentos) \ $P_X$ \ (resp. $M_X$) \  e \ $N$ \ una variable aleatoria definida
  sobre \ $\Nset$, independiente de los \ $X_n$, y \ $ S_N = \sum_{n=0}^N X_n$,
  %
  \[
  M_{S_N}(u) =  G_N \big( M_X(u) \big),
  \]
\end{teorema}
%
\begin{proof}
  Las  pruebas  siguen   punto  a  punto  los  mismos  pasos   que  las  de  los
  teoremas~\ref{Teo:MP:PropiedadesGeneradoraProbabilidad}
  y~\ref{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}.
\end{proof}

\

De nuevo, se puede hacer un  paso m\'as tratando de sumas aleatorias de vectores
aleatorios como en el teorema~\ref{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}:
%
\begin{teorema}
\label{Teo:MP:SumaAleatoriaGeneradoraMomentos}
%
  Sea  \ $X_n,  \quad n  \in  \Nset$ \,  una sucesi\'on  de vectores  aleatorios
  indepedientes de  misma distribuci\'on  (resp.  generadora de  probabilidad) \
  $P_X$ \  (resp. $M_X$) \  e \  $N$ \ una  variable aleatoria definida  sobre \
  $\Nset$,  independiente de los  \ $X_n$.  Sea el  vector aleatorio  \ $  S_N =
  \sum_{n=0}^N X_n$. Entonces
  %
  \[
  M_{S_N}(u) =  G_N \big( M_X(u) \big),
  \]
  %
\end{teorema}
%
\begin{proof}
  El         resultado        es         consecuencia         directa        del
  teorema~\ref{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}.
\end{proof}
%\cite{Fra09}%%p.73


% ================================= Funcion caracteristica

\subseccion{Funci\'on caracter\'istica}
\label{Ssec:MP:FuncionCaracteristica}

Si  la funci\'on generadora  de momentos  permite recuperar  los momentos  de un
vector  aleatorio, no  es definida  sobre todo  $\Cset^d$.  Sin  embargo, cuando
$\real{u_i} = 0$,  esta funci\'on es siempre definida  ($e^u \in \Sset_d \subset
\Bset_d$   \   donde   \    $\Sset_d$   es   la   hiperespfera   $d$-dimensional
unitaria).   Entonces,  una  funci\'on   generadora  muy   \'util  que   se  usa
frecuentemente  es la  de  momentos para  este  tipo de  argumentos,  lo que  es
conocida  como funci\'on  caracter\'istica  y  que es  al  final definida  sobre
$\Rset^d$  de  manera  siguiente~\cite{Luk61,  Gol61,  Fel68,  JohKot97,  Muk00,
  AthLah06}:

\begin{definicion}[funci\'on   caracter\'istica]
\label{Def:MP:FuncionCaracteristica}
% 
  La {\em funci\'on caracter\'istica}  (cf para {\em characteristic function} en
  ingles) de un vector aleatorio $d$-dimensional se define como
 %
  \[
  \Phi_X(\omega) = \Esp\left[ e^{\imath \omega^t X} \right]
  \]
  %
  para $\omega \in \Rset^d$.
\end{definicion}
%
De esta definici\'on se nota inmediatamente que
%
\[
\Phi_X(\omega) = M_X(\imath \omega) = G_X\left( e^{\imath \omega} \right) \qquad
\mbox{donde} \qquad e^{\imath \omega}  = \begin{bmatrix} e^{\imath u_1} & \cdots
  & e^{\imath u_d} \end{bmatrix}^t
\]
%
De hecho,  se puede definir esta  funci\'on para un argumento  complejo, pero es
equivalente a volver a la definici\'on de la generadora de momentos.

En su forma general, la funci\'on caracter\'istica se escribe
%
\[
\Phi_X(\omega) = \int_{\Rset^d} e^{\imath \omega^t x} \, dP_X(x)
\]
%
y  es  relacionada   a  la  transformada  de  Fourier-Stieltjes   de  la  medida
$P_X$~\cite[Chap.~5]{Pin09}. Cuando  \ $P_X$ \  admite una densidad \  $p_X$, la
funci\'on  es  una  transformada  de  Fourier  usual de  la  densidad  \  $p_X$,
introducida bajo el  impulso de Fourier en 1822 para  estudiar la difusi\'on del
calor~\cite{Fou22}.

Insistamos sobre  el hecho que  la importancia de  esta funci\'on reside  en que
siempre existe y est\'a bien  definida, dado que \ $\displaystyle \int_{\Rset^d}
\left| e^{\imath \omega^t x} \right| \, dP_X(x) = \int_{\Rset^d} dP_X(x) = 1$.
% \cite{Gol61}

Como para las generadoras ya introducidas, la funci\'on caracter\'istica permite
recuperar directamente los momentos a trav\'es de derivadas:
%
\begin{lema}
\label{Lem:MP:GeneracionMomentoViaCaracteristica}
%
  Para cualquier \  $k = \begin{bmatrix} k_1 & \cdots  & k_d \end{bmatrix}^t \in
  \Nset^d$ \  con \  $K =  \sum_{i=1}^d k_i$, derivando  $\Phi_X$ se  proba que,
  cuando existen
  %
  \[
  (-   \imath)^K  \,   \left.\frac{\partial^K   \Phi_X}{\partial  \omega_1^{k_1}
      \cdots    \partial    \omega_d^{k_d}}\right|_{\omega=0}    =    \Esp\left[
    \prod_{i=1}^d X_i^{k_i} \right] = m_{k_1,\ldots,k_d}
  \]
  %
  momento de orden \ $k$ \ de \ $X$.
\end{lema}
%
En particular, se recuperan
%
\begin{itemize}
\item $\Phi_X(0) = 1$, condici\'on de normalizaci\'on.
%
\item $- \imath \nabla_\omega M_X(0) = \Esp[X]$ \ promedio,
%
\item  $- \Hess_\omega  M_X(0) =  \Esp\left[ X  X^t \right]$,  \ie $\Cov[X]  = -
  \Hess_\omega M_X(0) + \nabla_\omega M_X(0) \nabla_\omega^t M_X(0)$ \ matriz de
  covarianza.
\end{itemize}

Fijense de  que \  $\Phi_X$ \  no es siempre  diferencial en  $\omega =  0$; Por
ejemplo,    en   el    caso    de   la    distribuci\'on   de    Cauchy--Lorentz
univariada~\footnote{Lo      mismo       occure      en      la      extensi\'on
  multivariada~\cite{SamTaq91}.}\ $p_X(x) =  \frac{\gamma}{\pi \left( \gamma^2 +
    (x-x_0)^2  \right)}$ \  con  \ $\gamma  >  0$, resulta  \ $\Phi_X(\omega)  =
e^{-\imath x_0  \omega -\gamma |\omega}  $. Esta funci\'on est\'a  definida para
todo $\omega$,  pero no es  derivable en  $\omega = 0$,  lo que coincide  con el
hecho  de  que  no  est\'an   definidos  los  momentos  para  esta  densidad  de
probabilidad.

Como las  funci\'ones \ $G_X$ \  y \ $M_X$, la  funci\'on caracter\'istica tiene
entre    otros    propiedades    similares     a    las    a    las    de    los
teoremas~\ref{Teo:MP:PropiedadesGeneradoraMomentos}
y~\ref{Teo:MP:SumaAleatoriaGeneradoraMomentos}:
%
\begin{teorema}%[Funci\'on generadora de probabilidad de una ]
\label{Teo:MP:PropiedadesFuncionCaracteristica}
%
  Sean  \   $X$  \  e  \   $Y$  \  dos   vectores  aleatorios  $d$-dimensionales
  independientes,  \ $A$  \ una  matriz de  \  $\Rset^{d' \times  d}$ \  y \  $b
  =  \begin{bmatrix} b_1  &  \cdots &  b_{d'}  \end{bmatrix}^t \in  \Rset^{d'}$.
  Entonces  para  cualquier  $\omega  =  \begin{bmatrix}  \omega_1  &  \cdots  &
    \omega_{d'} \end{bmatrix}^t \in \Rset^{d'}$:
  %
  \[
  \Phi_{A X + b}(\omega) =  e^{\imath \omega^t b} \Phi_X\left( A^t \omega \right),
  \]
  %
  y   para   cualquier  $\omega   =   \begin{bmatrix}   \omega_1   &  \cdots   &
    \omega_d \end{bmatrix}^t \in \Rset^d$:
  %
  \[
  \Phi_{X+Y}(\omega) = \Phi_X(\omega) \, \Phi_Y(\omega)
  \]
  %
  Adem\'as,  para \  $X_n, \quad  n  \in \Nset$  \, una  sucesi\'on de  vectores
  aleatorios  indepedientes  de   misma  distribuci\'on  (resp.   generadora  de
  momentos) \ $P_X$ \ (resp. $M_X$) \  e \ $N$ \ una variable aleatoria definida
  sobre \ $\Nset$, independiente de los \ $X_n$, y \ $ S_N = \sum_{n=0}^N X_n$,
  %
  \[
  \Phi_{S_N}(\omega) =  G_N \big( \Phi_X(\omega) \big),
  \]
\end{teorema}

Resumimos algunas otras propiedades importantes de la funci\'on caracter\'istica:
%
\begin{enumerate}
\item\label{Prop:MP:ContinuidadPhiX}  $\Phi_X$  es  una  funci\'on  continua  en
  $\Rset^d$~\cite[Prop.~5.2.1]{Pin09}.  Eso es una  consecuencia del  teorema de
  convergencia     dominada     (ver    teorema~\ref{Teo:MP:ConvergenciaDominada}
  pagina~\pageref{Teo:MP:ConvergenciaDominada}).
%
\item\label{Prop:MP:MaximoPhiX} $\left| \Phi_X(\omega) \right| \le 1 \Phi_X(0)$:
  \  $\left| \Phi_X(\omega)  \right|$ es  m\'axima en  $\omega =  0$.  Eso viene
  directamente de $\left| e^{\imath \omega^t x} \right| = 1$.
%
\item\label{Prop:MP:HermiticaPhiX}   $\Phi_X(-\omega)  =   \Phi_X^*(\omega)$:  \
  $\Phi_X$ tiene una s\'imetria hermitica.
%
\item  $\Phi_X$ es  una funci\'on  no negativa  definida, \ie  para  un conjunto
  arbitrario de \ $k  \ge 1$ \ n\'umeros complejos \ $a_1 , \ldots  , a_k$ \ y \
  $k$ \ vectores de \ $\Rset^d$ \ $w_1 , \ldots , w_k$, se cumple
  \[
  \sum_{i,j=1}^k a_i^* a_j \Phi_X(w_j-w_i) \ge 0
  \]
  %
  Dicho   de   otra   manera,   la   matriz  de   componente   \   $(i,j)$-esima
  $\Phi_X(w_j-w_i)$ es a hermitica (s\'imetria herm\'itica dada por la propiedad
  anterior, y no negativa definida).
%
\end{enumerate}

\SZ{Formulacion de inversion general; caso con densidad y se cuadrado integrable; uno-uno casi siempre}

\aver{

Si la pdf \ $p(x)$ es de cuadrado integrable, entonces 
$$
p(x) = \frac{1}{2	pi} \int e^{-i \xi x} C_X(\xi) \, d\xi .
$$
El requisito  para esta importante relaci\'on es  que \ $\int_{-\infty}^{\infty}
|p(x)|^2 \, dx<\infty$;  sin embargo, a\'un es v\'alida  para distribuciones con
una contribuci\'on  tipo $\delta$.  Por otro  lado los momentos,  si existen, se
obtienen derivando la funci\'on $C$ tal como expresa la siguiente proposici\'on:

Para  una   variable  aleatoria  compleja   $Z=X+iY$,  usando  la   noci\'on  de
transformada de Fourier bidimensional, se define:
$$
C_Z(\mu) \equiv \int e^{\mu^* z-\mu z^*} p(z) \, d^2z .
$$

{\teorema (Bochner, Goldberg).... } %%


\hfill

Cumulant generating function .... %%

\SZ{Se define los cumulantes. coinciden con los momentos centrados solo para $k = 1, 2, 3$.}

\hfill

}



\SZ{hablar de la cota de Chernoff con la mgf o pgf?}

\SZ{hablar del CLT y prueba}
