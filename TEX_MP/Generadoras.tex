\seccion{Funciones generadoras}
\label{Sec:MP:generatrices}

Como lo hemos visto, un vector aleatorio es completamente definida por su medida
de probabilidad $P$, o equivalemente por la medida imagen $P_X$, o a trav\'es de
la funci\'on de repartici\'on $F_X$. Sin  embargo, bajo el impulso de Laplace en
el  siglo XVII  (entre  otros), se  introdujo  caracterizaciones alternativas  a
trav\'es de transformaciones  de la medida de probabilidad,  conocidas como {\em
  funciones generadoras}  o {\em funciones  generatrices}~\footnote{De hecho, de
  manera general,  se introdujeron  tales funciones en  un marco  m\'as general,
  asociado   a   sucesiones  de   n\'umeros,   bajo   el   impulso  de   A.   de
  Moivre~\cite{Dem30};   ver   tambi\'en~\cite{Sti30,   Eul41,   Eul50,   Dem56}
  o~\cite[Sec.~1.2.9]{Knu97_v1}.        \label{Foot:MP:Generadora}}~\cite{Lap20}.
Existen varias funciones, cuyas tienes  propiedades particulares que vamos a ver
en las subsecciones siguientes. Entre  otros, estas funciones dadas como valores
de expectaci\'on  de funciones de  la variable aleatoria (discreta  o continua),
con un par\'ametro  real o complejo, permiten hallar  f\'acilmente los distintos
momentos de una distribuci\'on de probabilidad.


% ================================= Funcion generadora de probabilidad

\subseccion{Funci\'on generadora de probabilidad}
\label{Ssec:MP:GeneradoraProbabilidad}

De  manera  general,  siguiendo  el  enfoque  de A.   de  Moivre  (ver  nota  de
pie~\ref{Foot:MP:Generadora}) dada una sucesi\'on \ $a_n, \quad n \in \Nset$, se
define la  funci\'on generadora  dicha {\em ordinaria}  de la sucesi\'on  como \
$G(\{ a_n  \}_{n \in \Nset} \,  , \, z) =  \sum_{n \in \Nset} \,  z^n$. A veces,
esta serie es conocida como transformada en $z$ de la sucesi\'on \ $\{ a_n \}_{n
  \in \Nset}$. Tratando de variables aleatorias discretas sobre \ $\Nset$, con \
$p_n  = P_X(n)  = P(X  = n)$,  se puede  definir as\'i  la  funci\'on generadora
asociada a  la sucesi\'on \  $p_n$ y se  puede ver que no  es nada m\'as  que el
momento $\Esp\left[ z^X \right]$.  De manera general, la funci\'on generadora de
probabilidad  se define  de  la manera  siguiente~\cite{Fel68, JohKot97,  Muk00,
  AthLah06}:
%
\begin{definicion}[funci\'on   generadora   de   probabilidad  o   de   momentos
  factoriales]
\label{Def:MP:GeneradoraProbabilidadFactorial}
%
  Sea $X = \begin{bmatrix} X_1  & \cdots & X_d \end{bmatrix}^t$ vector aleatorio
  $d$ dimensional  definido sobre $\X  \subset \Rset^d$.  La  funci\'on definida
  por
  %
  \[
  G_X(z) =  \Esp\left[ \prod_{i=1}^d  z_i^{X_i} \right]\quad \mbox{con}  \quad z
  = \begin{bmatrix} z_1 & \cdots & z_d \end{bmatrix}^t \in \Cset^d
  \]
  %
  es conocida como  {\em funci\'on generadora de probabilidad}  o {\em funci\'on
    generadora de momentos factoriales} de $X$.
\end{definicion}
%
Esta funci\'on est definida sobre un producto cartesiano de anillos~\footnote{Si
  para \ $\left| z_i \right| \,  = \, R_i$ \ la integral converge uniformamente,
  para cualquier  \ $z \in  \Cset^d, \:  r_i \le \left|  z_i \right| \le  R_i$ \
  tenemos  por ejemplo  \ $\displaystyle  \left|  \int_{\Rset_+^d} \prod_{i=1}^d
    z_i^{x_i}  \,  dP_X(x)  \right|  \le \int_{\Rset_+^d}  \left|  \prod_{i=1}^d
    z_i^{x_i}  \right|  \,  dP_X(x)  \le \int_{\Rset_+^d}  \left|  \prod_{i=1}^d
    R_i^{x_i} \right| \, dP_X(x) < +\infty$.  El mismo enfoque se usa para todos
  los hipercuadrantes  \ $\otimes_i  \Rset_{\pm}$. Adem\'as, claramente,  para \
  $\left| z_i \right| \, =  \, 1$ \ tenemos $\displaystyle \left| \int_{\Rset^d}
    \prod_{i=1}^d  z_i^{x_i}  \,   dP_X(x)  \right|  \le  \int_{\Rset^d}  \left|
    \prod_{i=1}^d z_i^{x_i} \right| \,  dP_X(x) \le \int_{\Rset^d} dP_X(x) = 1$,
  lo       que      prueba       que      \       $r_i      \le       1      \le
  R_i$.\label{Foot:MP:GeneradoraProbabilidadExistencia}} en  el plano complejos,
$r_i \, \le \, \left|  z_i \right| \, \le \, R_i$ \ con \ $r_i  \le 1$ \ y \ $R_i
\ge 1$.

\

La denominaci\'on  {\em generadora de  probabilidad} (pgf para  {\em probability
  generating function} en ingles) se entiende sencillamente del hecho siguiente:
%
\begin{lema}
\label{Lem:MP:GeneracionProbabilidades}
%
  Cuando \ $\X = \Nset^d$ \ para cualquier \ \modif{$x = \begin{bmatrix} x_1 & \ldots &
    x_d \end{bmatrix}^t \in \Nset^d$, con $k = \sum_{i=1}^d x_i$}
  %
  \[
  \frac1{\prod_{i=1}^d  \modif{x_i!}} \, \left.\frac{\partial^k  G_X}{\partial z_1^{\modif{x_1}}
      \ldots \partial z_d^{\modif{x_d}}}\right|_{z=0} = P_X(\modif{x}) = P(X = \modif{x})
  \]
\end{lema}
%
\begin{proof}
  Se puede escribir la funci\'on \ $G_X$ \ bajo su forma de generadora ordinaria
  \ $\displaystyle  G_X(z) =  \sum_{\modif{x} \in \Nset^d}  \left( \prod_{i=1}^d
    z_i^{\modif{x_i}}   \right)   P(X  =   \modif{x})$   \   con  \   $\modif{x}
  =  \begin{bmatrix} \modif{x_1}  &  \cdots &  \modif{x_d} \end{bmatrix}^t$.   A
  continuaci\'on, se nota que la serie converge uniformamente por lo menos en la
  bola  $\Bset_d \equiv  \Bset_d(1)$,  probando que  $G_X$  es diferenciable  en
  $\Bset_d$, as\'i que  se puede ver esta series como el  desarollo de Taylor de
  $G_X$ (o, equivalentemente, diferenciar bajo la suma y tomar la derivada en $z
  = 0$), lo que cierra la prueba.
\end{proof}

De este resultado,  se puede notar que, en el caso  discreto, hay una relaci\'on
uno-a-uno entre  la medida  de probabilidad $P_X$  y la funci\'on  generadora de
probabilidad   $G_X$.     \modif{En   el   caso   general},    veremos   en   la
subsecci\'on~\ref{Ssec:MP:FuncionCaracterisica} que para  $z_j$ de la forma $z_j
= e^{\imath u_j}$ \  con \ $u_j \in \Rset$ \ la  transformaci\'on se inversa, de
manera que se  puede recuperar la medida  de probabilidad \ $P_X$ \  a partir de
$G_X$. Dicho de  otra manera, como la medida  \ $P_X$, \ la funci\'on  \ $G_X$ \
caracteriza completamente el vector aleatorio \ $X$.

\

Aparece  que la  funci\'on generadora  \ $G_X$  \ se  vincula tambi\'en  con los
momentos factoriales, justificando su secunda denominaci\'on, {\em generadora de
  momentos factoriales}  (fmgf para {\em factorial  moments generating function}
en ingles):
%
\begin{lema}
\label{Lem:MP:GeneracionMomentosFactoriales}
%
  Para cualquier \  $k = \begin{bmatrix} k_1 & \cdots  & k_d \end{bmatrix}^t \in
  \Nset^d$ \ con \ $K =  \sum_{i=1}^d k_i$, derivando $G_X$ se proba que, cuando
  existen~\footnote{En el  caso extremo,  el rayo de  convergencia de  la serie
    dando $G_X$ es igual a 1, as\'i  que no hay garantia que las derivadas en $z
    = 1$ existen.}
  %
  \[
  \left.\frac{\partial^K     G_X}{\partial     z_1^{k_1}     \cdots     \partial
      z_d^{k_d}}\right|_{z=1}    =   \Esp\left[    \prod_{i=1}^d    \left(   X_i
    \right)_{k_i} \right]
  \]
  %
  momento factorial~\footnote{Recuerdense que  $(x)_n = \prod_{i=0}^{n-1} (x-i),
    \quad n > 0$ \ s\'imbolo de Pochhammer, con la convenci\'on $(x)_0 = 1$; ver
    pagina~\pageref{Foot:MP:Pochhammer}} de $X$.
\end{lema}

De  este resultado,  se ve  por ejemplo  que, cuando  existen, se  recuperan los
momentos de $X$ a trav\'es de las derivadas de $G_X$:
%
\begin{itemize}
\item $G_X(1) = 1$, condici\'on de normalizaci\'on.
%
\item $\nabla_z G_X(1) = \Esp[X]$.
% \ donde $\nabla_z$ indica el gradiente, \ie el
%  vector de componente $i$-esima \ $\frac{\partial}{\partial z_i}$.
%
\item $\Hess_z G_X(1)  + \diag\left( \nabla_z G_X(1) \right)  = \Esp\left[ X X^t
  \right]$  \ donde  \ $\Hess_z$  \ es  la matrice  Hessiana
%, \ie  la  matriz de
%  componente $(i,j)$-esima \ $\frac{\partial^2}{\partial z_i \partial z_j}$, 
y \
  $\diag(a)$ es  une matriz  diagonal de componentes  \ $(i,i)$-esima \  $a_i$ \
  (vector $a$ sobre la diagonal).  Entonces  la matriz de covarianza es dada por
  \ $\Cov[X] =  \Hess_z G_X(1) + \diag\left( \nabla_z  G_X(1) \right) - \nabla_z
  G_X(1) \nabla_z^t G_X(1)$.
\end{itemize}

\

La funci\'on \ $G_X$ \ tiene unas propiedades permitiendo por ejemplo de manejar
sencillamente  distribuciones  de probabilidades  de  combinaciones lineales  de
vectores aleatorios independientes,  como lo vamos a ver  a trav\'es del teorema
siguiente.

\begin{teorema}%[Funci\'on generadora de probabilidad de una ]
\label{Teo:MP:PropiedadesGeneradoraProbabilidad}
%
  Sean  \   $X$  \  e  \   $Y$  \  dos   vectores  aleatorios  $d$-dimensionales
  independientes, $a  = \begin{bmatrix} a_1  & \cdots & a_d  \end{bmatrix}^t \in
  \Rset^d$ \  y \ $b  = \begin{bmatrix} b_1  & \cdots & b_d  \end{bmatrix}^t \in
  \Rset^d$.  Entonces  para  cualquier  $z  = \begin{bmatrix}  z_1  &  \cdots  &
    z_d \end{bmatrix}\in \Cset^d$ \ (donde existen las funciones):
  %
  \[
  G_{\diag(a) X + b}(z) =  \prod_{i=1}^d z_i^{b_i} G_X\left( z_1^{a_1} , \ldots ,
    z_d^{a_d} \right),
  \]
  %
  \[
  G_{X+Y}(z) = G_X(z) \, G_Y(z)
  \]
  %
  y para $z \in \Cset$
  %
  \[
  G_X\left( z^{a_1} , \ldots , z^{a_d} \right) = G_{a^t X}(z)
  \]
\end{teorema}
%
\begin{proof}
  El  primer  resultado es  inmediato,  escribiendo \  $z_i^{a_i  X_i  + b_i}  =
  z_i^{b_i}   \left(  z_i^{a_i}   \right)^{X_i}$.    El  secundo   viene  de   \
  $z_i^{X_i+Y_i}    =    z_i^{X_i}    z^{Y_i}$    \   conjuntamente    con    el
  teorema~\ref{Teo:MP:IndependenciaMomentos}   con   \   $f(X)  =   \prod_{i=1}^d
  z_i^{X_i}$  \ y \  $g(Y) =  \prod_{i=1}^d z_i^{Y_i}$.  El tercer  resultado es
  consecuencia de $\prod_{i=1}^d  \left( z^{a_i} \right)^{X_i} = z^{\sum_{i=1}^d
    a_i X_i}$.
\end{proof}
%
Estos  resultados permiten manejar  sencillamente la  medida de  probabilidad de
combinaciones lineales  de vectores aleatorios independientes y  de marginales a
trav\'es esta funci\'on generadora.

\

De  la  tercera identidad,  se  puede  hacer un  paso  m\'as  tratando de  sumas
aleatorias de vectores aleatorios:
%
\begin{teorema}
\label{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}
%
  Sea  \ $X_n,  \quad n  \in  \Nset$ \,  una sucesi\'on  de vectores  aleatorios
  indepedientes de  misma distribuci\'on  (resp.  generadora de  probabilidad) \
  $P_X$ \  (resp. $G_X$)  \ y  \ $N$ \  una variable  definida sobre  \ $\Nset$,
  independiente de los  \ $X_n$. Sea el vector aleatorio \  $ S_N = \sum_{n=0}^N
  X_n$. Entonces
  %
  \[
  G_{S_N}(z) =  G_N \big( G_X(z) \big),
  \]
  %
\end{teorema}
%
\begin{proof}
  Usando la formula de esperanza total del teorema.~\ref{Teo:MP:EsperanzaTotal}, se escribe
  %
  \begin{eqnarray*}
  G_{S_N}(z) & = & \Esp\left[ z^{\sum_{n=0}^N X_n} \right]\\[2.5mm]
  %
  & = & \Esp\left[ \Esp\left[ \left. z^{\sum_{n=0}^N X_n} \right| N  \right] \right]\\[2.5mm]
  %
  & = & \Esp\left[ G_X(z)^N \right]
  \end{eqnarray*}
\end{proof}


% ================================= Funcion generadora de momentos

\subseccion{Funci\'on generadora de momentos}
\label{Ssec:MP:GeneradoraMomentos}


Como lo hemos visto, la  funci\'on generadora de probabilidad permite recucperar
los  momentos  de  un  vector  aleatorio  a trav\'es  de  combinaciones  de  sus
derivadas.   Con una pequa\~na  modificaci\'on, se  puede definir  una funci\'on
generada  permitiendo  recuperar  m\'as  directamente los  momentos,  de  manera
siguiente~\cite{Fel68, JohKot97, Muk00, AthLah06}:
%
\begin{definicion}[funci\'on generadora de momentos]
\label{Def:MP:GeneradoraMomentos}
%
  La {\em  funci\'on generadora  de momentos} (mgf  para {\em  moment generating
    function} en ingles) de un vector aleatorio $d$-dimensional se define como
  %
  \[
  M_X(u) = \Esp\left[ e^{z^t X} \right]
  \]
  %
  para $u \in \Cset^d$.
\end{definicion}
%
De esta definici\'on se nota inmediatamente que
%
\[
M_X(u) = G_X\left( e^u \right)  \qquad \mbox{donde} \qquad e^u = \begin{bmatrix}
  e^{u_1} & \cdots & e^{u_d} \end{bmatrix}^t
\]
%
Entonces, como \ $G_X$, la  generadora de los momentos caracteriza completamente
el vector  aleatorio \ $X$.   Adem\'as, de  este v\'inculo entre  \ $G_X$ \  y \
$M_X$, y del  dominio de definici\'on de \  $G_X$, queda claro que \  $M_X$ \ es
definida sobre un  producto cartesiano de franjas del plano  complejo, $ v_i \le
\real{u_i} \le  V_i$ \ donde \  $v_i \le 0 \le  V_i$ \ llamamos  {\em indices de
  convergencia}.  En el  caso de variables escalares admitiendo  una densidad de
probabildad \  $p_X$, denotando $s =  -u$, esta funci\'on se  interpreta como la
transformada (bilateral) de Laplace de \ $p_X$.


La  generadora de  los momentos  permite recuperar  directamente los  momentos a
trav\'es de derivadas, sin hacer combinaciones:
%
\begin{lema}
\label{Lem:MP:GenracionMomentos}
%
  Para cualquier \  $k = \begin{bmatrix} k_1 & \cdots  & k_d \end{bmatrix}^t \in
  \Nset^d$ \ con \ $K =  \sum_{i=1}^d k_i$, derivando $M_X$ se proba que, cuando
  existen
  %
  \[
  \left.\frac{\partial^K     M_X}{\partial     u_1^{k_1}     \cdots     \partial
      u_d^{k_d}}\right|_{u=0}  = \Esp\left[  \prod_{i=1}^d  X_i^{k_i} \right]  =
  m_{k_1,\ldots,k_d}
  \]
  %
  momento de orden \ $k$ \ de \ $X$.
\end{lema}
%
En particular, se recuperan
%
\begin{itemize}
\item $M_X(0) = 1$, condici\'on de normalizaci\'on.
%
\item $\nabla_u M_X(0) = \Esp[X]$ \ promedio,
%
\item $\Hess_u M_X(0) = \Esp\left[ X X^t \right]$, \ie $\Cov[X] = \Hess_u M_X(0)
  - \nabla_u M_X(0) \nabla_u^t M_X(0)$ \ matriz de covarianza.
\end{itemize}

Como la funci\'on \ $G_X$, la  generadora de los momentos tiene unas propiedades
similares  a las  de  los teoremas~\ref{Teo:MP:PropiedadesGeneradoraProbabilidad}
y~\ref{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}:
%
\begin{teorema}%[Funci\'on generadora de probabilidad de una ]
\label{Teo:MP:PropiedadesGeneradoraMomentos}
%
  Sean  \   $X$  \  e  \   $Y$  \  dos   vectores  aleatorios  $d$-dimensionales
  independientes,  \ $A$  \ una  matriz de  \  $\Rset^{d' \times  d}$ \  y \  $b
  =   \begin{bmatrix}    b_1   &    \cdots   &   b_{d'}    \end{bmatrix}^t   \in
  \Rset^{d'}$.  Entonces para  cualquier $u  =  \begin{bmatrix} u_1  & \cdots  &
    u_{d'} \end{bmatrix}^t \in \Cset^{d'}$ \ (donde la funci\'on existe):
  %
  \[
  M_{A X + b}(u) =  e^{u^t b} M_X\left( A^t u \right),
  \]
  %
  y para cualquier  $u = \begin{bmatrix} u_1 & \cdots  & u_d \end{bmatrix}^t \in
  \Cset^d$ \ (donde la funci\'on existe):
  %
  \[
  M_{X+Y}(u) = M_X(u) \, M_Y(u)
  \]
  %
  Adem\'as,  para \  $X_n, \quad  n  \in \Nset$  \, una  sucesi\'on de  vectores
  aleatorios  indepedientes  de   misma  distribuci\'on  (resp.   generadora  de
  momentos) \ $P_X$ \ (resp. $M_X$) \  y \ $N$ \ una variable aleatoria definida
  sobre \ $\Nset$, independiente de los \ $X_n$, y \ $ S_N = \sum_{n=0}^N X_n$,
  %
  \[
  M_{S_N}(u) =  G_N \big( M_X(u) \big),
  \]
\end{teorema}
%
\begin{proof}
  Las  pruebas  siguen   punto  a  punto  los  mismos  pasos   que  las  de  los
  teoremas~\ref{Teo:MP:PropiedadesGeneradoraProbabilidad}
  y~\ref{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}.
\end{proof}

\

De nuevo, se puede hacer un  paso m\'as tratando de sumas aleatorias de vectores
aleatorios como en el teorema~\ref{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}:
%
\begin{teorema}
\label{Teo:MP:SumaAleatoriaGeneradoraMomentos}
%
  Sea  \ $X_n,  \quad n  \in  \Nset$ \,  una sucesi\'on  de vectores  aleatorios
  indepedientes de  misma distribuci\'on  (resp.  generadora de  probabilidad) \
  $P_X$ \  (resp. $M_X$) \  e \  $N$ \ una  variable aleatoria definida  sobre \
  $\Nset$,  independiente de los  \ $X_n$.  Sea el  vector aleatorio  \ $  S_N =
  \sum_{n=0}^N X_n$. Entonces
  %
  \[
  M_{S_N}(u) =  G_N \big( M_X(u) \big),
  \]
  %
\end{teorema}
%
\begin{proof}
  El         resultado        es         consecuencia         directa        del
  teorema~\ref{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}.
\end{proof}
%\cite{Fra09}%%p.73


% ================================= Funcion caracteristica

\subseccion{Funci\'on caracter\'istica}
\label{Ssec:MP:FuncionCaracteristica}

Si  la funci\'on generadora  de momentos  permite recuperar  los momentos  de un
vector  aleatorio, no  es definida  sobre todo  $\Cset^d$.  Sin  embargo, cuando
$\real{u_i} = 0$,  esta funci\'on es siempre definida.   Entonces, una funci\'on
generadora muy \'util que se usa frecuentemente es la de momentos para este tipo
de argumentos,  lo que es conocida  como funci\'on caracter\'istica y  que es al
final definida  sobre $\Rset^d$  de manera siguiente~\cite{Luk61,  Gol61, Fel68,
  SteWei71, JohKot97, Muk00, AthLah06, Sas13}:


\begin{definicion}[funci\'on   caracter\'istica]
\label{Def:MP:FuncionCaracteristica}
% 
  La {\em funci\'on caracter\'istica}  (cf para {\em characteristic function} en
  ingles) de un vector aleatorio $d$-dimensional se define como
 %
  \[
  \Phi_X(\omega) = \Esp\left[ e^{\imath \omega^t X} \right]
  \]
  %
  para $\omega \in \Rset^d$.
\end{definicion}
%
De esta definici\'on se nota inmediatamente que
%
\[
\Phi_X(\omega) = M_X(\imath \omega) = G_X\left( e^{\imath \omega} \right) \qquad
\mbox{donde} \qquad e^{\imath \omega}  = \begin{bmatrix} e^{\imath u_1} & \cdots
  & e^{\imath u_d} \end{bmatrix}^t
\]
%
De hecho,  se puede definir esta  funci\'on para un argumento  complejo, pero es
equivalente a volver a la definici\'on de la generadora de momentos.

En su forma general, la funci\'on caracter\'istica se escribe
%
\[
\Phi_X(\omega) = \int_{\Rset^d} e^{\imath \omega^t x} \, dP_X(x)
\]
%
y  es  relacionada   a  la  transformada  de  Fourier-Stieltjes   de  la  medida
$P_X$~\cite[Chap.~5]{Pin09}. Cuando  \ $P_X$ \  admite una densidad \  $p_X$, la
funci\'on  es  una  transformada  de  Fourier  usual de  la  densidad  \  $p_X$,
introducida bajo el  impulso de Fourier en 1822 para  estudiar la difusi\'on del
calor~\cite{Fou22}.

Insistamos sobre  el hecho que  la importancia de  esta funci\'on reside  en que
siempre existe y est\'a bien  definida, dado que \ $\displaystyle \int_{\Rset^d}
\left| e^{\imath \omega^t x} \right| \, dP_X(x) = \int_{\Rset^d} dP_X(x) = 1$.
% \cite{Gol61}

Como para las generadoras ya introducidas, la funci\'on caracter\'istica permite
recuperar directamente los momentos a trav\'es de derivadas:
%
\begin{lema}
\label{Lem:MP:GeneracionMomentoViaCaracteristica}
%
  Para cualquier \  $k = \begin{bmatrix} k_1 & \cdots  & k_d \end{bmatrix}^t \in
  \Nset^d$ \  con \  $K =  \sum_{i=1}^d k_i$, derivando  $\Phi_X$ se  proba que,
  cuando existen
  %
  \[
  (-   \imath)^K  \,   \left.\frac{\partial^K   \Phi_X}{\partial  \omega_1^{k_1}
      \cdots    \partial    \omega_d^{k_d}}\right|_{\omega=0}    =    \Esp\left[
    \prod_{i=1}^d X_i^{k_i} \right] = m_{k_1,\ldots,k_d}
  \]
  %
  momento de orden \ $k$ \ de \ $X$.
\end{lema}
%
En particular, se recuperan
%
\begin{itemize}
\item $\Phi_X(0) = 1$, condici\'on de normalizaci\'on.
%
\item $- \imath \nabla_\omega M_X(0) = \Esp[X]$ \ promedio,
%
\item  $- \Hess_\omega  M_X(0) =  \Esp\left[ X  X^t \right]$,  \ie $\Cov[X]  = -
  \Hess_\omega M_X(0) + \nabla_\omega M_X(0) \nabla_\omega^t M_X(0)$ \ matriz de
  covarianza.
\end{itemize}

Fijense de  que \  $\Phi_X$ \  no es siempre  diferencial en  $\omega =  0$; Por
ejemplo,    en   el    caso    de   la    distribuci\'on   de    Cauchy--Lorentz
univariada~\footnote{Lo      mismo       occure      en      la      extensi\'on
  multivariada~\cite{SamTaq91}.}\ $p_X(x) =  \frac{\gamma}{\pi \left( \gamma^2 +
    (x-x_0)^2  \right)}$ \  con  \ $\gamma  >  0$, resulta  \ $\Phi_X(\omega)  =
e^{-\imath x_0  \omega -\gamma |\omega}  $. Esta funci\'on est\'a  definida para
todo $\omega$,  pero no es  derivable en  $\omega = 0$,  lo que coincide  con el
hecho  de  que  no  est\'an   definidos  los  momentos  para  esta  densidad  de
probabilidad.

Resumimos algunas otras propiedades importantes de la funci\'on caracter\'istica:
%
\modif{
\begin{teorema}[Propiedades principales de la funci\'on caracter\'istica]
\label{Teo:MP:PropCarac}
\mbox{ }

\begin{enumerate}
\item\label{Prop:MP:ContinuidadPhiX}  $\Phi_X$   es  una  funci\'on   medible  y
  continua en $\Rset^d$~\cite[Prop.~5.2.1]{Pin09}.   Eso es una consecuencia del
  teorema           de            convergencia           dominada           (ver
  teorema~\ref{Teo:MP:ConvergenciaDominada}
  pagina~\pageref{Teo:MP:ConvergenciaDominada}).
%
\item\label{Prop:MP:PhiXZero} $\Phi_X(0) = 1  $: Eso es inmediato escribiendo la
  integral, siendo $P_X$ una medida de probabilidad.
%
\item\label{Prop:MP:MaximoPhiX}   $\left|  \Phi_X(\omega)   \right|   \le  1   =
  \Phi_X(0)$: \  $\left| \Phi_X(\omega)  \right|$ es m\'axima  en $\omega  = 0$.
  Eso viene directamente de $\left| e^{\imath \omega^t x} \right| = 1$.
%
\item\label{Prop:MP:HermiticaPhiX}    $\Phi_X(-\omega)    =   \Phi_X^*(\omega)$:
  $\Phi_X$ tiene una s\'imetria hermitica.
%
\item  $\Phi_X$ es  una no negativa  definida, \ie  para  un conjunto
  arbitrario de \ $n  \ge 1$ \ n\'umeros complejos \ $a_1 , \ldots  , a_n$ \ y \
  $n$ \ vectores \ $w_1 , \ldots , w_n$ \ de \ $\Rset^d$, se cumple
  \[
  \sum_{k,l=1}^n a_k^* a_l \Phi_X(w_l-w_k) \ge 0
  \]
  %
  Dicho   de   otra   manera,   la   matriz  de   componente   \   $(k,l)$-esima
  $\Phi_X(w_l-w_k)$ es a hermitica (s\'imetria herm\'itica dada por la propiedad
  anterior, y  no negativa  definida). Esta positividad  viene de  \ $\sum_{k,l}
  a_k^*  a_l e^{\imath (w_l-w_k)^t  x} =  \left| \sum_l  a_l e^{\imath  w_l^t x}
  \right|^2 \ge 0$.
%
\end{enumerate}
\end{teorema}

De hecho, existe una reciproca de este teorema, debido a S. Bochner~\footnote{De
  hecho, lo prob\'o Bochner en el caso escalar $d = 1$, pero se extiende al caso
  multivariado.}~\cite{Boc32, Boc59, Gol61, Pin09, Sas13}
%
\begin{teorema}[Bochner]\label{Teo:MP:Bochner}
%
  Una funci\'on \ $\Phi: \Rset^d \mapsto \Cset$ \ es continua, definida no negativa
  (con  \ $\Phi(0)  =  1$)  si y  solamente  existe una  medida  \  $\mu$ \  (de
  probabilidad) sobre \ $\B(\Rset^d)$ \ tal que
  %
  \[
  \forall \:  \omega \in \Rset^d, \quad \Phi(\omega)  = \int_{\Rset^d} e^{\imath
    \, \omega^t x} d\mu(x)
  \]
  %
  Dicho de  otra manera, cualquier  funci\'on continua, definida positiva  con \
  $\Phi(0) = 1$ \ es la funci\'on caracter\'istica de un vector aleatorio.
\end{teorema}
%
%\begin{proof}
En el  teorema, vimos que la  transformada de Fourier-Stieljes de  una medida de
probabilidad $P_X$  es medible, continua,  definida no negativa, con  $\Phi(0) =
1$. La  reciproca es  m\'as dificil  a probar y  necesita lemas  adicionales. Se
puede  encontrar una  linda prueba  en~\cite[Sec.~1.7]{Sas13} adonde  dejamor el
lector.
%\end{proof}

Como   lo  hemos   notado   en  las   subsecciones   anteriores,  la   funci\'on
caracter\'istica define  completamente el  vector aleatorio. En  particular, hay
una relaci\'on uno-uno (casi siempre) entre \ $\Phi_X$ \ y la medida \ $P_X$. En
particular, existe una  f\'ormula de inversion permitiendo volver  a la medida \
$P_X$ \ a partir de \ $\Phi_X$~\cite{Sas13}:
%
\begin{teorema}[Inversi\'on f\'ormula]\label{Teo:MP:InversionFourierStieljes}
%
  Sea \ $X$  \ vector aleatorio $d$-dimensional de  funci\'on caracter\'istica \
  $\Phi_X$.   Sea  \  $\displaystyle  A  = \optimes_{i=1}^d  (a_i  \;  b_i)  \in
  \B(\Rset^d)$ \ y \ $\partial A  = \optimes_{i=1}^d [a_i \; b_i] \setminus A$ \
  su borde. Entonces~\footnote{Se prolonga  la funci\'on \ $\frac{e^{- \imath \,
        a_j \omega_j} - e^{- \imath  \, b_j \omega_j}}{\imath \, \omega_j}$ \ en
    \ $\omega_j  = 0$ \ por  su l\'imite \ $\displaystyle  \lim_{\omega_j \to 0}
    \frac{e^{- \imath \, a_j \omega_j} - e^{- \imath \, b_j \omega_j}}{\imath \,
      \omega_j} = b_j - a_j$.},
  %
  \[
  \begin{array}{c}
  %
  \displaystyle \lim_{w_1 \to +\infty,\ldots, w_d \to \infty} \: \frac{1}{(2
  \pi)^d} \int_{\optimes_{j=1}^d [-w_j \; w_j]} \Phi_X(\omega) \prod_{j = 1}^d
  \frac{e^{- \imath \, a_j \omega_j} - e^{- \imath \, b_j \omega_j}}{\imath \,
  \omega_j} \: d\omega\\[2.5mm]
  %
  =\\[2.5mm]
  %
   \displaystyle \int_{\Rset^d} \prod_{j=1}^d \left(
     \un_{\left(  a_j \;  b_j \right)}(x_j)  +  \frac12 \un_{\left\{  a_j \;  b_j
       \right\}}(x_j) \right) dP_X(x)
  \end{array}
  \]
  %
  En particular, cuando $P_X$ vale 0  sobre el borde de $A$, es decir $P_X\left(
  \partial A \right) = 0$, se obtiene
  %

  \[
  \lim_{w_1  \to   +\infty,\ldots,  w_d  \to  \infty}   \:  \frac{1}{(2  \pi)^d}
  \int_{\optimes_{j=1}^d   [-w_j  \;  w_j]}   \Phi_X(\omega)  \prod_{j   =  1}^d
  \frac{e^{- \imath \,  a_j \omega_j} - e^{- \imath  \, b_j \omega_j}}{\imath \,
    \omega_j} \, d\omega \: = \: P_X(A).
  \]
\end{teorema}
%
Nota: el l\'imite \ $\displaystyle\lim_{T  \to +\infty} \int_{-T}^T$ \ se nota a
veces \ $\displaystyle \operatorname{vp} \!  \int_\Rset$, integral {\it en valor
  principal}.
%
\begin{proof}
  Por definici\'on de la funci\'on caracter\'istica, tenemos
  %
  \[
  \int_{\optimes_{j=1}^d   [-w_j  \;  w_j]} \Phi_X(\omega)  \prod_{j = 1}^d  \frac{e^{- \imath  \, a_j
      \omega_j} - e^{- \imath \, b_j \omega_j}}{\imath \, \omega_j} \, d\omega =
  \int_{\optimes_{j=1}^d   [-w_j  \;  w_j]} \int_{\Rset^d} e^{\imath  \omega^t x} \, dP_X(x) \prod_{j =
    1}^d  \frac{e^{- \imath a_j  \omega_j} -  e^{- \imath  b_j \omega_j}}{\imath
    \omega_j} \, d\omega
  \]
  %
  Ahora, notando que $\left| \frac{e^{- \imath \, a_j \omega_j} - e^{- \imath \,
        b_j \omega_j}}{\imath  \, \omega_j} \, e^{\imath \,  \omega^t x} \right|
  \le  b_j - a_j$  \ es  uniformamente acotado,  se puede  evocar el  teorema de
  Fubini~\ref{Teo:MP:Fubini} para intercambiar las  integrales, as\'i que, con \
  $e^{\imath \, \omega^t x} = \prod_{j=1}^d e^{\imath \, \omega_j x_j}$, tenemos
  %
  \[
  \int_{\optimes_{j=1}^d   [-w_j  \;  w_j]}   \Phi_X(\omega)  \prod_{j   =  1}^d
  \frac{e^{- \imath \,  a_j \omega_j} - e^{- \imath  \, b_j \omega_j}}{\imath \,
    \omega_j} \, d\omega = \int_{\Rset^d} \left( \prod_{j=1}^d \int_{-w_j}^{w_j}
    \frac{e^{- \imath \, \omega_j (a_j -  x_j)} - e^{- \imath \, \omega_j (b_j -
        x_j)}}{\imath \, \omega_j} \, d\omega_j \right) \, dP_X(x)
  \]
  %
  Se nota que
  %
  \begin{eqnarray*}
  \int_{-w_j}^{w_j} \frac{e^{- \imath \, \omega_j (a_j - x_j)} - e^{- \imath \, \omega_j
  (b_j - x_j)}}{\imath \, \omega_j} \, d\omega_j & = & - \int_{-w_j}^{w_j} \frac{e^{+
  \imath \, \omega_j (a_j - x_j)} - e^{+ \imath \, \omega_j (b_j - x_j)}}{\imath \,
  \omega_j} \, d\omega_j\\[2mm]
  %
  & = & \int_{-w_j}^{w_j} \frac{\sin(\omega_j (b_j - x_j)) - \sin(\omega_j (a_j -
  x_j))}{\omega_j} \, d\omega_j
  \end{eqnarray*}
  %
  por cambio de variables $\omega_j \to - \omega_j$ en la primera linea, tomando
  entonce  la media  suma de  los  terminos derecho/izquierdo  dando la  secunda
  linea. Seguimos notando que
  %
  \[
  \int_{-w}^w  \frac{\sin(\omega (c  - x))}{\omega}  \, d\omega  = \sign(c  - x)
  \int_{-w |c - x|}{w |c - x|} \frac{\sin(\omega)}{\omega} \, d\omega
  \]
  %
  es  decir, de  \ $\displaystyle  \lim_{T \to  +\infty}  \int_{-T}^T \frac{\sin
    \omega}{\omega} d\omega = \pi$~\cite[Ec.~3.721]{GraRyz15}, se obtiene

  %
  \[
  \lim_{w \to  +\infty} \int_{-w}^w \frac{\sin(\omega  (c - x))}{\omega}
  \, d\omega = \pi \sign(c-x)
  \]
  %
  Se  acaba  la   prueba  de  \  $\left|  \frac{\sin(\omega_j   (b_j  -  x_j)  -
      \sin(\omega_j  (a_j -  x_j))}{\omega_j} \right|  < 2$  \  conjuntamente al
  teorema de convergencia dominada~\ref{Teo:MP:ConvergenciaDominada} permitiendo
  permutar integral  y l\'imite,  y de $\sign(b_j-x_i)  - \sign(a_j-x_i) =  2 \,
  \un_{\left(   a_j   \;  b_j   \right)}(x_j)   +   \un_{\left\{   a_j  \;   b_j
    \right\}}(x_j)$.
\end{proof}

Dos teoremas de inversi\'on en los casos particular continuo y discreto permiten
respectivamente  volver  a   la  densidad  de  probabilidad  o   a  la  masa  de
probabilidad.
%
\begin{teorema}[Inversi\'on, caso continuo]\label{Teo:MP:InversionDensidad}
%
  Si \ $\Phi_X$ \ es integrable, entonces \ $P_X$ \ admite una densidad tal que
  %
  \[
  p_X(x)  =  \frac{1}{(2  \pi)^d}  \int_{\Rset^d} \Phi(\omega) \, e^{-  \imath  \,
    \omega^t x} \, d\omega
\]
\end{teorema}
%
\begin{proof}
\SZ{hacer o mandar a libros Sasvari p. 21}
\end{proof}

\begin{teorema}[Inversi\'on, caso discreto]\label{Teo:MP:InversionMasa}
%
  Para cualquier $x \in \Rset^d$,
  %
  \[
  \lim_{w_1  \to  \infty,\ldots,w_d \to  \infty} \,  \frac{1}{2^d  w_1 \ldots  w_d}
  \int_{\optimes_{j=1}^d [-w_j \; w_j]}  \Phi(\omega) \, e^{- \imath \, \omega^t x}
  \, d\omega = P_X(x)
  \]
\end{teorema}
%
\begin{proof}
  Por definici\'on de  la funci\'on caracter\'istica, y aplicando  el teorema de
  Fubini como en el caso general (mismo enfoque),
  %
  \begin{eqnarray*}
  \frac{1}{2^d w_1 \ldots w_d} \int_{\optimes_{j=1}^d [-w_j \; w_j]}
  \Phi_X(\omega) e^{- \imath \omega^t x} \, d\omega & = & \frac{1}{2^d w_1 \ldots
  w_d} \int_{\optimes_{j=1}^d [-w_j \; w_j]} \int_{\Rset^d} e^{\imath \omega^t y}
  \, dP_X(y) \, e^{- \imath \omega^t x} d\omega\\[2mm]
  %
  & = & \int_{\Rset^d} \left( \prod_{j=1}^d \frac{1}{2 w_j} \int_{-w_j}^{w_j}
  e^{\imath \, (y_j - x_j) w_j} dw_j \right) dP_X(y)\\[2mm]
  %
  & = & \int_{\Rset^d} \prod_{j=1}^d \frac{\sin(w_j (y_j-x_j))}{(y_j-x_j) w_j}  dP_X(y)
  \end{eqnarray*}
  %
  con   el  l\'imite   \  $\displaystyle   \lim_{y_j  \to   x_j}  \frac{\sin(w_j
    (y_j-x_j))}{w_j( y_j-x_j)}  = 1$. Ad\'emas, con  el mismo enfoque  que en el
  cas general acotando  el integrande, por el teorema  de convergencia dominada,
  se  puede intercambiar  l\'imite  e integral,  as\'i  que, por  $\displaystyle
  \lim_{x_j   \to   \infty}    \frac{\sin(w_j   (y_j-x_j))}{w_j(   y_j-x_j)}   =
  \un_{x_j}(y_j)$, lo cierra la prueba
\end{proof}

Como las  funci\'ones \ $G_X$ \  y \ $M_X$, la  funci\'on caracter\'istica tiene
entre    otros    propiedades    similares     a    las    a    las    de    los
teoremas~\ref{Teo:MP:PropiedadesGeneradoraMomentos}
y~\ref{Teo:MP:SumaAleatoriaGeneradoraMomentos}:
%
\begin{teorema}%[Funci\'on generadora de probabilidad de una ]
\label{Teo:MP:PropiedadesFuncionCaracteristica}
%
  Sean  \   $X$  \  e  \   $Y$  \  dos   vectores  aleatorios  $d$-dimensionales
  independientes,  \ $A$  \ una  matriz de  \  $\Rset^{d' \times  d}$ \  y \  $b
  =  \begin{bmatrix} b_1  &  \cdots &  b_{d'}  \end{bmatrix}^t \in  \Rset^{d'}$.
  Entonces  para  cualquier  $\omega  =  \begin{bmatrix}  \omega_1  &  \cdots  &
    \omega_{d'} \end{bmatrix}^t \in \Rset^{d'}$:
  %
  \[
  \Phi_{A X + b}(\omega) =  e^{\imath \omega^t b} \Phi_X\left( A^t \omega \right),
  \]
  %
  y   para   cualquier  $\omega   =   \begin{bmatrix}   \omega_1   &  \cdots   &
    \omega_d \end{bmatrix}^t \in \Rset^d$:
  %
  \[
  \Phi_{X+Y}(\omega) = \Phi_X(\omega) \, \Phi_Y(\omega)
  \]
  %
  Adem\'as,  para \  $X_n, \quad  n  \in \Nset$  \, una  sucesi\'on de  vectores
  aleatorios  indepedientes  de   misma  distribuci\'on  (resp.   generadora  de
  momentos) \ $P_X$ \ (resp. $M_X$) \  e \ $N$ \ una variable aleatoria definida
  sobre \ $\Nset$, independiente de los \ $X_n$, y \ $ S_N = \sum_{n=0}^N X_n$,
  %
  \[
  \Phi_{S_N}(\omega) =  G_N \big( \Phi_X(\omega) \big),
  \]
\end{teorema}

Un otro resultado  interesante se v\'incula a la noci\'on de  mezcla de escala y
toma la forma siguiente:
%
\begin{teorema}
  Sea \  $X$ \ vector aleatorio de  funci\'on caracter\'istica \ $\Phi_X$  \ y \
  $R$ \ variable aleatoria independiente de  \ $X$ \ y de medida de probabilidad
  \ $P_R$. Entonces, la funci\'on caracter\'istica de \ $R X$ \ es dada por
  %
  \[
  \Phi_{R X}(\omega) = \int_\Rset \Phi_X(r \omega) \, dP_R(r)
  \]
\end{teorema}
%
\begin{proof}
  Por definici\'on  de la funci\'on caracter\'istica, la  f\'ormula de esperanza
  total~\ref{Teo:MP:EsperanzaTotal}                     y                     el
  teorema~\ref{Teo:MP:EsperanzaCondicionalFXY}, se tiene
  %
  \begin{eqnarray*}
  \Phi_{R X}(\omega) & = & \Esp\left[ e^{\imath \, \omega^t R X} \right]\\[2mm]
  %
  & = & \Esp\left[ \Esp\left[ \left. e^{\imath \, \omega^t R X} \right| R \right] \right]\\[2mm]
  %
  & = & \int_\Rset \Esp\left[ \left. e^{\imath \, \omega^t r X} \right| R=r \right] dP_R(r)\\[2mm]
  %
  & = & \int_\Rset \Esp\left[ e^{\imath \, \omega^t r X} \right] dP_R(r)\\[2mm]
  %
  & = & \int_\Rset \Phi_X(r \omega) \, dP_R(r)
  \end{eqnarray*}
\end{proof}

 }

%\SZ{Hablar de la forma en el caso escamar con le funcion quantile?}

\aver{

%{\teorema (Bochner, Goldberg).... } %%


%\hfill

Cumulant generating function .... %%

\SZ{Se define los cumulantes. coinciden con los momentos centrados solo para $k = 1, 2, 3$.}

\hfill

}



\SZ{hablar de la cota de Chernoff con la mgf o pgf?}

\SZ{hablar del CLT y prueba}
