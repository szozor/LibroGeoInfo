\seccion{Funciones generadoras}
\label{Sec:MP:generatrices}

Como lo hemos visto, un vector aleatorio es completamente definida por su medida
de probabilidad $P$, o equivalemente por la medida imagen $P_X$, o a trav\'es de
la funci\'on de repartici\'on $F_X$. Sin  embargo, bajo el impulso de Laplace en
el  siglo XVII  (entre  otros), se  introdujo  caracterizaciones alternativas  a
trav\'es de transformaciones  de la medida de probabilidad,  conocidas como {\em
  funciones generadoras}  o {\em funciones  generatrices}~\footnote{De hecho, de
  manera general,  se introdujeron  tales funciones en  un marco  m\'as general,
  asociado   a  sucesiones   de   n\'umeros,   bajo  el   impulso   de  A.    de
  Moivre~\cite{Moi30};   ver   tambi\'en~\cite{Sti30,   Eul41,   Eul50,   Moi56}
  o~\cite[Sec.~1.2.9]{Knu97_v1}.         \label{Foot:MP:Generadora}}~\cite{Lap12,
  Lap14,   Lap20}.    Existen  varias   funciones,   cuyas  tienes   propiedades
particulares que vamos a ver  en las subsecciones siguientes. Entre otros, estas
funciones  dadas como  valores  de  expectaci\'on de  funciones  de la  variable
aleatoria (discreta  o continua), con  un par\'ametro real o  complejo, permiten
hallar   f\'acilmente  los   distintos   momentos  de   una  distribuci\'on   de
probabilidad.


% ================================= Funcion generadora de probabilidad

\subseccion{Funci\'on generadora de probabilidad}
\label{Ssec:MP:GeneradoraProbabilidad}

De  manera  general,  siguiendo  el  enfoque  de A.   de  Moivre  (ver  nota  de
pie~\ref{Foot:MP:Generadora}) dada  una sucesi\'on \ $\{ a_n  \}_{n \in \Nset}$,
se define la funci\'on generadora dicha  {\em ordinaria} de la sucesi\'on como \
$G\left( \{ a_n \}_{n  \in \Nset} \, , \, z \right) =  \sum_{n \in \Nset} a_n \,
z^n$. A veces, esta serie es  conocida como transformada en $z$ de la sucesi\'on
\ $\{ a_n \}_{n \in \Nset}$.  Tratando de variables aleatorias discretas sobre \
$\Nset$, con \  $p_n = P_X(n) = P(X  = n)$, se puede definir  as\'i la funci\'on
generadora asociada a la sucesi\'on \ $p_n$  y se puede ver que no es nada m\'as
que  el momento  $\Esp\left[  z^X  \right]$.  De  manera  general, la  funci\'on
generadora  de  probabilidad  se  define  de  la  manera  siguiente~\cite{Fel68,
  JohKot97, Muk00, AthLah06}:
%
\begin{definicion}[funci\'on   generadora   de   probabilidad  o   de   momentos
  factoriales]
\label{Def:MP:GeneradoraProbabilidadFactorial}
%
  Sea $X = \begin{bmatrix} X_1  & \cdots & X_d \end{bmatrix}^t$ vector aleatorio
  $d$-dimensional  definido sobre $\X  \subset \Rset^d$.  La  funci\'on definida
  por
  %
  \[
  G_X(z) =  \Esp\left[ \prod_{i=1}^d  z_i^{X_i} \right]\quad \mbox{con}  \quad z
  = \begin{bmatrix} z_1 & \cdots & z_d \end{bmatrix}^t \in \Cset^d
  \]
  %
  es conocida como  {\em funci\'on generadora de probabilidad}  o {\em funci\'on
    generadora de momentos factoriales} de $X$.
\end{definicion}
%
Esta    funci\'on   est    definida    sobre   un    producto   cartesiano    de
anillos~\footnote{Para \ $i =  1, \ldots, d$, sean los reales \  $r_i, \: R_i$ \
  tales que  \ $0 \le r_i  \le R_i$. Consideramos  \ $j = \begin{bmatrix}  j_1 &
    \cdots & j_d  \end{bmatrix}^t \in \{ -1  \; +1 \}^d$ \ y  a continuaci\'on \
  $\D_j  =  \optimes_{i=1}^d  \left(  j_i  \Rset_+  \right)$  \  ($  -\Rset_+  =
  \Rset_-$),  \  los  \ $2^d$  \  hipercuadrantes  de  \ $\Rset^d$.   Notamos  \
  $\rho_{j_i} = r_i \un_{\{-1\}}(j_i)  + R_i \un_{\{+1\}}(j_i)$.  Supongamos que
  en   cada  hipercuadrante  \   $\displaystyle  \int_{\D_j}   \,  \prod_{i=1}^d
  \rho_{j_i}^{x_i} \,  dP_X(x) < +\infty$. \ Es  sencillo ver que \  para \ $x_i
  \in \left(  j_i \Rset_+ \right)$,  \ $r_i \le  |z_i| \le R_i  \:\: \Rightarrow
  \:\: |z_i|^{x_i} \le \rho_{j_i}^{x_i}$.  Entonces, por teorema de convergencia
  dominada  las   integrales  \  $\displaystyle   \int_{\D_j}  \,  \prod_{i=1}^d
  z_i^{x_i}  \,  dP_X(x)$ \  convergen,  y  por  consecuencia, \  $\displaystyle
  \int_{\Rset^d} \prod_{i=1}^d  z_i^{x_i} \, dP_X(x)$ \ converge.   Ahora, en el
  producto  cartesiano  de  los  circulos  unitarios \  $|z_i|=1$  \  tenemos  \
  $\displaystyle \left| \prod_{i=1}^d z_i^{x_i}  \right| = 1$, de integral sobre
  \ $\D_j$  \ convergente (vale  $P_X(\D_j) \le 1$).   De nuevo, por  teorema de
  convergencia dominada,  $\displaystyle \int_{\D_j} \prod_{i=1}^d  z_i^{x_i} \,
  dP_X(x)$ \ converge sobre el producto cartesiano de circulos unitarios, lo que
  asegura la existancia  de \ $R_i \ge 1$  \ y $r_i \le 1$ (un  anillo puede ser
  resctricto al circulo).
  % Si  para  \  $\left| z_i  \right|  \,  =  \,  R_i$  \ la  integral  converge
  % uniformamente,  para cualquier  \  $z \in  \Cset^d,  \: r_i  \le \left|  z_i
  % \right|   \le  R_i$   \  tenemos   por  ejemplo   \   $\displaystyle  \left|
  %   \int_{\Rset_+^d}   \prod_{i=1}^d   z_i^{x_i}   \,  dP_X(x)   \right|   \le
  % \int_{\Rset_+^d}  \left|  \prod_{i=1}^d  z_i^{x_i}  \right| \,  dP_X(x)  \le
  % \int_{\Rset_+^d}  \left|  \prod_{i=1}^d   R_i^{x_i}  \right|  \,  dP_X(x)  <
  % +\infty$.   El  mismo  enfoque  se  usa para  todos  los  hipercuadrantes  \
  % $\optimes_i \Rset_{\pm}$.  Adem\'as, claramente, para \  $\left| z_i \right|
  % \,  = \,  1$ \  tenemos $\displaystyle  \left|  \int_{\Rset^d} \prod_{i=1}^d
  %   z_i^{x_i}  \,  dP_X(x)  \right|  \le \int_{\Rset^d}  \left|  \prod_{i=1}^d
  %   z_i^{x_i}  \right| \,  dP_X(x) \le  \int_{\Rset^d}  dP_X(x) =  1$, lo  que
  % prueba que \ $r_i \le 1 \le R_i$.
\label{Foot:MP:GeneradoraProbabilidadExistencia}}
 en  el plano complejos,
$r_i \, \le \, \left|  z_i \right| \, \le \, R_i$ \ con \ $r_i  \le 1$ \ y \ $R_i
\ge 1$.

\

La denominaci\'on  {\em generadora de  probabilidad} (pgf para  {\em probability
  generating function} en ingles) se entiende sencillamente del hecho siguiente:
%
\begin{lema}
\label{Lem:MP:GeneracionProbabilidades}
%
  Cuando \ $\X = \Nset^d$ \ para cualquier \ \modif{$x = \begin{bmatrix} x_1 & \ldots &
    x_d \end{bmatrix}^t \in 
  \Part{k,d} =  \left\{ x = \begin{bmatrix}  x_1 & \cdots  & x_d \end{bmatrix}^t
    \in \Nset^d \tq \sum_{i=1}^d x_i = k \right\}$ (ver notaciones)}
  %
  \[
  \frac1{\prod_{i=1}^d  \modif{x_i!}} \, \left.\frac{\partial^k  G_X}{\partial z_1^{\modif{x_1}}
      \ldots \partial z_d^{\modif{x_d}}}\right|_{z=0} = P_X(\modif{x}) = P(X = \modif{x})
  \]
\end{lema}
%
\begin{proof}
  Se puede escribir la funci\'on \ $G_X$ \ bajo su forma de generadora ordinaria
  \ $\displaystyle  G_X(z) =  \sum_{\modif{x} \in \Nset^d}  \left( \prod_{i=1}^d
    z_i^{\modif{x_i}}   \right)   P(X  =   \modif{x})$   \   con  \   $\modif{x}
  =  \begin{bmatrix} \modif{x_1}  &  \cdots &  \modif{x_d} \end{bmatrix}^t$.   A
  continuaci\'on, se nota que la serie converge uniformamente por lo menos en la
  bola  $\Bset_d \equiv  \Bset_d(1)$,  probando que  $G_X$  es diferenciable  en
  $\Bset_d$, as\'i  que esta series  es nada mas  que el desarollo de  Taylor de
  $G_X$ al torno de $z = 0$ (o, equivalentemente, se puede diferenciar la suma y
  tomar la derivada en $z = 0$), lo que cierra la prueba.
\end{proof}

De este resultado,  se puede notar que, en el caso  discreto, hay una relaci\'on
uno-a-uno entre  la medida  de probabilidad $P_X$  y la funci\'on  generadora de
probabilidad   $G_X$.     \modif{En   el   caso   general},    veremos   en   la
subsecci\'on~\ref{Ssec:MP:FuncionCaracterisica} que para  $z_j$ de la forma $z_j
= e^{\imath u_j}$ \  con \ $u_j \in \Rset$ \ la  transformaci\'on se inversa, de
manera que se  puede recuperar la medida  de probabilidad \ $P_X$ \  a partir de
$G_X$. Dicho de  otra manera, como la medida  \ $P_X$, \ la funci\'on  \ $G_X$ \
caracteriza completamente el vector aleatorio \ $X$.

\

Aparece  que la  funci\'on generadora  \ $G_X$  \ se  vincula tambi\'en  con los
momentos factoriales, justificando su secunda denominaci\'on, {\em generadora de
  momentos factoriales}  (fmgf para {\em factorial  moments generating function}
en ingles):
%
\begin{lema}
\label{Lem:MP:GeneracionMomentosFactoriales}
%
  \modif{Para cualquier \  $k = \begin{bmatrix} k_1 & \cdots  & k_d \end{bmatrix}^t \in \Part{K,d}$,}
  %\Nset^d$ \ con \ $K =  \sum_{i=1}^d k_i$,
  derivando $G_X$ se proba que,  cuando existen~\footnote{En el caso extremo, un
    anillo  del  dominio de  convergencia  de la  serie  dando  $G_X$ puede  ser
    restricto al circulo  unitario, as\'i que no hay  garantia que las derivadas
    en $z = 1$ existen.}
  %
  \[
  \left.\frac{\partial^K     G_X}{\partial     z_1^{k_1}     \cdots     \partial
      z_d^{k_d}}\right|_{z=1} = \Esp\left[ \prod_{i=1}^d \PocD{X_i}{k_i} \right]
  \]
  %
  momento  factorial~\footnote{Recuerdense que $\PocD{x}{n}  = \prod_{i=0}^{n-1}
    (x-i), \quad n > 0$ \  s\'imbolo de Pochhammer, con la convenci\'on $(x)_0 =
    1$; ver nota de pie~\ref{Foot:MP:Pochhammer}.} de $X$.
\end{lema}

De  este resultado,  se ve  por ejemplo  que, cuando  existen, se  recuperan los
momentos de $X$ a trav\'es de las derivadas de $G_X$:
%
\begin{itemize}
\item $G_X(1) = 1$, condici\'on de normalizaci\'on.
%
\item $\nabla_z G_X(1) = \Esp[X]$.
%
\item $\Hess_z G_X(1)  + \Diag\left( \nabla_z G_X(1) \right)  = \Esp\left[ X X^t
  \right]$  \  donde  \ $\Hess_z$  \  es  la  matrice  Hessiana y  \  $\Diag(a)$
  \modif{matriz diagonal de componentes \ $(i,i)$-\'esima igual a \ $a_i$ \ (ver
    notaciones).}
  % vector $a$ sobre la diagonal).
  Entonces la  matriz de covarianza  es dada por  \ $\Cov[X] = \Hess_z  G_X(1) +
  \Diag\left( \nabla_z G_X(1) \right) - \nabla_z G_X(1) \nabla_z^t G_X(1)$.
\end{itemize}

\

La funci\'on \ $G_X$ \ tiene unas propiedades permitiendo por ejemplo de manejar
sencillamente  distribuciones  de probabilidades  de  combinaciones lineales  de
vectores aleatorios independientes,  como lo vamos a ver  a trav\'es del teorema
siguiente.

\begin{teorema}%[Funci\'on generadora de probabilidad de una ]
\label{Teo:MP:PropiedadesGeneradoraProbabilidad}
%
  Sean  \   $X$  \  e  \   $Y$  \  dos   vectores  aleatorios  $d$-dimensionales
  independientes, $a  = \begin{bmatrix} a_1  & \cdots & a_d  \end{bmatrix}^t \in
  \Rset^d$ \  y \ $b  = \begin{bmatrix} b_1  & \cdots & b_d  \end{bmatrix}^t \in
  \Rset^d$.  Entonces  para  cualquier  $z  = \begin{bmatrix}  z_1  &  \cdots  &
    z_d \end{bmatrix}\in \Cset^d$ \ (donde existen las funciones):
  %
  \[
  G_{\Diag(a)  X +  b}(z)  = \left(  \prod_{i=1}^d  z_i^{b_i} \right)  G_X\left(
    z_1^{a_1} , \ldots , z_d^{a_d} \right),
  \]
  %
  \[
  G_{X+Y}(z) = G_X(z) \, G_Y(z)
  \]
  %
  y para $z \in \Cset$
  %
  \[
  G_X\left( z^{a_1} , \ldots , z^{a_d} \right) = G_{a^t X}(z)
  \]
\end{teorema}
%
\begin{proof}
  El  primer  resultado es  inmediato,  escribiendo \  $z_i^{a_i  X_i  + b_i}  =
  z_i^{b_i}   \left(  z_i^{a_i}   \right)^{X_i}$.    El  secundo   viene  de   \
  $z_i^{X_i+Y_i}    =    z_i^{X_i}    z^{Y_i}$    \   conjuntamente    con    el
  teorema~\ref{Teo:MP:IndependenciaMomentos}   con   \   $f(X)  =   \prod_{i=1}^d
  z_i^{X_i}$  \ y \  $g(Y) =  \prod_{i=1}^d z_i^{Y_i}$.  El tercer  resultado es
  consecuencia de $\prod_{i=1}^d  \left( z^{a_i} \right)^{X_i} = z^{\sum_{i=1}^d
    a_i X_i}$.
\end{proof}
%
Estos  resultados permiten manejar  sencillamente la  medida de  probabilidad de
combinaciones lineales  de vectores aleatorios independientes y  de marginales a
trav\'es esta funci\'on generadora.

\

De  la  tercera identidad,  se  puede  hacer un  paso  m\'as  tratando de  sumas
aleatorias de vectores aleatorios:
%
\begin{teorema}\label{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}
%
  Sea  \ $\{  X_n \}_{n  \in \Nset}$  \, una  sucesi\'on de  vectores aleatorios
  indepedientes de  misma distribuci\'on  (resp.  generadora de  probabilidad) \
  $P_X$ \  (resp.  $G_X$) \  y \  $N$ \ una  variable aletoria definida  sobre \
  $\Nset$, independiente  de los  \ $X_n$.  Sea  el vector  aleatorio \ $  S_N =
  \sum_{n=0}^N X_n$. Entonces
  %
  \[
  G_{S_N}(z) =  G_N \big( G_X(z) \big),
  \]
  %
\end{teorema}
%
\begin{proof}
  Usando la formula de esperanza total del teorema.~\ref{Teo:MP:EsperanzaTotal}, se escribe
  %
  \begin{eqnarray*}
  G_{S_N}(z) & = & \Esp\left[ z^{\sum_{n=0}^N X_n} \right]\\[2.5mm]
  %
  & = & \Esp\left[ \Esp\left[ \left. z^{\sum_{n=0}^N X_n} \right| N  \right] \right]\\[2.5mm]
  %
  & = & \Esp\left[ G_X(z)^N \right]
  \end{eqnarray*}
\end{proof}


% ================================= Funcion generadora de momentos

\subseccion{Funci\'on generadora de momentos}
\label{Ssec:MP:GeneradoraMomentos}


Como lo hemos visto, la  funci\'on generadora de probabilidad permite recucperar
los  momentos  de  un  vector  aleatorio  a trav\'es  de  combinaciones  de  sus
derivadas.   Con una pequa\~na  modificaci\'on, se  puede definir  una funci\'on
permitiendo   recuperar    m\'as   directamente   los    momentos,   de   manera
siguiente~\cite{Fel68, JohKot97, Muk00, AthLah06}:
%
\begin{definicion}[funci\'on generadora de momentos]
\label{Def:MP:GeneradoraMomentos}
%
  La {\em  funci\'on generadora  de momentos} (mgf  para {\em  moment generating
    function} en ingles) de un vector aleatorio $d$-dimensional se define como
  %
  \[
  M_X(u) = \Esp\left[ e^{u^t X} \right]
  \]
  %
  para $u \in \Cset^d$.
\end{definicion}
%
De esta definici\'on se nota inmediatamente que
%
\[
M_X(u) = G_X\left( e^u \right)  \qquad \mbox{donde} \qquad e^u = \begin{bmatrix}
  e^{u_1} & \cdots & e^{u_d} \end{bmatrix}^t
\]
%
Entonces, como \ $G_X$, la  generadora de los momentos caracteriza completamente
el vector  aleatorio \ $X$.   Adem\'as, de  este v\'inculo entre  \ $G_X$ \  y \
$M_X$, y del  dominio de definici\'on de \  $G_X$, queda claro que \  $M_X$ \ es
definida sobre un  producto cartesiano de franjas del plano  complejo, $ v_i \le
\real{u_i} \le  V_i$ \ donde \  $v_i \le 0 \le  V_i$ \ son llamados  {\em indices de
  convergencia}.  En el caso de variables escalares \modif{denotando \ $s = -u$,
  esta   funci\'on   se   interpreta   como   la   transformada   bilateral   de
  Laplace-Stieltjes de  la medida  \ $P_X$.   Si adem\'as \  $P_X$ \  admite una
  densidad \ $p_X$,  esta funci\'on se interpreta entonces  como la transformada
  bilateral de Laplace  usual de \ $p_X$. Se  refiera por ejemplo a~\cite{Wid46}
  para   m\'as   detalles   sobre   esta   transformaci\'on  (y   la   nota   de
  pie~\ref{Foot:MP:LaplaceStieltjes}                    de                    la
  secci\'on~\ref{Ssec:MP:FamiliaEliptica} m\'as adelante para el caso unilateral
  usual).}


Se muestra  que esta  funci\'on es  continua en su  dominio de  definici\'on, en
particular en  un entorno de  \ $u =  0$ donde queda  positiva. Eso viene  de la
continuidad  de $x \mapsto  e^{u^t x}$  \ y  de la  convergencia uniforme  de la
integral en el dominio de definici\'on (ver secciones anteriores y el teorema de
convergencia  dominada). Adem\'as,  si admite  un  desarollo de  Taylor en  este
punto, la generadora de los momentos permite recuperar directamente los momentos
a trav\'es de derivadas, sin hacer combinaciones:
%
\begin{lema}
\label{Lem:MP:GeneracionMomentos}
%
  Para cualquier \  \modif{$k > 0, \quad (i_1,\ldots,i_k) \in \{ 1 , \ldots , d \}^k $}
%  \Nset^d$ \ con \ $K =  \sum_{i=1}^d k_i$, 
  derivando $M_X$ se proba que, cuando existen
  %
  \[
  \modif{\left.\frac{\partial^k     M_X}{\partial     u_{i_1}     \cdots     \partial
      u_{i_k}}\right|_{u=0}  = \Esp\left[  \prod_{j=1}^k  X_{i_j} \right]  =
  m_{i_1,\ldots,i_k}[X]}
  \]
  %
  momento de orden \ $k$ \ de \ $X$.
\end{lema}
%
En particular, se recuperan
%
\begin{itemize}
\item $M_X(0) = 1$, condici\'on de normalizaci\'on.
%
\item $\nabla_u M_X(0) = \Esp[X]$ \ promedio,
%
\item $\Hess_u M_X(0) = \Esp\left[ X X^t \right]$, \ie $\Cov[X] = \Hess_u M_X(0)
  - \nabla_u M_X(0) \nabla_u^t M_X(0)$ \ matriz de covarianza.
\end{itemize}

Como la funci\'on \ $G_X$, la  generadora de los momentos tiene unas propiedades
similares  a las  de  los teoremas~\ref{Teo:MP:PropiedadesGeneradoraProbabilidad}
y~\ref{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}:
%
\begin{teorema}%[Funci\'on generadora de probabilidad de una ]
\label{Teo:MP:PropiedadesGeneradoraMomentos}
%
  Sean  \   $X$  \  e  \   $Y$  \  dos   vectores  aleatorios  $d$-dimensionales
  independientes,  \ $A$  \ una  matriz de  \  $\M_{d',d}(\Rset)$ \  y \  $b
  =   \begin{bmatrix}    b_1   &    \cdots   &   b_{d'}    \end{bmatrix}^t   \in
  \Rset^{d'}$.  Entonces para  cualquier $u  =  \begin{bmatrix} u_1  & \cdots  &
    u_{d'} \end{bmatrix}^t \in \Cset^{d'}$ \ (donde la funci\'on existe):
  %
  \[
  M_{A X + b}(u) =  e^{u^t b} M_X\left( A^t u \right),
  \]
  %
  y para cualquier  $u = \begin{bmatrix} u_1 & \cdots  & u_d \end{bmatrix}^t \in
  \Cset^d$ \ (donde la funci\'on existe):
  %
  \[
  M_{X+Y}(u) = M_X(u) \, M_Y(u)
  \]
  %
  Adem\'as,  para \  $\{ X_n  \}_{n \in  \Nset}$ \,  una sucesi\'on  de vectores
  aleatorios  indepedientes  de   misma  distribuci\'on  (resp.   generadora  de
  momentos) \ $P_X$ \ (resp. $M_X$) \  y \ $N$ \ una variable aleatoria definida
  sobre \ $\Nset$, independiente de los \ $X_n$, y \ $ S_N = \sum_{n=0}^N X_n$,
  %
  \[
  M_{S_N}(u) =  G_N \big( M_X(u) \big),
  \]
\end{teorema}
%
\begin{proof}
  Las  pruebas  siguen   punto  a  punto  los  mismos  pasos   que  las  de  los
  teoremas~\ref{Teo:MP:PropiedadesGeneradoraProbabilidad}
  y~\ref{Teo:MP:SumaAleatoriaGeneradoraProbabilidad}.
\end{proof}
%\cite{Fra09}%%p.73


% ================================= Funcion caracteristica

\subseccion{Funci\'on caracter\'istica}
\label{Ssec:MP:FuncionCaracteristica}

Si  la funci\'on generadora  de momentos  permite recuperar  los momentos  de un
vector  aleatorio, no  es definida  sobre todo  $\Cset^d$.  Sin  embargo, cuando
$\real{u_i} = 0$,  esta funci\'on es siempre definida.   Entonces, una funci\'on
generadora muy \'util que se usa frecuentemente es la de momentos para este tipo
de argumentos,  lo que es conocida  como funci\'on caracter\'istica y  que es al
final definida  sobre $\Rset^d$  de manera siguiente~\cite{Luk61,  Gol61, Fel68,
  SteWei71, JohKot97, Muk00, AshDol99, AthLah06, Sas13}:


\begin{definicion}[funci\'on   caracter\'istica]
\label{Def:MP:FuncionCaracteristica}
% 
  La {\em funci\'on caracter\'istica}  (cf para {\em characteristic function} en
  ingles) de un vector aleatorio $d$-dimensional se define como
 %
  \[
  \Phi_X(\omega) = \Esp\left[ e^{\imath \omega^t X} \right]
  \]
  %
  para $\omega \in \Rset^d$.
\end{definicion}
%
De esta definici\'on se nota inmediatamente que
%
\[
\Phi_X(\omega) = M_X(\imath \omega) = G_X\left( e^{\imath \omega} \right) \qquad
\mbox{donde} \qquad e^{\imath \omega}  = \begin{bmatrix} e^{\imath u_1} & \cdots
  & e^{\imath u_d} \end{bmatrix}^t
\]
%
De hecho,  se puede definir esta  funci\'on para un argumento  complejo, pero es
equivalente a volver a la definici\'on de la generadora de momentos.

En su forma general, la funci\'on caracter\'istica se escribe
%
\[
\Phi_X(\omega) = \int_{\Rset^d} e^{\imath \omega^t x} \, dP_X(x)
\]
%
y  es  relacionada   a  la  transformada  de  Fourier-Stieltjes   de  la  medida
$P_X$~\cite[Chap.~5]{Pin09}. Cuando  \ $P_X$ \  admite una densidad \  $p_X$, la
funci\'on  es  una  transformada  de  Fourier  usual de  la  densidad  \  $p_X$,
introducida bajo el  impulso de Fourier en 1822 para  estudiar la difusi\'on del
calor~\cite{Fou22}.

Insistamos sobre  el hecho que  la importancia de  esta funci\'on reside  en que
siempre existe y est\'a bien  definida, dado que \ $\displaystyle \int_{\Rset^d}
\left| e^{\imath \omega^t x} \right| \, dP_X(x) = \int_{\Rset^d} dP_X(x) = 1$.
% \cite{Gol61}

Como para las generadoras ya introducidas, la funci\'on caracter\'istica permite
recuperar directamente los momentos a trav\'es de derivadas:
%
\begin{lema}
\label{Lem:MP:GeneracionMomentoViaCaracteristica}
%
  Para cualquier \  \modif{$k > 0, \quad (i_1,\ldots,i_k) \in \{ 1 , \ldots , d \}^k$},
%  \Nset^d$ \  con \  $K =  \sum_{i=1}^d k_i$, 
  derivando $\Phi_X$ se proba que, cuando existen
  %
  \[
  (-   \imath)^k  \,   \left.\frac{\partial^k   \Phi_X}{\partial  \omega_{i_1}
      \cdots    \partial    \omega_{i_k}}\right|_{\omega=0}    =    \Esp\left[
    \prod_{j=1}^k X_{i_j} \right] = m_{i_1,\ldots,i_k}[X]
  \]
  %
  momento de orden \ $k$ \ de \ $X$.
\end{lema}
%
En particular, se recuperan
%
\begin{itemize}
\item $\Phi_X(0) = 1$, condici\'on de normalizaci\'on.
%
\item $- \imath \nabla_\omega M_X(0) = \Esp[X]$ \ promedio,
%
\item  $- \Hess_\omega  M_X(0) =  \Esp\left[ X  X^t \right]$,  \ie $\Cov[X]  = -
  \Hess_\omega M_X(0) + \nabla_\omega M_X(0) \nabla_\omega^t M_X(0)$ \ matriz de
  covarianza.
\end{itemize}

Notar que \ $\Phi_X$ \ no es siempre diferenciable en $\omega = 0$; Por ejemplo,
en el caso de la distribuci\'on de Cauchy--Lorentz univariada~\footnote{Lo mismo
  occure   en   la   extensi\'on   multivariada~\cite{SamTaq94}.}\   $p_X(x)   =
\frac{\gamma}{\pi \left(  \gamma^2 + (x-x_0)^2 \right)}$  \ con \  $\gamma > 0$,
resulta  \ $\Phi_X(\omega)  = e^{-\imath  x_0 \omega  -\gamma |\omega|}  $. Esta
funci\'on est\'a definida para todo $\omega$,  pero no es derivable en $\omega =
0$, lo que coincide con el hecho  de que no est\'an definidos los momentos mayor
o igual a uno para esta densidad de probabilidad.

Resumimos algunas otras propiedades importantes de la funci\'on caracter\'istica:
%
\modif{
\begin{teorema}[Propiedades principales de la funci\'on caracter\'istica]
\label{Teo:MP:PropCarac}
\mbox{ }

\begin{enumerate}
\item\label{Prop:MP:ContinuidadPhiX}  $\Phi_X$   es  una  funci\'on   medible  y
  continua en $\Rset^d$~\cite[Prop.~5.2.1]{Pin09}.   Eso es una consecuencia del
  teorema           de            convergencia           dominada           (ver
  teorema~\ref{Teo:MP:ConvergenciaDominada}).
  % pagina~\pageref{Teo:MP:ConvergenciaDominada}).
%
\item\label{Prop:MP:PhiXZero} $\Phi_X(0) = 1  $: Eso es inmediato escribiendo la
  integral, siendo $P_X$ una medida de probabilidad.
%
\item\label{Prop:MP:MaximoPhiX}   $\left|  \Phi_X(\omega)   \right|   \le  1   =
  \Phi_X(0)$: \  $\left| \Phi_X(\omega)  \right|$ es m\'axima  en $\omega  = 0$.
  Eso  viene directamente de  $\left| e^{\imath  \omega^t x}  \right| =  1$. Eso
  significa tambi\'en que $$\Phi_X(\Rset^r) \subset \{ z \in \Cset \tq |z| \le 1
  \}$$
%
\item\label{Prop:MP:HermiticaPhiX}    $\Phi_X(-\omega)    =   \Phi_X^*(\omega)$:
  $\Phi_X$ tiene una s\'imetria hermitica.
%
\item  $\Phi_X\in  \PD_d$,  es  definida  no  negativa,  \ie  para  un  conjunto
  arbitrario de \ $n  \ge 1$ \ n\'umeros complejos \ $a_1 , \ldots  , a_n$ \ y \
  $n$ \ vectores \ $w_1 , \ldots , w_n$ \ de \ $\Rset^d$, se cumple
  \[
  \sum_{k,l=1}^n a_k^* a_l \Phi_X(w_l-w_k) \ge 0
  \]
  %
  Dicho   de  otra   manera,   la  matriz   de   componente  \   $(k,l)$-\'esima
  $\Phi_X(w_l-w_k)$ es a hermitica (s\'imetria herm\'itica dada por la propiedad
  anterior,  y   no  negativa  definida),   \ie  matriz  de   $P_n(\Cset)$  (ver
  notaciones).  Esta positividad viene  de \ $\displaystyle \sum_{k,l=1}^n a_k^*
  a_l e^{\imath (w_l-w_k)^t x} =  \left| \sum_l a_l e^{\imath w_l^t x} \right|^2
  \ge 0$.
%
\end{enumerate}
\end{teorema}

Si una  funci\'on \  $f: \Rset^d \mapsto  \Cset$ \  es a simetr\'ia  hermitica y
definida  positivas, obviamente  $f(0) \in  \Rset_+$ ($n  = 1,  a_1 =  1,  x_1 =
0$). Adem\'as, tomando $a_1  = 1, \, a_2 = \pm 1$ y $w_1  = 0$, para cualquier \
$w_2 = w \in \Rset^d$ \ se obtiene \ $\left| f(w) \right| \le \left| \real{f(w)}
\right| \le  f(0)$: $|f|$ es m\'axima en  $0$.  En lo que  sigue, denotaremos el
convexo (ver notaciones)
%
\[
\PD_d =  \big\{ \Phi:  \Rset^d \mapsto \Cset  \: \mbox{ continuas,  a simetr\'ia
  hermitica y definida positivas con } \: \Phi(0) = 1 \big\}
\]
%
(la  clase de  las funci\'on  continuas,  a simetr\'ia  herm\'itica y  definidas
positivas es definida  bajo un factor real positivo).  Se notar\'a tambien, que,
por proyeci\'on del argumento de la funci\'on,
%
\[
\PD_1 \supset \PD_2 \cdots \supset \PD
\]
%

De  hecho, existe  una rec\'iproca  al teorema~\ref{Teo:MP:PropCarac},  debido a
S. Bochner~\footnote{De  hecho, lo prob\'o Bochner  en el caso escalar  $d = 1$,
  pero  se extiende  al  caso multivariado.}~\cite{Boc32,  Boc59, Gol61,  Pin09,
  Sas13}
%
\begin{teorema}[Bochner]\label{Teo:MP:Bochner}
%
  Una  funci\'on \  $\Phi: \Rset^d  \mapsto \Cset$  \ es  continua,  definida no
  negativa con \ $\Phi(0) = 1$, \ie  $\Phi \in \PD_d$, si y solamente existe una
  medida \ $\mu$ \ de probabilidad sobre \ $\B(\Rset^d)$ \ tal que
  %
  \[
  \forall \:  \omega \in \Rset^d, \quad \Phi(\omega)  = \int_{\Rset^d} e^{\imath
    \, \omega^t x} d\mu(x)
  \]
  %
  Dicho de  otra manera, cualquier  funci\'on continua, definida positiva  con \
  $\Phi(0) = 1$ \ es la funci\'on caracter\'istica de un vector aleatorio.
\end{teorema}
%
%\begin{proof}
En el teorema,  vimos que la transformada de Fourier-Stieltjes  de una medida de
probabilidad $P_X$  es medible, continua,  definida no negativa, con  $\Phi(0) =
1$. La  rec\'iproca es m\'as dificil  a probar y necesita  lemas adicionales. Se
puede  encontrar una  linda prueba  en~\cite[Sec.~1.7]{Sas13} adonde  dejamor el
lector.
%\end{proof}

%
\[
\]

Como   lo  hemos   notado   en  las   subsecciones   anteriores,  la   funci\'on
caracter\'istica define  completamente el  vector aleatorio. En  particular, hay
una relaci\'on uno-uno (casi siempre) entre \ $\Phi_X$ \ y la medida \ $P_X$. En
particular, existe una f\'ormula de inversi\'on permitiendo volver a la medida \
$P_X$ \ a partir de \ $\Phi_X$~\cite{AshDol99, Sas13}:
%
\begin{teorema}[F\'ormula de inversi\'on]\label{Teo:MP:InversionFourierStieltjes}
%
  Sea \ $X$  \ vector aleatorio $d$-dimensional de  funci\'on caracter\'istica \
  $\Phi_X$.   Sea  \  $\displaystyle  A  = \optimes_{i=1}^d  (a_i  \;  b_i)  \in
  \B(\Rset^d)$ \ y \ $\partial A  = \optimes_{i=1}^d [a_i \; b_i] \setminus A$ \
  su borde. Entonces~\footnote{Se prolonga  la funci\'on \ $\frac{e^{- \imath \,
        a_j \omega_j} - e^{- \imath  \, b_j \omega_j}}{\imath \, \omega_j}$ \ en
    \ $\omega_j  = 0$ \ por  su l\'imite \ $\displaystyle  \lim_{\omega_j \to 0}
    \frac{e^{- \imath \, a_j \omega_j} - e^{- \imath \, b_j \omega_j}}{\imath \,
      \omega_j} = b_j - a_j$.},
  %
  \[
  \begin{array}{c}
  %
  \displaystyle \lim_{w_1 \to +\infty,\ldots, w_d \to \infty} \: \frac{1}{(2
  \pi)^d} \int_{\optimes_{j=1}^d [-w_j \; w_j]} \Phi_X(\omega) \prod_{j = 1}^d
  \frac{e^{- \imath \, a_j \omega_j} - e^{- \imath \, b_j \omega_j}}{\imath \,
  \omega_j} \: d\omega\\[2.5mm]
  %
  =\\[2.5mm]
  %
   \displaystyle \int_{\Rset^d} \prod_{j=1}^d \left(
     \un_{\left(  a_j \;  b_j \right)}(x_j)  +  \frac12 \un_{\left\{  a_j \;  b_j
       \right\}}(x_j) \right) dP_X(x)
  \end{array}
  \]
  %
  En particular, cuando $P_X$ vale 0  sobre el borde de $A$, es decir $P_X\left(
  \partial A \right) = 0$, se obtiene
  %

  \[
  \lim_{w_1  \to   +\infty,\ldots,  w_d  \to  \infty}   \:  \frac{1}{(2  \pi)^d}
  \int_{\optimes_{j=1}^d   [-w_j  \;  w_j]}   \Phi_X(\omega)  \prod_{j   =  1}^d
  \frac{e^{- \imath \,  a_j \omega_j} - e^{- \imath  \, b_j \omega_j}}{\imath \,
    \omega_j} \, d\omega \: = \: P_X(A).
  \]
\end{teorema}
%
Nota: el l\'imite \ $\displaystyle\lim_{T  \to +\infty} \int_{-T}^T$ \ se nota a
veces \ $\displaystyle \operatorname{vp} \!  \int_\Rset$, integral {\it en valor
  principal}.
%
\begin{proof}
  Por definici\'on de la funci\'on caracter\'istica, tenemos
  %
  \[
  \int_{\optimes_{j=1}^d   [-w_j  \;  w_j]} \Phi_X(\omega)  \prod_{j = 1}^d  \frac{e^{- \imath  \, a_j
      \omega_j} - e^{- \imath \, b_j \omega_j}}{\imath \, \omega_j} \, d\omega =
  \int_{\optimes_{j=1}^d   [-w_j  \;  w_j]} \int_{\Rset^d} e^{\imath  \omega^t x} \, dP_X(x) \prod_{j =
    1}^d  \frac{e^{- \imath a_j  \omega_j} -  e^{- \imath  b_j \omega_j}}{\imath
    \omega_j} \, d\omega
  \]
  %
  Ahora, notando que $\left| \frac{e^{- \imath \, a_j \omega_j} - e^{- \imath \,
        b_j \omega_j}}{\imath  \, \omega_j} \, e^{\imath \,  \omega^t x} \right|
  \le  b_j - a_j$  \ es  uniformamente acotado,  se puede  evocar el  teorema de
  Fubini~\ref{Teo:MP:Fubini} para intercambiar las  integrales, as\'i que, con \
  $e^{\imath \, \omega^t x} = \prod_{j=1}^d e^{\imath \, \omega_j x_j}$, tenemos
  %
  \[
  \int_{\optimes_{j=1}^d   [-w_j  \;  w_j]}   \Phi_X(\omega)  \prod_{j   =  1}^d
  \frac{e^{- \imath \,  a_j \omega_j} - e^{- \imath  \, b_j \omega_j}}{\imath \,
    \omega_j} \, d\omega = \int_{\Rset^d} \left( \prod_{j=1}^d \int_{-w_j}^{w_j}
    \frac{e^{- \imath \, \omega_j (a_j -  x_j)} - e^{- \imath \, \omega_j (b_j -
        x_j)}}{\imath \, \omega_j} \, d\omega_j \right) \, dP_X(x)
  \]
  %
  Se nota que
  %
  \begin{eqnarray*}
  \int_{-w_j}^{w_j} \frac{e^{- \imath \, \omega_j (a_j - x_j)} - e^{- \imath \, \omega_j
  (b_j - x_j)}}{\imath \, \omega_j} \, d\omega_j & = & - \int_{-w_j}^{w_j} \frac{e^{+
  \imath \, \omega_j (a_j - x_j)} - e^{+ \imath \, \omega_j (b_j - x_j)}}{\imath \,
  \omega_j} \, d\omega_j\\[2mm]
  %
  & = & \int_{-w_j}^{w_j} \frac{\sin(\omega_j (b_j - x_j)) - \sin(\omega_j (a_j -
  x_j))}{\omega_j} \, d\omega_j
  \end{eqnarray*}
  %
  por cambio de variables $\omega_j \to - \omega_j$ en la primera linea, tomando
  entonce  la media  suma de  los  terminos derecho/izquierdo  dando la  secunda
  linea. Seguimos notando que
  %
  \[
  \int_{-w}^w  \frac{\sin(\omega (c  - x))}{\omega}  \, d\omega  = \sign(c  - x)
  \int_{-w |c - x|}{w |c - x|} \frac{\sin(\omega)}{\omega} \, d\omega
  \]
  %
  es  decir, de  \ $\displaystyle  \lim_{T \to  +\infty}  \int_{-T}^T \frac{\sin
    \omega}{\omega} d\omega = \pi$ \ \cite[Ec.~3.721]{GraRyz15}, se obtiene
  %
  \[
  \lim_{w \to  +\infty} \int_{-w}^w \frac{\sin(\omega  (c - x))}{\omega}
  \, d\omega = \pi \sign(c-x)
  \]
  %
  Se  acaba  la   prueba  de  \  $\left|  \frac{\sin(\omega_j   (b_j  -  x_j)  -
      \sin(\omega_j  (a_j -  x_j))}{\omega_j} \right|  < 2$  \  conjuntamente al
  teorema de convergencia dominada~\ref{Teo:MP:ConvergenciaDominada} permitiendo
  permutar integral  y l\'imite,  y de $\sign(b_j-x_i)  - \sign(a_j-x_i) =  2 \,
  \un_{\left(   a_j   \;  b_j   \right)}(x_j)   +   \un_{\left\{   a_j  \;   b_j
    \right\}}(x_j)$.
\end{proof}

Dos teoremas de inversi\'on en los casos particular continuo y discreto permiten
respectivamente  volver  a   la  densidad  de  probabilidad  o   a  la  masa  de
probabilidad.
%
\begin{teorema}[Inversi\'on, caso continuo]\label{Teo:MP:InversionDensidad}
%
  Si \ $\Phi_X$ \ es integrable, entonces \ $P_X$ \ admite una densidad tal que
  %
  \[
  p_X(x)  =  \frac{1}{(2  \pi)^d}  \int_{\Rset^d} \Phi(\omega) \, e^{-  \imath  \,
    \omega^t x} \, d\omega
\]
\end{teorema}
%
\begin{proof}
  Varias pruebas  existen (ej.~\cite[p.~21]{Sas13}). Una  bastante directa puede
  ser       de      salir      de       la      f\'ormula       general      del
  teorema~\ref{Teo:MP:InversionFourierStieltjes}, de  fijar \ $a$ \ y  \ poner $b
  \equiv x \in  \Rset^d$. Se toma la derivada  \ $\frac{\partial^d}{\partial x_1
    \cdots \partial x_d}$ \ de  la integral \ $\displaystyle \frac{1}{(2 \pi)^d}
  \int_{\optimes_{j=1}^d   [-w_j  \;  w_j]}   \Phi_X(\omega)  \prod_{j   =  1}^d
  \frac{e^{- \imath \,  a_j \omega_j} - e^{- \imath  \, b_j \omega_j}}{\imath \,
    \omega_j} \: d\omega$ \ y se  evoca el teorema de convergencia dominada para
  intercambiar  derivaci\'on e integraci\'on,  y luego  tomar el  l\'imite, para
  obtener el resultado.
\end{proof}

\begin{teorema}[Inversi\'on, caso discreto]\label{Teo:MP:InversionMasa}
%
  Para cualquier $x \in \Rset^d$,
  %
  \[
  \lim_{w_1  \to  \infty,\ldots,w_d \to  \infty} \,  \frac{1}{2^d  w_1 \ldots  w_d}
  \int_{\optimes_{j=1}^d [-w_j \; w_j]}  \Phi(\omega) \, e^{- \imath \, \omega^t x}
  \, d\omega = P_X(x)
  \]
\end{teorema}
%
\begin{proof}
  Por definici\'on de  la funci\'on caracter\'istica, y aplicando  el teorema de
  Fubini como en el caso general (mismo enfoque),
  %
  \begin{eqnarray*}
  \frac{1}{2^d w_1 \ldots w_d} \int_{\optimes_{j=1}^d [-w_j \; w_j]}
  \Phi_X(\omega) e^{- \imath \omega^t x} \, d\omega & = & \frac{1}{2^d w_1 \ldots
  w_d} \int_{\optimes_{j=1}^d [-w_j \; w_j]} \int_{\Rset^d} e^{\imath \omega^t y}
  \, dP_X(y) \, e^{- \imath \omega^t x} d\omega\\[2mm]
  %
  & = & \int_{\Rset^d} \left( \prod_{j=1}^d \frac{1}{2 w_j} \int_{-w_j}^{w_j}
  e^{\imath \, (y_j - x_j) w_j} dw_j \right) dP_X(y)\\[2mm]
  %
  & = & \int_{\Rset^d} \prod_{j=1}^d \frac{\sin(w_j (y_j-x_j))}{(y_j-x_j) w_j}  dP_X(y)
  \end{eqnarray*}
  %
  con   el  l\'imite   \  $\displaystyle   \lim_{y_j  \to   x_j}  \frac{\sin(w_j
    (y_j-x_j))}{w_j( y_j-x_j)}  = 1$. Ad\'emas, con  el mismo enfoque  que en el
  cas general acotando  el integrande, por el teorema  de convergencia dominada,
  se  puede  intercambiar  l\'imite  e  integral,  as\'i  que  \  $\displaystyle
  \lim_{x_j   \to   \infty}    \frac{\sin(w_j   (y_j-x_j))}{w_j(   y_j-x_j)}   =
  \un_{x_j}(y_j)$, lo cierra la prueba
\end{proof}

Como las  funciones \  $G_X$ \  y \ $M_X$,  la funci\'on  caracter\'istica tiene
entre      otros      propiedades      similares      a     las      de      los
teoremas~\ref{Teo:MP:PropiedadesGeneradoraMomentos}
y~\ref{Teo:MP:SumaAleatoriaGeneradoraMomentos}:
%
\begin{teorema}%[Funci\'on generadora de probabilidad de una ]
\label{Teo:MP:PropiedadesFuncionCaracteristica}
%
  Sean  \   $X$  \  e  \   $Y$  \  dos   vectores  aleatorios  $d$-dimensionales
  independientes,  \ $A$  \ una  matriz de  \  $\M_{d',d}(\Rset)$ \  y \  $b
  =  \begin{bmatrix} b_1  &  \cdots &  b_{d'}  \end{bmatrix}^t \in  \Rset^{d'}$.
  Entonces  para  cualquier  $\omega  =  \begin{bmatrix}  \omega_1  &  \cdots  &
    \omega_{d'} \end{bmatrix}^t \in \Rset^{d'}$:
  %
  \[
  \Phi_{A X + b}(\omega) =  e^{\imath \omega^t b} \, \Phi_X\left( A^t \omega \right),
  \]
  %
  y   para   cualquier \  $\omega   =   \begin{bmatrix}   \omega_1   &  \cdots   &
    \omega_d \end{bmatrix}^t \in \Rset^d$:
  %
  \[
  \Phi_{X+Y}(\omega) = \Phi_X(\omega) \, \Phi_Y(\omega)
  \]
  %
  Adem\'as,  para \  $\{ X_n  \}_{n \in  \Nset}$ \,  una sucesi\'on  de vectores
  aleatorios  indepedientes  de   misma  distribuci\'on  (resp.   generadora  de
  momentos) \ $P_X$ \ (resp. $M_X$) \  e \ $N$ \ una variable aleatoria definida
  sobre \ $\Nset$, independiente de los \ $X_n$, y \ $ S_N = \sum_{n=0}^N X_n$,
  %
  \[
  \Phi_{S_N}(\omega) =  G_N \big( \Phi_X(\omega) \big),
  \]
\end{teorema}

Gracia a la  funci\'on caractr\'istica, se muestra tambi\'en  que para un vector
aleatorio $d$-dimensional \ $X$, conocer  la distribuci\'on de cualquier $a^t X$
para $a  \in \Sset_d$  permite conocer la  distribuci\'on de  \ $X$~\cite{Mui82,
  BilBre99, Sas13}:
%
\begin{teorema}[Cram\'er-Wold]\label{Teo:MP:CramerWold}
  Sea \ $X$ \ vector aleatorio $d$-dimensional. La distribuci\'on \ $p_X$ \ de \
  $X$  \  es completamente  determinada  por  el  conjunto de  distribuciones  \
  $\left\{ p_{a^t X},  \: \forall \: a \in  \Sset_d \right\}$  \ de las
  variables \ $a^t X$.
\end{teorema}
%
\begin{proof}
  Por definici\'on  de la funci\'on caracter\'istica, $\forall \: \omega \in \Rset$
  %
  \begin{eqnarray*}
  \Phi_{a^t X}(\omega) & = & \Esp\left[ e^{\imath \, \omega a^t X} \right]\\[2mm]
  %
  & = & \Phi_X(\omega a)
  \end{eqnarray*}
  %
  Se concluye  notando que \ $\left\{  a \omega \tq a  \in \Sset_d, \:
    \omega \in \Rset \right\} = \Rset^d$.
\end{proof}

Un otro resultado  interesante se v\'incula a la noci\'on de  mezcla de escala y
toma la forma siguiente:
%
\begin{teorema}
  Sea \  $X$ \ vector aleatorio de  funci\'on caracter\'istica \ $\Phi_X$  \ y \
  $M$ \ variable aleatoria independiente de  \ $X$ \ y de medida de probabilidad
  \ $P_M$.  Entonces, la funci\'on caracter\'istica  de la mezcla de escala \ $M
  X$ \ es dada por
  %
  \[
  \Phi_{M X}(\omega) = \int_\Rset \Phi_X(m \omega) \, dP_M(m)
  \]
\end{teorema}
%
\begin{proof}
  Por definici\'on  de la funci\'on caracter\'istica, la  f\'ormula de esperanza
  total~\ref{Teo:MP:EsperanzaTotal}                     y                     el
  teorema~\ref{Teo:MP:EsperanzaCondicionalFXY}, se tiene
  %
  \begin{eqnarray*}
  \Phi_{M X}(\omega) & = & \Esp\left[ e^{\imath \, \omega^t M X} \right]\\[2mm]
  %
  & = & \Esp\left[ \Esp\left[ \left. e^{\imath \, \omega^t M X} \right| M \right] \right]\\[2mm]
  %
  & = & \int_\Rset \Esp\left[ \left. e^{\imath \, \omega^t m X} \right| M=m \right] dP_M(m)\\[2mm]
  %
  & = & \int_\Rset \Esp\left[ e^{\imath \, \omega^t m X} \right] dP_M(m)\\[2mm]
  %
  & = & \int_\Rset \Phi_X(m \omega) \, dP_M(m)
  \end{eqnarray*}
\end{proof}
 
De la funci\'on  caracter\'istica (o tambi\'en de la  generadoda de momentos) se
puede probar  resultados sobre la escritura estoc\'astica  de vectores aleaorios
que se va a revelar frecuentement muy \'util:
%
\begin{teorema}\label{Teo:MP:IgualdadDistribucionFuncionVA}
  Sean  dos vectores  aleatorios $d$-dimensional  \ $X$  \ e  \ $Y$  \  de misma
  distribuci\'on  de  probabilidad. Entonces,  para  cualquier  funci\'on \  $f:
  \Rset^d  \to \Rset^{d'}$ \  medible, $f(X)$  \ y  \ $f(Y)$  \ tienen  la misma
  distribuci\'on de probabilidad ($d'$ puede  ser menor, igual o mayor que $d$),
  \ie
  %
  \[
  X  \egald  Y \quad  \Longrightarrow  \quad  f(X) \, \egald \, f(Y)
  \]
\end{teorema}
%
\begin{proof}
  Este resultado se encuentra entre otros en~\cite[aserci\'on~2~p.~13]{FanKot90}
  o \cite{Zol86} y se proba sencillamente por la funci\'on caracter\'istica:
  %
  \[
  \Phi_{f(X)}(\omega)  =  \Esp\left[  e^{\imath  \,  \omega^t  f(X)}  \right]  =
  \int_{\Rset^d}  e^{\imath  \,  \omega^t  f(x)}  \,  dP_X(x)  =  \int_{\Rset^d}
  e^{\imath \, \omega^t f(x)} \, dP_Y(x) = \Phi_{f(Y)}(\omega)
  \]
  %
  Se concluye  del caract\'er uno-uno  entre la funci\'on caracter\'istica  y la
  medida de probabilidad.
\end{proof} 
}

%\SZ{Hablar de la forma en el caso escamar con le funcion quantile?}

%{\teorema (Bochner, Goldberg).... } %%


% ================================= Funcion generadora de momentos

\modif{
\subseccion{Funci\'on generadora de cumulantes y secunda funci\'on caracter\'istica}
\label{Ssec:MP:GeneradoraCumulantes}

A  veces aparece  m\'as comodo  trabajar con  momentos especiales  llamados {\em
  cumulantes}.   Esos fueron  introducidos  por  T.  N.   Thiele  en los  a\~nos
1889~\cite{Thi89,   Thi03,   Cra46,  Hal00,   Hal06}   y   aparecen  bajo   esta
denominaci\'on  en  un  papel  de  R.   Fisher \&  J.   Wishart  cuatro  decadas
despu\'es~\cite{FisWis32}.  Estos  cumulantes aparecen a  trav\'es del logaritmo
de  la  funci\'on   generadora  de  momentos.   Como  lo  vamos   a  ver  en  la
seci\'on~\ref{Ssec:MP:FamiliaExponencial}  tratando de  la  familia exponencial,
aparece que el logarotmo de  la generadora de momentos tiene una significaci\'on
f\'isica,  permitiendo  de  calcular  energias  libres o  internas  en  f\'isica
estadistica  a  trav\'es de  lo  que se  conoce  como  funci\'on de  partici\'on
(ver~\cite{Gib02,           Fis30}           y          m\'as           adelante
secci\'on~\ref{Ssec:MP:FamiliaExponencial}). Se refiera tambi\'en a~\cite{Luk70,
  LacAmb97, KenStu52} para lo que sigue.
% Gibbs 1901, Fisher 1929,
% Hald, Anders (1998). A History of Mathematical Statistics from 1750 to 1930. New York: Wiley.
% ISBN 978-0-471-17912-2.

\begin{definicion}[Generadora de los cumulantes]
  Sea \ $X$ \ vector aleatorio y  \ $M_X(u) = \Esp\left[ e^{u^t X} \right]$ \ su
  generadora de  momentos. Entonces, se  define la funci\'on {\em  generadora de
    los cumulantes} como
  %
  \[
  C_X(u) = \log M_X(u) = \log\left( \Esp\left[ e^{u^t X} \right] \right)
  \]
\end{definicion}
%
Del hecho que $M_X$ es compleja, hace  falta entender el logaritmo como el de un
n\'umero complejo~\cite{AblFok03,  CarKro05}.  De  la continuidad de  $M_X$, con
$M_X(0) = 1$, la generadora de los  momentos es real positiva por lo menos en un
enterno de $u = 0$, as\'i que  la generadora de los cumulantes va a ser definida
por lo menos en un enterno de $u = 0$.

A partir de esta definici\'on, se define los dichos {\em cumulantes} de la misma
manera que para los momentos:
%
\begin{definicion}[Cumulantes]
  Sea  \ $X$  \ vector  aleatorio $d$-dimensional  y $C_X(u)  = \log  M_X(u)$ la
  generadora de  los momentos.  Esta funci\'on  es definida por  lo menos  en un
  entorno  de $u  = 0$,  $C_X(0)  = 0$  \ y  si  admite en  desarollo de  Taylor
  al torno de $u = 0$, se escribe
  %
  \[
  C_X(u)     =    \sum_{k=1}^{+\infty}     \,     \sum_{(i_1,\ldots,i_k)    \in     \{1 , \ldots , d \}^k}
  \kappa_{i_1,\ldots,i_k}[X] \, \frac{u_{i_1} \ldots u_{i_k}}{k!}
  \]
  %
  donde  el  tensor  \ $\kappa_k[X]$  \  de  order  \  $k$  \ y  de  componentes
  $\kappa_{i_1,\ldots,i_k}[X]$ llamados {\em cumulentes},
  %
  \[
  \kappa_{i_1,\ldots,i_k}[X] \, = \, \left.\frac{\partial^k C_X}{\partial u_{i_1}
      \ldots \partial u_{i_k}} \right|_{u = 0}
  \]
  %
  es llamado {\em cumulente de orden $k$ de $X$.}
\end{definicion}

Se podra notar que
%
\begin{itemize}
\item $\kappa_1[X] = \nabla C_X(0) = m_X$ \ media de $X$;
%
\item $\kappa_2[X] = \Hess C_X(0) = \Sigma_X = \zeta_2[X]$ \ covarianza de $X$;
%
\item $\kappa_3[X] = \zeta_3[X]$ \ momento centrado de orden 3;
%
\item Pero, $\forall \: k > 3, \quad \kappa_k[X] \ne \zeta_k[X]$.
\end{itemize}

%Sin embargo, en  el caso escalar \  $d=1$ \ de \ $m_k[X]  = \left. \frac{\partial^k
%    M_X}{\partial     u^k}    \right|_{u=0}    =     \left.     \frac{\partial^k
%    \exp(C_X)}{\partial   u^k}   \right|_{u=0}$   \   o   $\kappa_k[X]   =   \left.
%  \frac{\partial^k C_X}{\partial  u^k} \right|_{u=0} =  \left.  \frac{\partial^k
%    \log(C_X)}{\partial u^k} \right|_{u=0}$. Se relacionan cumulentes y momentos
%de  la  formula  de  Fa\`a   di  Bruno~\footnote{Esta  formula  es  asociada  al
%  matem\'atico italiano Francesco  Fa\`a di Bruno que publico  su formula en los
%  a\~nos 1855, 1857.  Pero fue  probada desde 1800 por el matem\'atico franc\'es
%  Louis Fran\c{c}ois Antoine Arbogast~\cite{Abo00}}  que da las derivadas de una
%funci\'on compuesta~\cite{Faa55,  Faa57} mediante los  polinimios incompletos de
%Bell~\cite{Bel27}, dados para $n \ge k$
%%
%\[
%B_{n,k}(x_1,\ldots,x_{n-k+1})  = n!   \sum_{j  \in D_{n,k}}  \prod_{i=1}^{n-k+1}
%\frac{1}{j_j!} \left( \frac{x_i}{i!} \right)^{j_i}
%\]
%%
%con
%%
%\[
%D_{n,k}  =  \left\{ j  \in  \Nset^{n-k+1} \tq  \sum_{i=1}^{n-k+1}  j_i  = k  \et
%  \sum_{i=1}^{n-k+1} i j_i = n \right\}
%\]
%
%\begin{lema}[Relaci\'on momentos-cumulantes -- caso escalar]
%  Sea  \  $X$ \  variable  aleatoria escalar.  Cuando  existen,  los momentos y
%  cumulantes son relacionados por las formulas
%  %
%  \[\left\{\begin{array}{lll}
%  m_k[X] & = & \displaystyle \sum_{n=1}^k B_{k,n}( \kappa_1[X] , \ldots ,
%  \kappa_{n-k+1}[X])\\[2mm]
%  %
%  \kappa_k[X] & = & \displaystyle \sum_{n=1}^k (-1)^n (n-1)! B_{k,n}( m_1[X] ,
%  \ldots , m_{n-k+1}[X])
%  \end{array}\right.\]
%  %
%  Notando que \ $C_{X-m_X}(u) = -u m_X + C_X(u)$ \ se obtiene inmediatamente
%  %
%  \[\left\{\begin{array}{lll}
%  \zeta_k[X] & = & \displaystyle \sum_{n=1}^k B_{k,n}( 0 , \kappa_2[X] , \ldots ,
%  \kappa_{n-k+1}[X])\\[4mm]
%  %
%  \kappa_k[X] & = & \displaystyle \sum_{n=1}^k (-1)^n (n-1)! B_{k,n}( 0 ,
%  \zeta_2[X] , \ldots , \zeta_{n-k+1}[X])
%  \end{array}\right.\]
%\end{lema}
%
%De hecho,  se podri\'a definir as\'i  de manera {\em ad-hoc},  los cumulantes que
%las generadoras admiten o no desarollos de Taylor en $u = 0$.
%La  generalizaci\'on  en el  caso  multivariado  es  posible, mediante  calculos
%bastante   pesados,  usando   el  mismo   
De hecho, se  puede relacionar momentos y cumulantes con  el enfoque de derivada
de funciones  compuestas~\cite[Teo.~5.1.4]{Sta99} o~\cite{Har06}. Eso  conduce a
las relaciones siguientes (formula  de Leonov y Shiryayev~\cite{LeoShi59, Shi84,
  LacAmb97, Bri01}):
%
\begin{lema}[Relaci\'on momentos-cumulantes]% -- caso general]
%
  Sea \  $X$ \ vector aleatorio.   Cuando existen, los momentos  y cumulantes de
  son relacionados por las formulas
  %
  \[\left\{\begin{array}{lll}
  m_{i_1,\ldots,i_k}[X] & = & \displaystyle \sum_{\pi \in \Pi_k} \prod_{B \in
  \pi} \kappa_{i_B}[X] \\[5mm]
  %
  \kappa_{i_1,\ldots,i_k}[X] & = & \displaystyle \sum_{\pi \in \Pi_k}
  (-1)^{|\pi|-1} \Gamma(|\pi|) \prod_{B \in \pi} m_{i_B}[X]
  \end{array}\right.\]
  %
  donde \  $\Pi_k$ \  es el conjunto  de las  particiones~\footnote{Por ejemplo,
    $\Pi_3  = \Big\{  \big\{ \{1,2,3\}  \big\} \:  , \:  \big\{ \{1,2\}  , \{3\}
    \big\} \: , \: \big\{ \{1,3\} ,  \{2\} \big\} \: , \: \big\{ \{2,3\} , \{1\}
    \big\}$; cuando \ $\pi = \big\{ \{1,2\} , \{3\} \Big\}$, tenemos el producot
    para \ $B = \{1,2\}$ con los indices \  $i_1,i_2$ \ y \ $B = \{3\}$ \ con el
    indice $i_3$.}  de \ $\{  1 ,  \ldots , k  \}$ \ y  \ $m_{j_B}$ \  denota el
  momento de indices $\{ j_l \tq \: l \in B \}$.

  % Notando  que \ $C_X(u) =  u^t m_X +  \log \left( M_{X-m_X}(u) \right)$  \ se
  % obtiene inmediatamente
  Adem\'as, momentos centrales y cumulantes se vinculan bajo la forma
  %
  \[\left\{\begin{array}{lll} \zeta_{i_1,\ldots,i_k}[X] & = & \displaystyle
  \sum_{\pi \in \Pi_k} \prod_{B \in \pi} \Big( \kappa_{i_B}[X] - m_{X_{i_B}}
  \un_{\{1\}}(|B|) \Big) \\[5mm]
  %
  \kappa_{i_1,\ldots,i_k}[X]  &  = &  \displaystyle  m_{X_{i_1}}
  \un_{\{1\}}(k) +  \sum_{\pi \in  \Pi_k}
  (-1)^{|\pi|-1}  \Gamma(|\pi|) \prod_{B \in  \pi} \zeta_{i_B}[X]
  \end{array}\right.\]
  %
\end{lema}
%
\begin{proof}
  Recordamosnos que para \ $k \in \Nset_0, \quad (i_1 , \ldots , i_k) \in \{ 1 ,
  \ldots , d \}^k$,
  %
  \[
  m_{i_1,\ldots,i_k}[X]    =   \left.\frac{\partial^k    M_X}{\partial   u_{i_1}
      \cdots    \partial    u_{i_k}}\right|_{u=0}    \qquad   \mbox{y}    \qquad
  \kappa_{i_1,\ldots,i_k}[X]  =   \left.\frac{\partial^k  C_X}{\partial  u_{i_1}
      \cdots \partial u_{i_k}}\right|_{u=0}
  \]
  %
  Salimos ahora  de la formula de  Hardy~\cite[Prop.~1]{Har06}, generalizando la
  formula  de Fa\`a  di Bruno~\cite{Faa55,  Faa57}~\footnote{Esta formula  da la
    derivada  $k$-\'esima  de  una  funci\'on  compuesta  escalar  mediante  los
    polinimios  incompletos de  Bell~\cite{Bel27}.   La formula  es asociada  al
    matem\'atico italiano Francesco Fa\`a di Bruno que public\'o su f\'ormula en
    los  a\~nos 1855, 1857.   Pero fue  probada desde  1800 por  el matem\'atico
    franc\'es Louis Fran\c{c}ois Antoine Arbogast~\cite{Arb00}.}: Para \ $h(x) =
  f\left( g(u) \right), \quad \forall \:  n \in \Nset_0, \quad \forall \: (i_1 ,
  \ldots , i_n ) \in \{ 1 , \ldots , d \}^n$,
  %
  \[
  \frac{\partial^n h}{\partial u_{i_1} \cdots  \partial u_{i_n}} = \sum_{\pi \in
    \Pi_n} f^{(|\pi|)}\left( g(u) \right) \prod_{B \in \pi} \frac{\partial^{|B|}
    g}{\displaystyle \prod_{j \in B} \partial u_{i_j}}
  \]
  %
  donde \ $f^{(l)}$ \ es la $l$-\'esima derivada de \ $f$. Cuando se lo aplica a \
  $g = C_X$ \  y \ $f(u) = \exp(u)$, dando \  $f^{(n)}(u) = \exp(u)$, se obtiene
  inmediatamente  la   relaci\'on  dando  los  momentos  en   funci\'on  de  los
  cumulantes. Al rev\'es, cuando  se lo aplica a \ $g = M_X$ \  y \ $f(u) = \log
  u$,  dando  \  $f^{(n)}(u)  = \frac{(-1)^{n-1}  \Gamma(n)}{u^n}$,  se  obtiene
  inmediatamente  la  relaci\'on  dando  los  cumulantes  en  funci\'on  de  los
  momentos.

  Para lo de los momentos centrales, se nota simplemente que para \ $k = 1$ \ el
  cumulante y el momento coinciden, y que para $k > 1$ lo cumulantes de $X$ y de
  $X-m_X$ coinciden.
  % que, de $\frac{\partial^n e^{u^t
  %     m_X}$. Reciprocamente,  \ $\frac{\partial \Psi}{\partial  \omega_i}(0) =
  %   \imath m_{X_i}$ \  y \ para \ $n  > 1$, $\frac{\partial^n \Psi_X}{\partial
  %     \omega_{i_1}   \cdots  \partial   \omega_{i_n}}(0)   =  \frac{\partial^n
  %     \log(\Phi_{X-m_X})}{\partial      \omega_{i_1}      \cdots      \partial
  %     \omega_{i_n}}(0)$
\end{proof}
%
% Se notar\'a  que para \ $k = 1$  \ el cumulante y el  momento coinciden, y que
%  para $k  > 1$  lo cumulantes  de $X$  y de  $X-m_X$ coinciden,  as\'i  que se
% vincular\'a  sencillamente cumulantes y momentos centrales  con estas formulas
% Para lo de los momentos centrales, se nota simplemente que para \ $k = 1$ \ el
% cumulante y el momento coinciden, y que para $k > 1$ lo cumulantes de $X$ y de
% $X-m_X$ coinciden.

De hecho, se  podri\'a definir as\'i de manera {\em  ad-hoc} los cumulantes, que
las generadoras admitan o no desarollos de Taylor en $u = 0$.

\

Para cerrar  esta secci\'on, se puede evocar  el hecho que la  generadora de los
momentos no  esta siempre  bien definida sobre  todo $\Cset^d$, mientras  que la
funci\'on  caracter\'istica es  siempre bien  definida. A  pesar de  que  no sea
necesario (el comportamiento  en $u=0$ es importante), se  usa frecuentemente el
logaritmo de la funci\'on caracter\'istica para definir los cumulantes.
%
\begin{definicion}[Secunda funci\'on caracter\'istica]
  Sea  \ $X$  \  vector aleatorio  y  \ $\Phi_X(\omega)  = \Esp\left[  e^{\imath
      \omega^t X}  \right]$ \ su funci\'on caracter\'istica.  Etonces, se define
  la {\em secunda funci\'on caracter\'istica} como
  %
  \[
  \Psi_X(\omega)  =  \log   \Phi_X(\omega)  =  \log\left(  \Esp\left[  e^{\imath
        \omega^t X} \right] \right)
  \]
\end{definicion}
%
De nuevo  del hecho que $\Phi_X$  es compleja, hace falta  entender el logaritmo
como el de  un n\'umero complejo~\cite{AblFok03, CarKro05}, y  de la continuidad
de $\Phi_X$, con $\Phi_X(0) = 1$, la funci\'on caracter\'istica es real positiva
por lo menos en un enterno de $u = 0$, as\'i que $\Psi$ va a ser definida por lo
menos en  un entorno de $u  = 0$. Si admite  un desarollo de  Taylor, se muestra
inmediatamente que los cumulantes satisfacen la relaci\'on siguiente:
%
\begin{lema}[Cumulantes a partir de la secunda funci\'on caracter\'istica]\label{Lem:MP:CumSecFctCarac}
%
  Sea \ $X$ \ vector aleatorio $d$-dimensional y $\Psi_X(u) = \log \Phi_X(u)$ su
  secunda  funci\'on caracter\'istica.  Si  admite en  desarollo de  Taylor, los
  cumulantes de orden $k$ satisfacen
  %
  \[
  \kappa_{i_1,\ldots,i_k}[X]    \,   =   \,    (-\imath)^k   \left.\frac{\partial^k
      \Psi_X}{\partial    \omega_{i_1}    \ldots   \partial    \omega_{i_k}}
  \right|_{\omega   =    0},   \qquad    (i_1 , \ldots , i_k) \in \{ 1 , \ldots , d \}^k
  \]
\end{lema}


Varias propiedades de las  funciones \ $C_X$ \ y \ $\Psi_X$  \ se deducen de las
de  \ $M_X$  \ y  de \  $\Phi_X$. Nos  enfocamos en  la propiedad  relacionana a
combinaciones lineales  de vectores independientes, con  consecuencias sobre los
cumulantes:
%
\begin{teorema}%[Funci\'on generadora de probabilidad de una ]
\label{Teo:MP:PropiedadesGeneradoraCumulantes}
%
  Sean  \   $X$  \  e  \   $Y$  \  dos   vectores  aleatorios  $d$-dimensionales
  independientes,  \ $A$  \ una  matriz de  \ $\M_{d',d}(\Rset)$  \ y  \  $b \in
  \Rset^{d'}$.   Entonces  para  cualquier  $u  \in  \Cset^{d'}$  \  (donde  las
  funciones existen):
  %
  \[
  C_{A  X +  b}(u) =  u^t b  + C_X\left(  A^t u  \right) \qquad  \mbox{y} \qquad
  \Psi_{A  X +  b}(\omega) =  \imath  \, \omega^t  b +  \Psi_X\left( A^t  \omega
  \right)
  \]
  %
  y  para cualquier  $u \in  \Cset^d$ \  o \  $\omega \in  \Rset^d$ \  (donde la
  funciones existen):
  %
  \[
  C_{X+Y}(u)  = C_X(u)  +  C_Y(u) \qquad  \mbox{y}  \qquad \Psi_{X+Y}(\omega)  =
  \Psi_X(\omega) + \Psi_Y(\omega)
  \]
  %
  Las  consecuencias de este  sobre los  cumulantes es  que~\footnote{La primera
    relaci\'on  se generaliza  sencillamente con  $A$ matricial,  pero  para dar
    expresiones compactas,  se necesita introducir  m\'as profundamente calculos
    tensiorales, lo  que va  m\'as all\'a del  enfoque de este  libro.},
  %
  \[
  \forall \: a \in \Rset, \quad \kappa_K[a X] = a^K \kappa_K[X] \qquad \mbox{y}
  \qquad \kappa_K[X+Y] = \kappa_K[X] + \kappa_K[Y]
  \]
\end{teorema}
%
\begin{proof}
  Las relaciones tratanto de \ $C_X$ \ y \ $\Psi_X$ \ son consecuencias directas
  de           los           teoremas~\ref{Teo:MP:PropiedadesGeneradoraMomentos}
  y~\ref{Teo:MP:PropiedadesCaracteristicaMomentos},  tomando el  logaritmo  de \
  $M_X$ o  de $\Phi_X$ respectivamente.  Las relaciones sobre los  cumulantes es
  entonces  consecuencias de  las  de  \ $C_X$  \  (o de  \  $\Psi_X$)  y de  la
  definici\'on  de  los  cumulantes a  trav\'es  derivadas  de  $C_X$\ (o  de  \
  $\Psi_X$).
\end{proof}
%
Se notar\'a que, remarcablemente y contrariamente a los momentos, los cumulantes
de cualquier orden son funciones lineales de variables aleatorias independientes
(para los momentos, vale sol\'o hasta el orden $3$).  }