\seccion{Vectores aleatorios complejos y matrices aleatorias en algunas palabras.}
\label{Sec:MP:VectoresComplejosMatricesAleatorias}

En  varios campos, como  en mec\'anica  cu\'antica, procesamiento  de se\~nales,
estad\'istica, estamos frente a datos complejas o puestas en matrices. A\'un que
se  puede poner en  biyecci\'on \  $\Cset$ \  y \  $\Rset^2$ \  o, similarmente,
$\M_{d,d'}(\Rset)$ \  y \ $\Rset^{d  d'}$, puede parecer m\'as  natural trabajar
conservando la  estructura de los  datos. Adem\'as, permite  frecuentemente usar
escrituras m\'as compactas que pasar por vectores reales.

En  el contexto  complejo por  ejemplo,  en el  marco de  las comunicaciones  se
modulan las  se\~nales en general  en fase y  cuadratura, es decir con  ambos un
seno  y un coseno.   Formalmente, se  puede considerar  modulaciones exponencial
complejas,  dando lugar a  vectires aleatorias.  Es un  ejemplo entre  otros que
motivan  el  estudio  de  vectores aleatorio  a  valores  complejas~\cite{Lap17,
  SchSch03, EriKoi06} .

Con respeto  al caso  de matrices,  los primeros estudios  de tales  matrices se
encuentrand entre  los a\~nos 1928 y 1955~\cite{Wis28,  NeuGol47, Wig55}.  Desde
esta  \'epoca, fueron  estudiados intensivamente  que  sea formalmente  o en  el
contexto de grande dimensi\'on~\cite{MarPas67, BouPas95}.  Sin embargo, trabajar
con  matrices, \ie  conservando la  estructura, implica  ciertas matices  en las
escrituras  de  las  distribuciones  de  probabilidades,  momentos  o  funciones
generadoras.  Los vamos a ver brevemente, dejando el lector a libros o articulos
especializados  como~\cite{AkeBai15,  GupNag99,  AndGui10,  LivNov18,  EdeRao05,
  Meh04, Car83, Daw81, MezSna08, TulVer04} para tener m\'as detalles.
%
% A.K. Gupta,  V.L. Girko, "Multidimensional statistical analysis  and theory of
% random matrices" , VSP (1996)
%

% ================================= Caso complejo

\subseccion{Vectores aleatorios complejos}
\label{Ssec:MP:VAComplejos}

Formalmente, un vector aleatorio complejo se define de la misma manear que en el
caso real:
%
\begin{definicion}[Vector aleatorio complejo]
\label{Def:MP:VectorAleatorioComplejo}
%
  Un vector aleatorio complejo es una funci\'on medible
  %
  \[
  Z: (\Omega,\A,P) \mapsto (\Cset^d,\B(\Cset^d),P_Z).
  \]
  %
  donde  $\B(\Cset^d)$  son  los  borelianos  de  $\Cset^d$,  $\sigma$-\'algebra
  generada  por los  productos  cartesianos  $\big( (-\infty  \;  b_1] +  \imath
  (-\infty \;  c_1] \big) \times \cdots  \times \big( (-\infty \;  b_d] + \imath
  (-\infty \;  c_d] \big)$  y donde  la medida $P_Z$  sobre $\B(\Cset^d)$  es la
  medida im\'agen de $P$. Como en el caso real,
  %
  \[
  (Z \in  B) \equiv  Z^{-1}(B) =  \{ \omega \in  \Omega \tq  Z(\omega) \in  B \}
  \qquad \mbox{y} \qquad P_Z(B) = P(Z \in B).
  \]
\end{definicion}

Sin embargo, se puede poner en biyecci\'on  \ $\Cset^d$ \ y \ $\Rset^{2 d}$ \
de tal manera de que se  puede definir naturalmente un vector complejo aleatorio
a   partir   de   un   vector   aleatorio  real   de   la   manera   alternativa
equivalente~\cite[Cap.~17]{Lap17}:
%
\begin{definicion}[Vector aleatorio complejo -- definici\'on equivalente]
\label{Def:MP:VectorAleatorioComplejoEquivalente}
%
  Un vector aleatorio complejo se define como
  %
  \[
  Z = X + \imath Y
  \]
  %
  donde \  $\widetilde{Z} \equiv  \begin{bmatrix} X\\ Y  \end{bmatrix}$ \  es un
  vector aleatorio  de \  $\Rset^{2 d}$. La  medida de probabilidad  im\'agen es
  entonces
  %
  \[
  P_Z \equiv P_{\widetilde{Z}} = P_{X,Y}
  \]
  %
\end{definicion}

Resuelte de esta definici\'on equivalente los hechos siguientes:
%
\begin{itemize}
\item La  funci\'on de repartici\'on  de $Z$ se  escribe como \ la  funci\'on de
  repartici\'on conjunta de \ $X$ \ e \ $Y$,
  %
  \[
  F_Z \equiv F_{\widetilde{Z}} = F_{X,Y}
  \]
  %
  Notando de  que es una  funci\'on de \  $x$ \ e \  $y$, $F_Z$ \  hace aparecer
  explicitamente ambos \ $z$ \ y \ $z^*$ complejos conjugados.
%
\item   Si  la   medida  \   $P_{\widetilde{Z}}$  \   admite  una   derivada  de
  Radon-Nykod\'ym con respeto a la medida  de Lebesgue sobre \ $\Rset^{2 d}$, se
  define  la  densidad   de  probabilidad  de  \  $Z$  \   como  \  $f_Z  \equiv
  f_{\widetilde{Z}} =  f_{X,Y}$. A partir  de la funci\'on de  repartici\'on, se
  escribe entonces o a trav\'es de  la derivada $(2 d)$-\'esima de $F_{X,Y}$ con
  respeto a las compontentes \ $x_i$ \ e \ $y_i$.
  % NO, LO QU ES COMENTADO ES FALSO! o, de manera equivalente,
  %
  %\[
  %f_Z(z) =  \frac{\partial^{2 d}}{\partial z_1  \cdots \partial z_d  \, \partial
  %  z_1^* \cdots \partial z_d^*}
  %\]
  %
%
\item Los momentos de orden \ $k$ \ siendos definido a partir de las componentes
  de \ $X$ \ y de \ $Y$, se definen tambi\'en bajo la forma
  %
  \[
  m_{i_1  ,  \ldots ,  i_l  \,  ;  \, i'_1  ,  \ldots  , i'_n}[Z]  =  \Esp\left[
    \prod_{j=1}^l Z_{i_j}  \, \prod_{j=1}^n Z_{i'_j}^*  \right] \quad \mbox{con}
  \quad l+n = k, \quad (i_1 , \ldots i_l , i'_1 \ldots i'_n) \in \{ 1 , \ldots , d \}^k
  % \sum_i (k_i + k'_i) = K
  \]
  %
  y similarmente para los momentos centrales  $\zeta_{i_1 , \ldots , i_l \, ; \,
    i'_1 ,  \ldots ,  i'_n}[Z]$ y los  cumulantes a  partir del logaritmo  de la
  funci\'on caracter\'istica por ejemplo (er m\'as adelante).  En particular,
  %
  \begin{itemize}
  \item La media de \ $Z = X + \imath Y$ \ es definida por
    %
    \[
    m_Z = \Esp[Z] = \Esp[X] + \imath \Esp[Y]
    \]
    %
    La  media de  $Z^*$ no  lleva  informaci\'on m\'as  de orden  1 ($m_{Z^*}  =
    \Esp[Z^*] = m_Z^*$).
  %
  \item La matriz de covarianza es definida por
    \[
    \Sigma_Z \equiv \Cov[Z] \equiv \Esp\left[ (Z-m_Z) (Z-m_Z)^\dag \right]
    \]
    %
    con \  $Z^\dag =  (Z^*)^t$ \ el  transconjugado (transpuesta  conjugada, ver
    notaciones).    Fijense  de  que,   volviendo  al   vector  $\widetilde{Z}^t
    = \begin{bmatrix} X^t & Y^t \end{bmatrix}$ tenemos por un lado
    %
    \[
    \Sigma_{\widetilde{Z}}  = \begin{bmatrix}
      \Sigma_X & \Sigma_{X,Y}\\ \Sigma_{X,Y}^t & \Sigma_Y\end{bmatrix}
    \]
    %
    conteniendo todas las convarianzas, y por el otro lado,
    %
    \[
    \Sigma_Z =  \left( \Sigma_X  + \Sigma_Y \right)  - \imath \left(  \Sigma_{X,Y} -
      \Sigma_{X,Y}^t \right)
    \]
    %
    Se puede  ver que  la covarianza de  $Z$ no  contiene todos los  terminos de
    orden  2.  Por  eso, se  define  tambi\'en la  {\em pseudo-covarianza},  sin
    terminos conjugados,
    \[
    \check{\Sigma}_Z \equiv \pCov[Z] \equiv \Esp\left[ (Z-m_Z) (Z-m_Z)^t \right]
    \]
    %
    Ahora, se puede ver que
    %
    \[
    \check{\Sigma}_Z  =  \left( \Sigma_X  -  \Sigma_Y  \right)  + \imath  \left(
      \Sigma_{X,Y} + \Sigma_{X,Y}^t \right)
    \]
    %
    Entonces,  se  recupera  inmediatamente   $\Sigma_X,  \:  \Sigma_Y$  \  y  \
    $\Sigma_{X,Y}$  \  a  partir  de  \ $\Sigma_Z$  \  y  \  $\check{\Sigma}_Z$;
    Claramente,  los  momentos  centrales de  orden  2  son  dados por  ambas  \
    $\Sigma_Z$ \ y \ $\check{\Sigma}_Z$.
  \end{itemize}
  %
  Los momentos  as\'i definidos heriden  naturalmente de las propiedades  de las
  del caso real.
%
\item  Se puede  ver que  \ $\Sigma_Z  \in P_d(\Cset)$  (hermitica semi-definida
  positiva), es decir que \ $\Sigma_Z = \Sigma_Z^\dag$ \ y \ $\forall \, \mu \in
  \Cset,  \quad \mu^\dag  \Sigma_Z \mu  \ge 0$.   Al  rev\'es, $\check{\Sigma}_Z
  \not\in P_d(\Cset)$; esta matriz  es solamente sim\'etrica \ $\check{\Sigma}_Z
  = \check{\Sigma}_Z^t \in S_d(\Cset)$.
%
\item Las generadoras son respectivamente equivalentes a las de $\widetilde{Z}$,
  o  usando   a  la  vez   $Z$  y  $Z^*$.    Por  ejemplo,  para   la  funci\'on
  caracter\'istica, se la puede definir de argumento complejo como
  %
  \[
  \Phi_Z(\omega)  =  \Esp\left[ e^{\imath  \real{\omega^\dag  Z}} \right]  \quad
  \mbox{con} \quad \omega \in \Cset^d
  \]
  %
  (ver  por  ejemplo~\cite[Cap.~17]{Lap17}).   De  nuevo se  define  la  secunda
  funci\'on  caracter\'istica  tomando   el  logaritmo  $\Psi_X(\omega)  =  \log
  \Phi_X(\omega)$.    Las   funciones   generadoras  as\'i   definidas   heriden
  naturalmente  de  las  propiedades  de   las  del  caso  real.   Entre  otros,
  interpretando la funci\'on caracter\'istica como funci\'on de ambos \ $\omega$
  \ y \ $\omega^*$, y derivando como si ser\'ian variables ``independientes'':
  %
  \begin{itemize}
  \item $\Esp[Z]  = - 2 \, \imath  \left. \nabla_{\omega^*} \Phi_Z  \right|_{\omega =
      0}$,
  %
  \item $\Cov[Z] = - 4 \left. \Hess_{\omega^*,\omega} \Phi_Z \right|_{\omega = 0} +
    4 \left. \nabla_{\omega^*}  \Phi_Z \nabla_{\omega}^t \Phi_Z  \right|_{\omega =
      0}$,
  %
  \item  $\pCov[Z] =  - 4 \left.  \Hess_{\omega^*}  \Phi_Z \right|_{\omega  = 0}  +
    4 \left. \nabla_{\omega^*} \Phi_Z \nabla_{\omega^*}^t \Phi_Z \right|_{\omega =
      0}$
  \end{itemize}
  %
  donde $\Hess_{\omega^*,\omega}$  significa que  se diferencia en  $\omega^*$ y
  luego en $\omega$ (o vice-versa).
\end{itemize}

En el  marco de vectores  complejos, aparece una subclase  particular invariante
por rotaci\'on: los vectores circulares.
%
\begin{definicion}[Vector aleatorio complejo circular]\label{Def:MP:VectorAleatorioComplejoCircular}
%
  Un  vector   aleatorio  complejo   \  $Z$  \   es  dicho  {\em   circular}  en
  torno~\footnote{En la literatura, la noci\'on  de circular es dada para $\mu =
    0$~\cite[Def.~24.3.2]{Lap17}, pero  se extiende sin costo  adicional al caso
    de la  definici\'on dada en este libro.}   a un vector $\mu  \in \Cset^d$ si
  para cualquier $\theta \in [0 \; 2 \pi)$,
  %
  \[
  e^{\imath \theta} \left( Z - \mu \right) \, \egald \, Z - \mu
  \]
  %
  donde \ $\egald$ \ significa ``igualdad en distribuci\'on'' (ver notaciones).
\end{definicion}

Los vectores circulares tienen propiedades particulares importantes:
%
\begin{itemize}
\item  Si $Z$  es circular  en torno a  un vector  $\mu$ y  admite una  media,
  entonces
  %
  \[
  m_Z = \Esp[Z] = \mu
  \]
  % 
  Eso  viene del  hecho que  $e^{\imath  \theta} \Esp\left[  Z -  \mu \right]  =
  \Esp\left[  e^{\imath  \theta}  (Z  -  \mu)  \right]  =  \Esp\left[  Z  -  \mu
  \right]$. Entonces,  para cualquier \ $\theta \in  [0 \; 2 \pi),  \: \left(1 -
    e^{\imath \theta} \right) \Esp\left[ Z -  \mu \right] = 0$, lo que proba que
  \ $\Esp\left[ Z - \mu \right] = 0$.
%
\item Si $Z$ es circular en torno a un vector $\mu$ y admite momentos de orden
  2, entonces la pseudo-covarianza es  cero,
  %
  \[
  \check{\Sigma}_Z = \pCov[Z] = 0
  \]
  %
  Recordandose que $m_Z = \mu$, eso  viene del hecho que $e^{2 \imath \theta}
  \Esp\left[ (Z - m_Z) (Z - m_Z)^t \right] = \Esp\left[ \left( e^{\imath \theta}
      (Z - m_Z)  \right) \left( e^{\imath \theta} (Z -  m_Z) \right)^t \right] =
  \Esp\left[ (Z - m_Z) (Z - m_Z)^t \right]$.  Entonces, para cualquier \ $\theta
  \in [0 \; 2 \pi), \: \left(1  - e^{2 \, \imath \theta} \right) \Esp\left[ (Z -
    m_Z) (Z - m_Z)^t  \right] = 0$, lo que cierra la  prueba. La consecuencia es
  que en el contexto circular,
  %
  \[
  \Sigma_X = \Sigma_Y \quad \mbox{y} \quad \Sigma_{X,Y}^t = - \Sigma_{X,Y}
  \]
  %
\end{itemize}

Fijense de que si la pseudo-covarianza  de un vector aleatorio complejo es cero,
eso  no implica de  que el  vector es  circular.  Por  ejemplo, sea  $Z$ tomando
valores sobre  $\Z = \{ 1+\imath \;  1-\imath \; -1+\imath \;  -1-\imath \}$ con
probabilidades   $p  =   \begin{bmatrix}   \frac13  &   \frac14   &  \frac15   &
  \frac{13}{60}  \end{bmatrix}^t$. No  puede  ser circular  porque, por  ejemplo
$e^{\imath \frac{\pi}{4}} Z$ toma sus valores en $\{ \sqrt2 \; -\sqrt2 \; \imath
\sqrt2 \; -\imath \sqrt2 \} \ne \Z$ o, por ejemplo, $e^{\imath \frac{\pi}{2}} Z$
toma   sus  valores   en  $\Z$   pero  con   el  vector   de   probabilidad  $p'
= \begin{bmatrix} \frac15 & \frac13 & \frac{13}{60} & \frac14\end{bmatrix}^t \ne
p$.

Cuando la pseudo-covarianza es cero se dice a veces que el vector es circular al
orden  2.  M\'as  precisamente,  en   la  literatura,  se  usa  la  definici\'on
siguiente~\cite[Def.~17.4.1]{Lap17}:
%
\begin{definicion}[Vector aleatorio complejo propio]\label{Def:MP:VectorAleatorioComplejoPropio}
%
  Un vector aleatorio complejo \ $Z$  \ es dicho {\em propio} (proper en ingles)
  si admite momentos hasta el orden 2 y ambos,
  %
  \[
  \Esp[Z] = 0, \qquad \pCov[Z] = 0
  \]
\end{definicion}
%
Se podr\'ia ampliar  esta definici\'on hablando de vector propio  en torno a un
vector $\mu$, conservando solamente la  nulidad de la pseudo-covarianza. Como lo
vimos tratando de vectores circulares, tenemos la implicaci\'on siguiente:

\begin{teorema}[Circularidad]
\label{Teo:MP:Circularidad}
%
  Sea   \   $Z$ vector alatorio complejo.  Entonces,
%
  \[
  Z \:  \mbox{ circular en torno  a } \:  m \qquad \Longrightarrow \qquad  Z \:
  \mbox{ propio en torno de } \: m
  \]
\end{teorema}

Los vectores propios tienen  propiedades particulares, entre otros las siguientes.

\begin{teorema}[Conservaci\'on del car\'acter propio por transformaci\'on lineal]\label{Teo:MP:PropioLineal}
%
  Sea  $Z$  vector  aleatorio  complejo  propio  de  $\Cset^d$,  entonces,  para
  cualquier matriz $A \in \M_{d',d}(\Cset)$, el vector aleatorio $A Z$ es propio.
\end{teorema}
\begin{proof}
  La prueba es obiva, notando que $\Esp[A Z]  = A \Esp[Z] = 0$ y que $\pCov[A Z]
  = A \pCov[Z] A^t = 0$.
\end{proof}


\begin{teorema}[Car\'acter propio y proyecci\'on]\label{Teo:MP:PropioProy}
%
  Un vector  aleatorio $Z$ complejo de  $\Cset^d$ es propio si  y solamente si para
  cualquier $c \in \Cset^d$, la variable $c^t Z$ es propia.
\end{teorema}
\begin{proof}
  Claramente, de \  $\Esp\left[ c^t Z \right] = c^t \Esp[Z]$  \ y \ $\pCov\left[
    c^t Z \right] = c^t \pCov[Z] c$,  \ tenemos que si $Z$ es propio, \ $\Esp[Z]
  = 0  \: \Rightarrow  \Esp\left[ c^t  Z \right]  = 0$ \  y \  $\pCov[Z] =  0 \:
  \Rightarrow \pCov\left[  c^tZ \right] =  0$. \newline Reciprocamente,  si para
  cualquier $c$ la variable \ $c^t Z$  \ es propia, se puede elegir $d$ vectores
  $c_i^t$ puestas  en una matriz  $C$ invertible. Entonces  $\Esp[C Z] =  0$ por
  hypotesis, lo  que da  $C \Esp[Z] =  0 \:  \Rightarrow \: \Esp[Z]  = 0$  de la
  invertibilidad. De la misma manera, tenemos por hypothesis $\pCov[C Z] = 0$ lo
  que significa que  $C \pCov[Z] C^t = 0  \: \Rightarrow \: \pCov[Z] =  0$ de la
  invertibilidad de $C$.
\end{proof}

%\SZ{
%A real random vector is proper if and only if it is constant. CASI SIEMPRE
%}

% ================================= Caso matricial real

\subseccion{Matrices aleatorias reales}
\label{Ssec:MP:MA}

De la misma manera que se  puede querer trabajar con vectores complejos, a veces
los datos son naturalmente puestas en matrices aleatorias. Por ejemplo, se puede
querer estimar  una matriz de covarianza  a partir de una  secuencia de vectores
aleatorios \  $X_i, \: i =  1, \ldots ,  n$ \ de media  cero y de misma  ley. Un
estimador  natural es  de reemplazar  el  promedio estadistico  por un  promedio
empirico usando las  observaciones/los vectores aleatorios \ $\widehat{\Sigma}_X
= \frac1n \sum_{i=1}^n X_i X_i^t$ \ (en practica, se reemplaza los \ $X_i$ \ por
observaciones/sampleos  \  $x_i$   \  y  se  evalua  entonces   un  sampleo  del
estimador). Claramente \ $\widehat{\Sigma}_X$ \ es aleatoria por construcci\'on,
y tiene naturalmente la estructura de una matriz.

En lo que sigue, nos enfocaremos  en las matrices reales. Para el caso complejo,
se prodr\'a referirse a la subsecci\'on  anterior. Notando que se puede poner en
biyecci\'on \ $\M_{d,d'}(\Rset)$  \ y \ $\Rset^{d d'}$, une  manera de tratar de
matrices  aleatorias \  $X$ \  puede ser  de trabajar  con su  vectorizaci\'on \
$\vec{X} = \begin{bmatrix} X_1^t &  \cdots & X_{d'}^t \end{bmatrix}^t$ \ donde \
$X_i$  \ es  la  \ $i$-\'esima  columna de  \  $X$, las  vectorizaciones de  las
operaciones  matriciales~\cite[Cap.~2]{MagNeu79}  (ver tambi\'en~\cite{NeuWan83,
  Har08}), y  referirse a  la secci\'on tratando  de vectores  aleatorios.  Pero
resulte a veces  m\'as directo conservar la estructura  matricial y trabajar con
esa.

Formalmente, una  matriz aleatorio real se define  de la misma manear  que en el
caso de vectores reales, de la manera siguiente:
%
\begin{definicion}[Matriz aleatoria real]
\label{Def:MP:MatrizAleatorioaReal}
%
  Una matriz aleatoria real es una funci\'on medible
  %
  \[
  X:  (\Omega,\A,P) \mapsto  \left( \M_{d,d'}(\Rset)  ,  \B\left( \M_{d,d'}(\Rset)
    \right) , P_X \right).
  \]
  %
  donde   $\B\left(    \M_{d,d'}(\Rset)   \right)$   son    los   borelianos   de
  $\M_{d,d'}(\Rset)$, $\sigma$-\'algebra  generada por los  productos cartesianos
  $\optimes_{i=1,j=1}^{i=d,j=d'}  \left( -\infty \;  b_{i,j} \right]$  \ y
  donde la  medida \ $P_X$  sobre \ $\B\left(  \M_{d,d'}(\Rset) \right)$ \  es la
  medida im\'agen de $P$. Nuevamente,
  %
  \[
  (X \in  B) \equiv  X^{-1}(B) =  \{ \omega \in  \Omega \tq  X(\omega) \in  B \}
  \qquad \mbox{y} \qquad P_X(B) = P(X \in B).
  \]
\end{definicion}

En lo que sigue, nos vamos  a concentrar sobre dos situaciones particulares: (i)
el  caso general de  matrices de  \ $\M_{d',d}$  y (ii)  el conjunto  de matrices
sim\'etricas  $S_d(\Rset)$ (o  un subconjunto)  que tiene  la  particularidad de
tener componentes  iguales. El el \'ultimo  caso, a veces  resuelte m\'as comodo
tener en  cuenta esta  simetria, es  de decir que  la matriz  tiene a  los m\'as
$\frac{d (d+1)}{2}$ componentes linealmente independientes.



% --------------------------------- Linealmente independiente

\paragraph{Caso general}
%\subsubseccion{Caso general}
%\label{Sssec:MP:MatricesAleatoriasGeneral}

De  manera general,  trabajamos en  el  contexto de  matrices no  necesiaramente
cuadradas,  y  con componentes  potencialemnte  linealmente independientes  (sin
simetria particular).  En este marco  general, $X$ viviendo sobre  $\X \subseteq
\M_{d,d'}(\Rset)$:
%
\begin{itemize}
\item La funci\'on  de repartici\'on \ $F_X$ \ es  la distribuci\'on conjunta de
  las  componentes  $X_{i,j}$.   Si  admite  una  densidad,  se  define  como  \
  $\displaystyle      p_X       =      \frac{\partial^{d      d'} F_X}{\prod_{i=1}^d
    \prod_{j=1}^{d'} \partial x_{i,j}}$.
%
\item Los  momentos y  cumulantes se definen  como en  el caso de  vectores; Por
  ejemplo, todos los momentos de orden $k$ son dados por
  %
  \[
  m_k[X] = \Esp\left[ X^{\otimes k} \right]
  \]
  %
  donde \ $\cdot^{\otimes k}$ \ es \ $k$ \ veces el \modif{producto externo (ver
    notaciones)}, y  similarmente para los  momentos centrales $\zeta_K-k[X]$  y a
  trav\'es  de  la secunda  funci\'on  caracter\'istica  para  los cumulantes  \
  $\kappa_K[X]$ (ver m\'as adelante). En particular,
  %
  \begin{itemize}
  \item La media es definida por
    %
    \[
    m_X = \Esp\left[ X \right]
    \]
  %
  \item  La covarianza  es  definida por
    \[
    \Sigma_X  \equiv \Cov[X]  =  \Esp\left[ (X-m_X)  \otimes  (X-m_X) \right]  =
    \Esp\left[ X \otimes X \right] - m_X \otimes m_X
    \]
    %
    \modif{$\Sigma_X$ es un tensor de  orden $4$ de componentes $\left( \Sigma_X
      \right)_{i,j,k,l} = \Cov[X_{i,j} X_{k,l}]$.}.\newline
    % Tipicamente,  si la  vemos  en bloques  de  tama\~no \  $d  \times d$,  la
    % componente \  $(k,l)$-\'esima del bloc  \ $(i,j)$-\'esima corresponde  a \
    % $\Cov\left[  X_{i,j}  ,   X_{k,l}  \right]$.\newline  
    M\'as generalmente,  tratando de covarianza  etre dos matrices  aleatorias \
    $X$ \ e \ $Y$, se define el tensor covarianza conjunta como
   %
    \[
    \Sigma_{X,Y} \equiv  \Cov\left[ X ,  Y \right] : \Esp\left[  (X-m_X) \otimes
      (Y-m_Y) \right] = \Esp\left[ X \otimes Y \right] - m_X \otimes m_Y
    \]
  \end{itemize}
%
\item  Se puede  escribir  tambi\'en  las funciones  generadoras  con una  forma
  matricial; por  ejemplo, tratando de  la funci\'on caract\'eristica, va  a ser
  una  funci\'on de  $d d'$  variables que  se puede  poner en  una matriz  de \
  $\M_{d,d'}(\Rset)$ \ de tal manera que
  %
  \[
  \Phi_X(\omega) =  \Esp\left[ e^{\imath  \Tr\left( \omega^t X  \right)} \right]
  \quad \mbox{con} \quad \omega \in \M_{d,d'}(\Rset)
  \]
  %
  De nuevo  se define la  secunda funci\'on caracter\'istica tomando  el logaritmo
  $\Psi_X(\omega) = \log \Phi_X(\omega)$.
%
\item Ahora es sencillo ver de que, si existent, se puede recuperar los momentos
  por  diferenciaci\'on   como  en  el  caso  de   vectores,
  %
  \[
  -\imath \left. \frac{\partial \Phi_X}{\partial \omega_{i,j}} \right|_{\omega =
    0} = \Esp[X_{i,j}]
  \]
  %
  o
  %
  \[
  - \left. \frac{\partial^2 \Phi_X}{\partial \omega_{i,j} \partial \omega_{k,l}}
  \right|_{\omega = 0} = \Esp\left[ X_{i,j} X_{k,l} \right]
  \]
  %
  Se  prodr\'ia  referirse a~\cite[Cap.~8]{MagNeu99}  para  usar  las reglas  de
  derivaci\'on matricial  para hacer los calculos  en la mayor\'ia  de los casos
  que   se  encuentran  en   la  literatura   (vamos  a   ver  ejemplos   en  la
  secci\'on~\ref{Sec:MP:EjemplosDistribucionesProb}).
%
\item Al  final, se  notara que si  las columnas  de $X$ son  independientes, se
  factorisa funci\'on caracter\'istica  a partir de las de  cada columna: Sean \
  $X =  \begin{bmatrix} X_{(1)} & \cdots  & X_{(d')} \end{bmatrix}$ \  con los \
  $X_{(i)}$ \  $d$-dimensionales, y \  $\omega = \begin{bmatrix}  \omega_{(1)} &
    \cdots   &  \omega_{(d')}   \end{bmatrix}$   \  con   \  $\omega_{(i)}   \in
  \Rset^d$. Entonces
  %
    \[
    X_{(i)}, \:\: \mbox{independientes} \qquad \Rightarrow \qquad \Phi_X(\omega)
    = \prod_{i=1}^{d'} \Phi_{X_{(i)}}(\omega_{(i)})
  \]
  %
  Es inmediato de $e^{\imath \,  \Tr(\omega^t X)} = e^{\imath \, \sum_{i=1}^{d'}
    \omega_{(i)}^t  X_{(i)}}  =  \prod_{i=1}^{d'}  e^{\imath  \,  \omega_{(i)}^t
    X_{(i)}}$       \        y       de       la        independencia       (ver
  teorema~\ref{Teo:MP:IndependenciaMomentos}                                    o
  teorema~\ref{Teo:MP:PropiedadesFuncionCaracteristica}).
%
\end{itemize}



% --------------------------------- Simetricas

\paragraph{Caso sim\'etrico}
%\subsubseccion{Caso sim\'etrico}
%\label{Sssec:MP:MatricesAleatoriasSimetrico}

En  el contexto  sim\'etrico,  \ie el  espacio  de llegada  es  \ $\X  \subseteq
S_d(\Rset)$ (ej.   el cono $P_d^+(\Rset)$), aparece  que por lo  m\'as la matriz
tiene  $\frac{d (d+1)}{2}$  \  componentes linealmente  independientes. A  veces
resuelte m\'as  comodo definir  la funci\'on caracter\'istica  en este  caso con
$\omega \in  S_d(\Rset)$ para  respectar las simetrias  del problema y  no tener
ninguna degenerencia  de esa misma (ver por  ejemplo~\cite{PedRic91, And03}).  A
veces, es a\'un  dificil o imposible calcular en  todo \ $\M_{d,d}(\Rset)$. Eso
tiene consecuencias:
%
\begin{itemize}
\item Si existen,  se puede recuperar los momentos  por diferenciaci\'on como en
  el caso de  vectores, pero hay que tener  en cuenta el hecho  que  si $i \ne
  j$, $\omega_{i,j} = \omega_{j,i}$ aparece dos veces en $\omega$. Entonces, por
  ejemplo, se puede ver inmediatamente que
  %
  \[
  -\imath \left. \frac{\partial \Phi_X}{\partial \omega_{i,j}} \right|_{\omega =
    0} = \left( 2 - \un_{\{i\}}(j) \right) \Esp[X_{i,j}]
  \]
  %
  o que
  %
  \[
  - \left. \frac{\partial^2 \Phi_X}{\partial \omega_{i,j} \partial \omega_{k,l}}
  \right|_{\omega  =  0}  =  \left(  2  -  \un_{\{i\}}(j)  \right)  \left(  2  -
    \un_{\{l\}}(k) \right) \Esp\left[ X_{i,j} X_{k,l} \right]
  \]
\end{itemize}
