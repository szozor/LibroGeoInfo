\seccion{Vectores aleatorios complejos y matrices aleatorias en algunas palabras.}
\label{Sec:MP:VectoresComplejosMatricesAleatorias}

En  varios  campos,  como  en  mecanica cuantica,  procesamiento  de  se\~nales,
estadistica, estamos frente  a datos complejas o puestas  en matrices. A\'un que
se  puede poner en  biyecci\'on \  $\Cset$ \  y \  $\Rset^2$ \  o, similarmente,
$\M_{d,d'}(\Rset)$ \  y \  $\Rset^{d d'}$, puede  parecer m\'as  natural trabajar
conservando la  estructura de los  datos. Adem\'as, permite  frecuentemente usar
escrituras  m\'as  compactas que  pasar  por  vectores  reales.  Conservando  la
estructura implica  ciertas matices en  las escrituras de las  distribuciones de
probabilidades, momentos  o funciones generadoras.  Los vamos  a ver brevemente,
dejando el lector a  libros especializados como~\cite{GupNag99} para tener m\'as
detalles.

% ================================= Caso complejo

\subseccion{Vectores aleatorios complejos}
\label{Ssec:MP:VAComplejos}

Formalmente, un vector aleatorio complejo se define de la misma manear que en el
caso real, de la manera siguiente:
%
\begin{definicion}[Vector aleatorio complejo]
\label{Def:MP:VectorAleatorioComplejo}
%
  Un vector aleatorio complejo es una funci\'on medible
  %
  \[
  Z: (\Omega,\A,P) \mapsto (\Cset^d,\B(\Cset^d),P_Z).
  \]
  %
  donde  $\B(\Cset^d)$  son  los  borelianos  de  $\Cset^d$,  $\sigma$-\'algebra
  generada por los productos cartesianos  $(-\infty \; b_1] + \imath (-\infty \;
  c_1] \times \cdots \times (-\infty \;  b_d] + \imath (-\infty \; c_d]$ y donde
  la medida $P_Z$  sobre $\B(\Cset^d)$ es la medida im\'agen de  $P$. Como en el
  caso real,
  %
  \[
  (Z \in  B) \equiv  Z^{-1}(B) =  \{ \omega \in  \Omega \tq  Z(\omega) \in  B \}
  \qquad \mbox{y} \qquad P_Z(B) = P(Z \in B).
  \]
\end{definicion}

Sin embargo, se puede poner en biyecci\'on  \ $\Cset^d$ \ y \ $\Rset^{2 d}$ \
de tal manera de que se  puede definir naturalmente un vector complejo aleatorio
a   partir   de   un   vector   aleatorio  real   de   la   manera   alternativa
equivalente~\cite[Cap.~17]{Lap17}:
%
\begin{definicion}[Vector aleatorio complejo -- definici\'on equivalente]
\label{Def:MP:VectorAleatorioComplejoEquivalente}
%
  Un vector aleatorio complejo se define como
  %
  \[
  Z = X + \imath Y
  \]
  %
  donde \  $\widetilde{Z} \equiv  \begin{bmatrix} X\\ Y  \end{bmatrix}$ \  es un
  vector aleatorio de \ $\Rset^{2 \,  d}$. La medida de probabilidad im\'agen es
  entonces
  %
  \[
  P_Z \equiv P_{\widetilde{Z}} = P_{X,Y}
  \]
  %
\end{definicion}

Resuelte de esta definici\'on equivalente los hechos siguientes:
%
\begin{itemize}
\item La  funci\'on de repartici\'on  de $Z$ se  escribe como \ la  funci\'on de
  repartici\'on conjunta de \ $X$ \ e \ $Y$,
  %
  \[
  F_Z \equiv F_{\widetilde{Z}} = F_{X,Y}
  \]
  %
  Notando de  que es una  funci\'on de \  $x$ \ e \  $y$, $F_Z$ \  hace aparecer
  explicitamente ambos \ $z$ \ y \ $z^*$ complejos conjugados.
%
\item   Si  la   medida  \   $P_{\widetilde{Z}}$  \   admite  una   derivada  de
  Radon-Nykod\'ym con respeto a la medida  de Lebesgue sobre \ $\Rset^{2 \, d}$,
  se  define  la  densidad de  probabilidad  de  \  $Z$  \  como \  $f_Z  \equiv
  f_{\widetilde{Z}} =  f_{X,Y}$. A partir  de la funci\'on de  repartici\'on, se
  escribe entonces o a trav\'es de la derivada $2 \, d$-\'esima de $F_{X,Y}$ con
  respeto a las compontentes \ $x_i$ \ e \ $y_i$ o, de manera equivalente,
  %
  \[
  f_Z(z) = \frac{\partial^{2 \, d}}{\partial z_1 \cdots \partial z_d \, \partial
    z_1^* \cdots \partial z_d^*}
  \]
  %
%
\item Los momentos de orden \ $K$ \ siendos definido a partir de las componentes
  de \ $X$ \ y de \ $Y$, se definen tambi\'en bajo la forma
  %
  \[
  m_{k_1 , \ldots , k_d \, ; \, k'_1 , \ldots , k'_d} = \Esp\left[ \prod_{i=1}^d
    Z_i^{k_i}  \,  \prod_{i=1}^d   Z_i^{* \: k'_i}  \right]  \quad
  \mbox{con} \quad \sum_i (k_i + k'_i) = K
  \]
  %
  y similarmente para los momentos centrales  $\zeta_{k_1 , \ldots , k_d \, ; \,
    k'_1 , \ldots , k'_d}$.  En particular,
  %
  \begin{itemize}
  \item La media de \ $Z = X + \imath Y$ \ es definida por
    %
    \[
    m_Z = \Esp[Z] = \Esp[X] + \imath \Esp[Y]
    \]
    %
    La media de $Z^*$ no lleva informaci\'on m\'as de orden 1.
  %
  \item La matriz de covarianza es definida por
    \[
    \Sigma_Z \equiv \Cov[Z] \equiv \Esp\left[ (Z-m_Z) (Z-m_Z)^\dag \right]
    \]
    %
    donde  \  $Z^\dag  =  (Z^*)^t$  \ dicho  {\em  transconjugado}  (transpuesta
    conjugada).    Fijense  de   que,  volviendo   al   vector  $\widetilde{Z}^t
    = \begin{bmatrix} X^t & Y^t \end{bmatrix}$ tenemos por un lado
    %
    \[
    \Sigma_{\widetilde{Z}}  = \begin{bmatrix}
      \Sigma_X & \Sigma_{X,Y}\\ \Sigma_{X,Y}^t & \Sigma_Y\end{bmatrix}
    \]
    %
    conteniendo todas las convarianzas, y por el otro lado,
    %
    \[
    \Sigma_Z =  \left( \Sigma_X  + \Sigma_Y \right)  - \imath \left(  \Sigma_{X,Y} -
      \Sigma_{X,Y}^t \right)
    \]
%    \SZ{Decir  antes,  que $\Sigma_{X,Y}  \equiv  \Cov[X,Y]$  y $\Sigma_{Y,X}  =
%      \Sigma_{X,Y}^t$}.  
Se puede ver que la covarianza de $Z$ no contiene todos
    los  terminos   de  orden   2.  Por  eso,   se  define  tambi\'en   la  {\em
      pseudo-covarianza}, sin terminos conjugados,
    \[
    \check{\Sigma}_Z \equiv \pCov[Z] \equiv \Esp\left[ (Z-m_Z) (Z-m_Z)^t \right]
    \]
    %
    Ahora, se puede ver que
    %
    \[
    \check{\Sigma}_Z =  \left( \Sigma_X  - \Sigma_Y \right)  + \imath \left(  \Sigma_{X,Y} + 
      \Sigma_{X,Y}^t \right)
    \]
    %
    Entonces,  se  recupera  inmediatamente   $\Sigma_X,  \:  \Sigma_Y$  \  y  \
    $\Sigma_{X,Y}$  \  a  partir  de  \ $\Sigma_Z$  \  y  \  $\check{\Sigma}_Z$;
    Claramente,   los   momentos   centrales   de   orden  2   son   dados   por
    \underline{ambas} \ $\Sigma_Z$ \ y \ $\check{\Sigma}_Z$.
  \end{itemize}
  %
  Los momentos  as\'i definidos heriden  naturalmente de las propiedades  de las
  del caso real.
%
\item  Se puede  ver que  \ $\Sigma_Z  \in P_d(\Cset)$  (hermitica semi-definida
  positiva), es decir que \ $\Sigma_Z = \Sigma_Z^\dag$ \ y \ $\forall \, \mu \in
  \Cset,  \quad \mu^\dag  \Sigma_Z \mu  \ge 0$.   Al  rev\'es, $\check{\Sigma}_Z
  \not\in P_d(\Cset)$; esta matriz  es solamente sim\'etrica \ $\check{\Sigma}_Z
  = \check{\Sigma}_Z^t \in S_d(\Cset)$.
%
\item Las generadoras son respectivamente equivalentes a las de $\widetilde{Z}$,
  o  usando   a  la  vez   $Z$  y  $Z^*$.    Por  ejemplo,  para   la  funci\'on
  caracter\'istica, se la puede definir de argumento complejo como
  %
  \[
  \Phi_Z(\omega)  =  \Esp\left[ e^{\imath  \real{\omega^\dag  Z}} \right]  \quad
  \mbox{con} \quad \omega \in \Cset^d
  \]
  %
  (ver  por  ejemplo~\cite[Cap.~17]{Lap17}).   Las funciones  generadoras  as\'i
  definidas heriden naturalmente de las  propiedades de las del caso real. Entre
  otros, interpretando  la funci\'on caracter\'istica como funci\'on  de ambos \
  $\omega$ \ y \ $\omega^*$, y derivando como si ser\'ian variables ``independientes'':
  %
  \begin{itemize}
  \item $\Esp[Z]  = - 2 \, \imath  \left. \nabla_{\omega^*} \Phi_Z  \right|_{\omega =
      0}$,
  %
  \item $\Cov[Z] = - 4 \left. \Hess_{\omega^*,\omega} \Phi_Z \right|_{\omega = 0} +
    4 \left. \nabla_{\omega^*}  \Phi_Z \nabla_{\omega}^t \Phi_Z  \right|_{\omega =
      0}$,
  %
  \item  $\pCov[Z] =  - 4 \left.  \Hess_{\omega^*}  \Phi_Z \right|_{\omega  = 0}  +
    4 \left. \nabla_{\omega^*} \Phi_Z \nabla_{\omega^*}^t \Phi_Z \right|_{\omega =
      0}$
  \end{itemize}
\end{itemize}

En el  marco de vectores  complejos, aparece una subclase  particular invariante
por rotaci\'on, lo qu es conocido como vectores circulares:
%
\begin{definicion}[Vector aleatorio complejo circular]\label{Def:MP:VectorAleatorioComplejoCircular}
%
  Un  vector   aleatorio  complejo   \  $Z$  \   es  dicho  {\em   circular}  en
  torno~\footnote{En la literatura, la noci\'on  de circular es dada para $\mu =
    0$~\cite[Def.~24.3.2]{Lap17}, pero  se extiende sin costo  adicional al caso
    de la  definici\'on dada en este libro.}   a un vector $\mu  \in \Cset^d$ si
  para cualquier $\theta \in [0 \; 2 \pi)$,
  %
  \[
  e^{\imath \theta} \left( Z - \mu \right) \, \egald \, Z - \mu
  \]
  %
  donde \ $\egald$ \ significa ``igualdad en distribuci\'on'' (ver notaciones).
\end{definicion}

Los vectores circular tienen propiedades particulares importantes
%
\begin{itemize}
\item  Si $Z$  es circular  al torno  de  un vector  $\mu$ y  admite una  media,
  entonces
  %
  \[
  m_Z = \Esp[Z] = \mu
  \]
  % 
  Eso viene  del hecho de  que $e^{\imath \theta}  \Esp\left[ Z - \mu  \right] =
  \Esp\left[  e^{\imath  \theta}  (Z  -  \mu)  \right]  =  \Esp\left[  Z  -  \mu
  \right]$. Entonces,  para cualquier \ $\theta \in  [0 \; 2 \pi),  \: \left(1 -
    e^{\imath \theta} \right) \Esp\left[ Z -  \mu \right] = 0$, lo que proba que
  \ $\Esp\left[ Z - \mu \right] = 0$.
%
\item Si $Z$ es circular al torno  de un vector $\mu$ y admite momentos de orden
  2, entonces la pseudo-covarianza es  nula,
  %
  \[
  \check{\Sigma}_Z = \pCov[Z] = 0
  \]
  %
  Recordandose que $m_Z = \mu$, eso  viene del hecho de que $e^{2 \imath \theta}
  \Esp\left[ (Z - m_Z) (Z - m_Z)^t \right] = \Esp\left[ \left( e^{\imath \theta}
      (Z - m_Z)  \right) \left( e^{\imath \theta} (Z -  m_Z) \right)^t \right] =
  \Esp\left[ (Z - m_Z) (Z - m_Z)^t \right]$.  Entonces, para cualquier \ $\theta
  \in [0 \; 2 \pi), \: \left(1  - e^{2 \, \imath \theta} \right) \Esp\left[ (Z -
    m_Z) (Z - m_Z)^t  \right] = 0$, lo que cierra la  prueba. La consecuencia es
  que en el contexto circular,
  %
  \[
  \Sigma_X = \Sigma_Y \quad \mbox{y} \quad \Sigma_{X,Y}^t = - \Sigma_{X,Y}
  \]
  %
\end{itemize}

Fijense de que si la pseudo-covarianza  de un vector aleatorio complejo es nula,
eso  no implica de  que el  vector es  circular.  Por  ejemplo, sea  $Z$ tomando
valores sobre  $\Z = \{ 1+\imath \;  1-\imath \; -1+\imath \;  -1-\imath \}$ con
probabilidades   $p  =   \begin{bmatrix}   \frac13  &   \frac14   &  \frac15   &
  \frac{13}{60}  \end{bmatrix}^t$. No  puede  ser circular  porque, por  ejemplo
$e^{\imath \frac{\pi}{4}} Z$ toma sus valores en $\{ \sqrt2 \; -\sqrt2 \; \imath
\sqrt2 \; -\imath \sqrt2 \} \ne \Z$ o, por ejemplo, $e^{\imath \frac{\pi}{2}} Z$
toma   sus  valores   en  $\Z$   pero  con   el  vector   de   probabilidad  $p'
= \begin{bmatrix} \frac15 & \frac13 & \frac{13}{60} & \frac14\end{bmatrix}^t \ne
p$.

Cuando la pseudo-covarianza es nula se dice a veces que el vector es circular al
orden  2.  M\'as  precisamente,  en   la  literatura,  se  usa  la  definici\'on
siguiente~\cite[Def.~17.4.1]{Lap17}:
%
\begin{definicion}[Vector aleatorio complejo propio]\label{Def:MP:VectorAleatorioComplejoPropio}
%
  Un vector aleatorio complejo \ $Z$  \ es dicho {\em propio} (proper en ingles)
  si admite momentos hasta el orden 2 y ambos,
  %
  \[
  \Esp[Z] = 0, \qquad \pCov[Z] = 0
  \]
\end{definicion}
%
Se podr\'ia ampliar  esta definici\'on hablando de vector propio  al torno de un
vector $\mu$, conservando solamente la nulidad de la pseudo-covarianza.

Los vectores propios tienen  propiedades particulares, entre otros las siguientes.

\begin{teorema}[Conservaci\'on del caracter propio por transformaci\'on lineal]\label{Teo:MP:PropioLineal}
%
  Sea  $Z$  vector  aleatorio  complejo  propio  de  $\Cset^d$,  entonces,  para
  cualquier matriz $A \in \M_{d',d}(\Cset)$, el vector aleatorio $A Z$ es propio.
\end{teorema}
\begin{proof}
  La prueba es obiva, notando que $\Esp[A Z]  = A \Esp[Z] = 0$ y que $\pCov[A Z]
  = A \pCov[Z] A^t = 0$.
\end{proof}


\begin{teorema}[Caracter propio y proyecci\'on]\label{Teo:MP:PropioProy}
%
  Un vector  aleatorio $Z$ complejo de  $\Cset^d$ es propio si  y solamente para
  cualquier $c \in \Cset^d$, la variable $c^t Z$ es propia.
\end{teorema}
\begin{proof}
  Claramente, de \  $\Esp\left[ c^t Z \right] = c^t \Esp[Z]$  \ y \ $\pCov\left[
    c^t Z \right] = c^t \pCov[Z] c$ \ tenemos que si $Z$ es propio, \ $\Esp[Z] =
  0  \: \Rightarrow  \Esp\left[ c^t  Z  \right] =  0$ \  y  \ $\pCov[Z]  = 0  \:
  \Rightarrow \pCov\left[  c^tZ \right] =  0$. \newline Reciprocamente,  si para
  cualquier $c$ la variable \ $c^t Z$  \ es propia, se puede elegir $d$ vectores
  $c_i^t$ puestas  en una matriz  $C$ invertible. Entonces  $\Esp[C Z] =  0$ por
  hypotesis, lo  que da  $C \Esp[Z] =  0 \:  \Rightarrow \: \Esp[Z]  = 0$  de la
  invertibilidad. De la misma manera, tenemos por hypothesis $\pCov[C Z] = 0$ lo
  que significa que  $C \pCov[Z] C^t = 0  \: \Rightarrow \: \pCov[Z] =  0$ de la
  invertibilidad de $C$.
\end{proof}

%\SZ{
%A real random vector is proper if and only if it is constant. CASI SIEMPRE
%}

% ================================= Caso matricial

\subseccion{Matrices aleatorias}
\label{Ssec:MP:MA}

De la misma manera que se  puede querer trabajar con vectores complejos, a veces
los datos son naturalmente puestas en matrices aleatorias. Por ejemplo, se puede
querer estimar  una matriz de covarianza  a partir de una  secuencia de vectores
aleatorios \  $X_i, \: i =  1, \ldots ,  n$ \ de media  nula y de misma  ley. Un
estimador  natural es  de reemplazar  el  promedio estadistico  por un  promedio
empirico usando las  observaciones/los vectores aleatorios \ $\widehat{\Sigma}_X
= \frac1n \sum_{i=1}^n X_i X_i^t$ \ (en practica, se reemplaza los \ $X_i$ \ por
observaciones/sampleos  \  $x_i$   \  y  se  evalua  entonces   un  sampleo  del
estimador). Claramente \ $\widehat{\Sigma}_X$ \ es aleatoria por construcci\'on,
y tiene naturalmente la estructura de una matriz.

En lo que sigue, nos enfocaremos  en las matrices reales. Para el caso complejo,
se prodr\'a referirse a la subsecci\'on  anterior. Notando que se puede poner en
biyecci\'on \ $\M_{d,d'}(\Rset)$  \ y \ $\Rset^{d d'}$, une  manera de tratar de
matrices  aleatorias \  $X$ \  puede ser  de trabajar  con su  vectorizaci\'on \
$\vec{X} = \begin{bmatrix} X_1^t &  \cdots & X_{d'}^t \end{bmatrix}^t$ \ donde \
$X_i$  \ es  la  \ $i$-\'esima  columna de  \  $X$, las  vectorizaciones de  las
operaciones  matriciales~\cite[Cap.~2]{MagNeu79}  (ver tambi\'en~\cite{NeuWan83,
  Har08}), y  referirse a  la secci\'on tratando  de vectores  aleatorios.  Pero
resulte a veces  m\'as directo conservar la estructura  matricial y trabajar con
esa.

Formalmente, una  matriz aleatorio real se define  de la misma manear  que en el
caso de vectores reales, de la manera siguiente:
%
\begin{definicion}[Matriz aleatoria real]
\label{Def:MP:MatrizAleatorioaReal}
%
  Una matriz aleatoria real es una funci\'on medible
  %
  \[
  X:  (\Omega,\A,P) \mapsto  \left( \M_{d,d'}(\Rset)  ,  \B\left( \M_{d,d'}(\Rset)
    \right) , P_X \right).
  \]
  %
  donde   $\B\left(    \M_{d,d'}(\Rset)   \right)$   son    los   borelianos   de
  $\M_{d,d'}(\Rset)$, $\sigma$-\'algebra  generada por los  productos cartesianos
  $\optimes_{i=1,j=1}^{i=d,j=d'}  \left( -\infty \;  b_{i,j} \right]$  \ y
  donde la  medida \ $P_X$  sobre \ $\B\left(  \M_{d,d'}(\Rset) \right)$ \  es la
  medida im\'agen de $P$. Nuevamente,
  %
  \[
  (X \in  B) \equiv  X^{-1}(B) =  \{ \omega \in  \Omega \tq  X(\omega) \in  B \}
  \qquad \mbox{y} \qquad P_X(B) = P(X \in B).
  \]
\end{definicion}

En lo que sigue, nos vamos  a concentrar sobre dos situaciones particulares: (i)
el  caso general de  matrices de  \ $\M_{d',d}$  y (ii)  el conjunto  de matrices
sim\'etricas  $S_d(\Rset)$ (o  un subconjunto)  que tiene  la  particularidad de
tener componentes  iguales. El el \'ultimo  caso, a veces  resuelte m\'as comodo
tener en  cuenta esta  simetria, es  de decir que  la matriz  tiene a  los m\'as
$\frac{d (d+1)}{2}$ componentes linealmente independientes.



% --------------------------------- Linealmente independiente


\subsubseccion{Caso general}
\label{Sssec:MP:MatricesAleatoriasGeneral}

De  manera general,  trabajamos en  el  contexto de  matrices no  necesiaramente
cuadradas,  y  con componentes  potencialemnte  linealmente independientes  (sin
simetria particular).  En este marco  general, $X$ viviendo sobre  $\X \subseteq
\M_{d,d'}(\Rset)$:
%
\begin{itemize}
\item La funci\'on  de repartici\'on \ $F_X$ \ es  la distribuci\'on conjunta de
  las  componentes  $X_{i,j}$.   Si  admite  una  densidad,  se  define  como  \
  $\displaystyle      p_X       =      \frac{\partial^{d      d'} F_X}{\prod_{i=1}^d
    \prod_{j=1}^{d'} \partial x_{i,j}}$.
%
\item Los  momentos se definen como en  el caso de vectores;  Por ejemplo, todos
  los momentos de orden $K$ son dados por
  %
  \[
  m_K = \Esp\left[ X^{\otimes K} \right]
  \]
  %
  donde \  $\cdot^{\otimes K}$ \ es  \ $K$ \  veces el producto de  Kronecker, y
  similarmente para los momentos centrales $\zeta_K$. En particular,
  %
  \begin{itemize}
  \item La media es definida por
    %
    \[
    m_X = \Esp\left[ X \right]
    \]
  %
  \item  La covarianza  es  definida por
    \[
    \Sigma_X  \equiv \Cov[X]  =  \Esp\left[ (X-m_X)  \otimes  (X-m_X) \right]  =
    \Esp\left[ X \otimes X \right] - m_X \otimes m_X
    \]
    %
    Tipicamente,  si  la  vemos en  bloques  de  tama\~no  \  $d \times  d$,  la
    componente  \ $(k,l)$-\'esima  del bloc  \ $(i,j)$-\'esima  corresponde  a \
    $\Cov\left[ X_{i,j} , X_{k,l} \right]$.\newline M\'as generalmente, tratando
    de covarianza  etre dos matrices aleatorias  \ $X$ \  e \ $Y$, se  define la
    matriz de covarianza conjunta como
   %
    \[
    \Sigma_{X,Y} \equiv  \Cov\left[ X ,  Y \right] : \Esp\left[  (X-m_X) \otimes
      (Y-m_Y) \right] = \Esp\left[ X \otimes Y \right] - m_X \otimes m_Y
    \]
  \end{itemize}
%
\item  Se puede  escribir  tambi\'en  las funciones  generadoras  con una  forma
  matricial; por  ejemplo, tratando de  la funci\'on caract\'eristica, va  a ser
  una  funci\'on de  $d d'$  variables que  se puede  poner en  una matriz  de \
  $\M_{d,d'}(\Rset)$ \ de tal manera que
  %
  \[
  \Phi_X(\omega) =  \Esp\left[ e^{\imath  \Tr\left( \omega^t X  \right)} \right]
  \quad \mbox{con} \quad \omega \in \M_{d,d'}(\Rset)
  \]
%
\item Ahora es sencillo ver de que, si existent, se puede recuperar los momentos
  por  diferenciaci\'on   como  en  el  caso  de   vectores,
  %
  \[
  -\imath \left. \frac{\partial \Phi_X}{\partial \omega_{i,j}} \right|_{\omega =
    0} = \Esp[X_{i,j}]
  \]
  %
  o
  %
  \[
  - \left. \frac{\partial^2 \Phi_X}{\partial \omega_{i,j} \partial \omega_{k,l}}
  \right|_{\omega = 0} = \Esp\left[ X_{i,j} X_{k,l} \right]
  \]
  %
  Se  prodr\'ia  referirse a~\cite[Cap.~8]{MagNeu99}  para  usar  las reglas  de
  derivaci\'on matricial  para hacer los calculos  en la mayor\'ia  de los casos
  que   se  encuentran  en   la  literatura   (vamos  a   ver  ejemplos   en  la
  secci\'on~\ref{Sec:MP:EjemplosDistribucionesProb}).
\end{itemize}



% --------------------------------- Simetricas


\subsubseccion{Caso sim\'etrico}
\label{Sssec:MP:MatricesAleatoriasSimetrico}

En  el  contexto  sim\'etrico, \ie  el  espacio  de  llegada  es \  $\X  \subseteq
S_d(\Rset)$ (ej.   el cono $P_d^+(\Rset)$), aparece  que por lo  m\'as la matriz
tiene  $\frac{d (d+1)}{2}$  \  componentes linealmente  independientes. A  veces
resuelte m\'as  comodo definir  la funci\'on caracter\'istica  en este  caso con
$\omega \in  S_d(\Rset)$ para  respectar las simetrias  del problema y  no tener
ninguna degenrencia  de esa misma  (ver por ejemplo~\cite{PedRic91,  And03}).  A
veces, es a\'un  dificil o imposible calcular en  $\\M_{d,d}(\Rset)$ entero. Eso
tiene consecuencias:
%
\begin{itemize}
\item Si existen,  se puede recuperar los momentos  por diferenciaci\'on como en
  el caso de  vectores, pero hay que tener  en cuenta el hecho de que  si $i \ne
  j$, $\omega_{i,j} = \omega_{j,i}$ aparece dos veces en $\omega$. Entonces, por
  ejemplo, se puede ver inmediatamente que
  %
  \[
  -\imath \left. \frac{\partial \Phi_X}{\partial \omega_{i,j}} \right|_{\omega =
    0} = \left( 2 - \un_{\{i\}}(j) \right) \Esp[X_{i,j}]
  \]
  %
  o que
  %
  \[
  - \left. \frac{\partial^2 \Phi_X}{\partial \omega_{i,j} \partial \omega_{k,l}}
  \right|_{\omega  =  0}  =  \left(  2  -  \un_{\{i\}}(j)  \right)  \left(  2  -
    \un_{\{l\}}(k) \right) \Esp\left[ X_{i,j} X_{k,l} \right]
  \]
\end{itemize}

\SZ{Descomposici\'on de Bartlett, versi\'on de la descomposici\'on de Cholesky al caso de Wishart. Viene de que si $M \in \P_d(\Kset)$, existe una matriz triangular inferior $L$ tal que $M = L L^\dag$\cite[Teo.~14.5.11]{Har67}}