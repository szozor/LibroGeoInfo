% --------------------------------- Familia  exponencial
\subseccion{Familia exponencial}
\label{Ssec:MP:FamiliaExponencial}

% Familia exponencial:
% --------------------
% Kay 1993, p. 110 & 124
% Lehman & Casela 1998, p. 23
% Kotz & Balakrishnan 2000, p. 659
% Robert 2007, p. 115
% van den Bos 2007, p. 30
% Cencov 1982, p. 245, 279
% Ibarola Perez 2012, p. 174
% Mukhopadhyay 2000, p. 141


% Estadistica suficiente:
% -----------------------
% Kay 1993, p. 102-103
% Lehman & Casela 1998, p. 32-44
% Robert 2007, p. 14
% Cencov 1982, p. 28
% Ibarola Perez 2012, p. 163
% Mukhopadhyay 2000, p. 284

Muchas de las leyes que hemos visto, que sean discreta o continuas, partenecen a
una clase que comparte propriedades  particulares, y que juega un rol particular
que sea en f\'isica (ej. en  problema de maximizaci\'on de entrop\'ia de Shannon
como lo vamos a ver en  el c\'apitulo~\ref{Cap:SZ:Informacion}) o en el marco de
la inferencia bayesiana. Esta clase es la familia dicha exponencial~\cite{Dar35,
  Koo36, And70, LehCas98, IbPer12,  Muk00, KotBal00, Rob07, Bos07, Cen82, Kay93,
  NieNoc10}.  En inferencia bayesiana,  cuando una distribuci\'on de sampleo cae
en esta  familia, se  puede por  ejemplo deducir a  priori conjugados,  es decir
tales que  la distribuci\'on dicha a  posteriori caiga en la  misma familia (\ie
tenga la  misma forma  param\'etrica pero con  par\'ametros dependientes  de los
datos)~\footnote{Ver nota de pie~\footref{Foot:SZ:BayesPrior}. Recordamos que si
  observaciones tienen  una distribuci\'on \ $p_{X|\Theta=\theta}(x)$  \ con una
  distribuci\'on a  priori del par\'ametro  $p_\Theta(\theta)$, por la  regla de
  Bayes el a  posteriori, es decir la ley del  parametro dados las observaciones
  es   dada  por   \   $p_{\Theta|X=x}(\theta)  \propto   p_{X|\Theta=\theta}(x)
  p_\Theta(\theta)$  \ con  \ $\propto$  \ significando  ``proporcional  a''. Si
  $p_{\Theta|X=x}$ \ tiene la misma forma param\'etrica que \ $p_\Theta(\theta)$
  \ inferir  \ $\theta$ \ con  datos que se  observan se reduce a  actualizar el
  par\'ametro de la  ley a posteriori.}. Notar que esta familia  es conocido o a
veces  {\em   familia  de  Koopman-Darmois}  debido  a   la  introducci\'on  por
Koopman~\cite{Koo36}  o  Darmois~\cite{Dar35}   en  los  a\~nos  1935-1936  (ver
tambi\'en Pitman~\cite{Pit36}) y es definida de la manera siguiente:
%
\begin{definicion}[Familia exponencial y exponencial natural]\label{Def:MP:FamiliaExponencial}
%
  Sea  \ $X$  \  vector aleatorio  definido  sobre \  $\X  \subset \Rset^d$,  de
  densidad  de probabilidad  \  $p_X$ \  \ con  respecto  a una  medida \  $\mu$
  (discreta, continua o  mixta). La distribuci\'on de probabilidad  \ $p_X$ \ es
  dicha de  la {\em familia  exponencial de orden  $k$}, $k \in \Nset^*$,  si se
  escribe de la forma
  %
  \[
  p_X(x) = C(\theta) \, h(x) \, \exp\left( \eta(\theta)^t S(x) \right)
  \]
  %
  donde
  %
  \[
  C:  \Theta \subset  \Rset^m \mapsto  \Rset_+,  \qquad h:  \X \mapsto  \Rset_+,
  \qquad \eta: \Theta \mapsto \Rset^k, \qquad S: \X \mapsto \Rset^k
  \]
  %
  con \ $m \in \Nset$.  En otras palabras, la familia exponencial es una familia
  parametrica  de la  forma as\'i  definida, con  $\Theta$ \  el espacio  de los
  par\'ametros. La familia es dicha {\em exponencial natural} si tiene la forma
  %
  \[
  p_X(x) = \frac{1}{Z(\eta)}  \, h(x) \, \exp\left( \eta^t  S(x) \right) = h(x)
  \, \exp\left( \eta^t S(x) - \varphi(\eta) \right)
  \]
  %
  con
  %
  \[
  \eta \in N \subset \Rset^k, \qquad Z: \Rset^k \mapsto \Rset_+, \qquad \varphi = \log(Z),
  \qquad h: \X \mapsto \Rset_+, \qquad S: \X \mapsto \Rset^k
  \]
  %
  donde  \ $\displaystyle  N =  \left\{  \eta \in  \Rset^k \tq  \int_\X h(x)  \,
    \exp\left(  \eta^t  S(x) -  \varphi(\eta)  \right)  \,  d\mu(x) <  +  \infty
  \right\}$ \ es (convexo y) llamado {\em espacio de par\'ametros naturales}.
\end{definicion}
%
Se notara  que con  la reparametrizaci\'on \  $\eta(\theta)$ \ se  puede siempre
(por lo  menos formalmente) escribir una  ley de la familia  exponencial bajo su
forma natural. Con respeto a cada termino:
%
\begin{itemize}
\item  $S$  es  llamada  {\em  estad\'istica suficiente}  o  {\em  estad\'istica
    exaustiva}. Esta  denominaci\'on viene  del hecho que  el conocimiento  de \
  $S(x)$  \  es  suficiente para  estimar  $\eta$,  o  un resumen  exaustivo  de
  $\eta$. En particular, la estimaci\'on de verosimilitud~\footnote{El estimador
    del m\'aximo  de verosilitud \ $\eta_{\mathrm{mv}}$  \ es el \  $\eta$ \ que
    m\'aximiza \ $p_X$  \ o cualquier funci\'on creciente  como el logaritmo por
    ejemplo.    Para   la   familia   exponencial,  eso   da   sencillamente   \
    $\eta_{\mathrm{mv}}$  \  satisficiendo  \  $S(x)  =  \nabla  \varphi(\eta)$,
    dependiente    solamente     de    $S(x)$.},    o     cualquier    estimador
  Bayesiano~\footnote{Ver nota  de pie~\footref{Foot:SZ:BayesPrior}. Recuerdense
    que se modeliza  el par\'ametro como aleatorio, as\'i  que la distribuci\'on
    que se considera se ve como la distribuci\'on condicional \ $p_{X|N=\eta}(x)
    =   h(x)  \exp\left(   \eta^t  S(x)   -  \varphi(\eta)   \right)$,   con  la
    distribuci\'on  a  priori   \  $p_N(\eta)$.   De  la  regla   de  Bayes,  la
    distribuci\'on a  posteriori, \ie del par\'ametro dadas  las observaciones \
    $x$ \  se escribe \ $p_{N|X=x}(\eta) \propto  p_{X|N=\eta}(x) p_N(\eta)$. Si
    se da un  costo de estimaci\'on \ $C(\widehat{\eta},N)  > 0$ \ caracerizando
    la  ``distancia'' entre  la estimaci\'on  y el  parametro  ``verdadero'', se
    busca la funci\'on  $\widehat{\eta}$ que minimiza el costo  promedio, lo que
    es equivalente para cada \ $x$ \  a buscar el valor \ $\widehat{\eta}$ \ que
    minimiza \  $\displaystyle \int C(\widehat{\eta},\eta)  \, \exp\left( \eta^t
      S(x)  - \varphi(\eta)  \right) \,  p_N(\eta)  \, d\mu(\eta)$~\cite{Rob07}:
    claramente,  el m\'inimo depende  solamente de  $S(x)$.}  de  $\eta$ depende
  solamente  de $S(X)$~\cite{Kay93,  LehCas98, Rob07,  Cen82,  IbaPer12, Muk00}.
  Formalmente, una  estadistica \ $T(X)$ \  es suficiente para  un par\'ametro \
  $\theta$ \ si la  distribuci\'on de \ $X$ \ condicionalmente a  \ $S(X) = s$ \
  no depende m\'as de \ $\theta$.   Por ejemplo, para la familia exponencial, en
  el caso discreto, tenemos,\ $\displaystyle p_{X|S(X) = s}(x) = \frac{P\big( (X
    = x) \cap (S(X) = s) \big)}{P(S(X)  = s)} = \frac{h(x) \exp\left( \eta^t s -
      \varphi(\eta) \right) \un_{\{ S(x) \}}(s)}{\sum_x h(x) \exp\left( \eta^t s
      -  \varphi(\eta) \right) \un_{\{  S(x) \}}(s)}  = \frac{h(x)  \un_{\{ S(x)
      \}}(s)}{\sum_x h(x) \un_{\{ S(x) \}}(s)}$.
%
\item $\theta$  es el par\'ametro, escalar  o multivariado, y  $\eta$ es llamado
  {\em par\'ametro natural}.
%
\item  La funci\'on  $Z$ es  llamada {\em  funci\'on de  partici\'on};  A veces,
  $\varphi$ es as\'i llamada {\em  log-funci\'on de partici\'on}; no solo juegan
  un  rol en  la  normalizaci\'on de  la  ley, pero  tienen una  significaci\'on
  f\'isica  como lo vamos  a evocar.  Aparecio $Z$  en f\'isica  esdatistica por
  ejemplo en trabajos de Gibbs~\cite{Gib01, Gib02}.
\end{itemize}

Se notara que,  en particular, \ $Z$ \  es relacionada a los momentos  de $S$, o
$\varphi$ a los cumulantes:
%
\begin{teorema}[Funci\'on de partici\'on y generadoras]
%
  Sea \  $X$ \  de distribuci\'on exponencial  natural de par\'ametro  natural \
  $\eta$ \ y estad\'istica suficiente \ $S$  \ y denotamos \ $Z$ la funci\'on de
  partici\'on y  $\varphi$ su logaritmo.  Entonces,  las funci\'ones generadoras
  de los momentos y de los cumulantes \ $M_{S(X)}$ \ y \ $C_{S(X)}$ \ del vector
  aleatorio \ $S$ son relacionadas a \ $Z$ \ y \ a \ $\varphi = \log Z$ \ por, \
  $\forall \: u \: \mbox{ tal que } \: u+\eta \in N$,
  %
  \[
  M_{S(X)}(u) =  \frac{Z(u+\eta)}{Z(\eta)} \qquad \mbox{y}  \qquad C_{S(X)}(u) =
  \varphi(u+\eta) - \varphi(\eta)
  \]
  %
  En particular, si \  $\varphi$ \ es diferenciable tenemos
  %
  \[
  \nabla \varphi (\eta) = \Esp\left[ S(X) \right]
  \]
  %
  y si \ $\varphi$ \ es dos veces diferenciable tenemos
  %
  \[
  \Hess \varphi (\eta) = \Cov\left[ S(X) \right]
  \]
  %
  Pasando, de  \ $\Cov\left[ S(X)  \right] \ge 0$  \ tenemos que, hessiana  de \
  $\varphi$ \ siendo positiva, \ $\varphi$ \ es convexa~\cite{CamMar09}.
%
\end{teorema}
%
\begin{proof}
  De la definici\'on de la funci\'on generadora de \ $S(X)$, tenemos
  %
  \begin{eqnarray*}
  M_{S(X)}(u) & = &\Esp\left[ \exp\left( u^t S(X) \right) \right]\\[2mm]
  %
  & = & \int_\X \frac{1}{Z(\eta)} \, h(x) \, \exp\left( (u + \eta)^t S(x) \right) \,
  d\mu(x)\\[2mm]
  %
  & = & \frac{Z(u+\eta)}{Z(\eta)} \int_\X \frac{1}{Z(\eta)} \, h(x) \,
  \exp\left( (u + \eta)^t S(x) \right) \, d\mu(x)\\[2mm]
  %
  & = &  \frac{Z(u+\eta)}{Z(\eta)}
  \end{eqnarray*}
  %
  La secunda  relaci\'on es inmediata  de \ $C_X  = \log M_X$ \  conjuntamente a
  \ $\varphi = \log Z$.

  A continuaci\'on, los cumulantes y momentos coinciden hasta el orden $3$, dando inmediatamenta
  %
  \begin{eqnarray*}
  \Esp\left[ S(X) \right] & = & \left. \nabla_u C_{S(X)} \right|_{u=0}\\[2mm]
  %
  & = & \left. \nabla_u \left( \varphi(u+\eta) - \varphi(\eta) \right) \right|_{u=0}\\[2mm]
  %
  & = & \nabla \varphi(\eta)
  \end{eqnarray*}
  %
  y
  %
  \begin{eqnarray*}
  \Cov\left[ S(X) \right] & = & \left. \Hess_u C_{S(X)} \right|_{u=0}\\[2mm]
  %
  & = & \left. \Hess_u \left( \varphi(u+\eta) - \varphi(\eta)  \right) \right|_{u=0}\\[2mm]
  %
  & = & \Hess \varphi(\eta)
  \end{eqnarray*}
\end{proof}


\SZ{Van den Bos 2007, p. 33, informaci\'on de Fisher}

\SZ{Volver a esta en el problema MaxEnt (revisar el texto)}