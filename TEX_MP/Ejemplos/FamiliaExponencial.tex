% --------------------------------- Familia  exponencial
\subseccion{Familia exponencial}
\label{Ssec:MP:FamiliaExponencial}

% Familia exponencial:
% --------------------
% Kay 1993, p. 110 & 124
% Lehman & Casela 1998, p. 23
% Kotz & Balakrishnan 2000, p. 659
% Robert 2007, p. 115
% van den Bos 2007, p. 30
% Cencov 1982, p. 245, 279
% Ibarola Perez 2012, p. 174
% Mukhopadhyay 2000, p. 141


% Estadistica suficiente:
% -----------------------
% Kay 1993, p. 102-103
% Lehman & Casela 1998, p. 32-44
% Robert 2007, p. 14
% Cencov 1982, p. 28
% Ibarola Perez 2012, p. 163
% Mukhopadhyay 2000, p. 284

Muchas de las leyes que hemos visto, que sean discreta o continuas, partenecen a
una clase que comparte propriedades  particulares, y que juega un rol particular
que sea en f\'isica (ej. en  problema de maximizaci\'on de entrop\'ia de Shannon
como lo vamos a ver en  el c\'apitulo~\ref{Cap:SZ:Informacion}) o en el marco de
la inferencia bayesiana. Esta clase es la familia dicha exponencial~\cite{Dar35,
  Koo36, And70, LehCas98, IbaPer12, Muk00, KotBal00, Rob07, Bos07, Cen82, Kay93,
  NieNoc10}.  En inferencia bayesiana,  cuando una distribuci\'on de sampleo cae
en esta  familia, se  puede por  ejemplo deducir a  priori conjugados,  es decir
tales que  la distribuci\'on dicha  a posteriori caiga  en la misma  familia \ie
tenga la  misma forma  param\'etrica pero con  par\'ametros dependientes  de los
datos         (Ver         notas         de         pie~\ref{Foot:MP:BayesPrior}
y~\ref{Foot:MP:BayesPriorConjugado}).
%. Recordamos que si
%  observaciones tienen  una distribuci\'on \ $p_{X|\Theta=\theta}(x)$  \ con una
%  distribuci\'on a  priori del par\'ametro  $p_\Theta(\theta)$, por la  regla de
%  Bayes el a  posteriori, es decir la ley del  parametro dados las observaciones
%  es   dada  por   \   $p_{\Theta|X=x}(\theta)  \propto   p_{X|\Theta=\theta}(x)
%  p_\Theta(\theta)$  \ con  \ $\propto$  \ significando  ``proporcional  a''. Si
%  $p_{\Theta|X=x}$ \ tiene la misma forma param\'etrica que \ $p_\Theta(\theta)$
%  \ inferir  \ $\theta$ \ con  datos que se  observan se reduce a  actualizar el
%  par\'ametro de la  ley a posteriori.}. 
Notar que esta familia  es conocido o a
veces  {\em   familia  de  Koopman-Darmois}  debido  a   la  introducci\'on  por
Koopman~\cite{Koo36}  o  Darmois~\cite{Dar35}   en  los  a\~nos  1935-1936  (ver
tambi\'en Pitman~\cite{Pit36}) y es definida de la manera siguiente:
%
\begin{definicion}[Familia exponencial y exponencial natural]\label{Def:MP:FamiliaExponencial}
%
  Sea  \ $X$  \  vector aleatorio  definido  sobre \  $\X  \subset \Rset^d$,  de
  densidad  de probabilidad  \  $p_X$ \  \ con  respecto  a una  medida \  $\mu$
  (discreta, continua o  mixta). La distribuci\'on de probabilidad  \ $p_X$ \ es
  dicha de  la {\em familia  exponencial de orden  $k$}, $k \in \Nset_0$,  si se
  escribe de la forma
  %
  \[
  p_X(x) = C(\theta) \, h(x) \, \exp\left( \eta(\theta)^t S(x) \right)
  \]
  %
  donde
  %
  \[
  C:  \Theta \subset  \Rset^m \mapsto  \Rset_+,  \qquad h:  \X \mapsto  \Rset_+,
  \qquad \eta: \Theta \mapsto \Rset^k, \qquad S: \X \mapsto \Rset^k
  \]
  %
  con  \ $m  \in \Nset_0$.   En otras  palabras, la  familia exponencial  es una
  familia parametrica de  la forma as\'i definida, con $\Theta$  \ el espacio de
  los par\'ametros.  La familia es dicha  {\em exponencial natural}  si tiene la
  forma
  %
  \[
  p_X(x) = \frac{1}{Z(\eta)}  \, h(x) \, \exp\left( \eta^t  S(x) \right) = h(x)
  \, \exp\left( \eta^t S(x) - \varphi(\eta) \right)
  \]
  %
  con
  %
  \[
  \eta \in \mathrm{N} \subset \Rset^k, \qquad Z: \Rset^k \mapsto \Rset_+, \qquad
  \varphi = \log(Z), \qquad h: \X \mapsto \Rset_+, \qquad S: \X \mapsto \Rset^k
  \]
  %
  donde  \ $\displaystyle \mathrm{N}  = \left\{  \eta \in  \Rset^k \,  \left| \,
      \int_\X h(x) \, \exp\left( \eta^t  S(x) - \varphi(\eta) \right) \, d\mu(x)
      < +  \infty \right.  \right\}$ \  es (convexo y)  llamado {\em  espacio de
    par\'ametros naturales}.
\end{definicion}
%
Se notara  que con  la reparametrizaci\'on \  $\eta(\theta)$ \ se  puede siempre
(por lo  menos formalmente) escribir una  ley de la familia  exponencial bajo su
forma natural. Con respecto a cada termino:
%
\begin{itemize}
\item  $S(X)$ es  llamada {\em  estad\'istica suficiente}  o  {\em estad\'istica
    exaustiva}. Esta  denominaci\'on viene  del hecho que  el conocimiento  de \
  $S(X)$  \ es  suficiente para  estimar $\eta$,  o es  un resumen  exaustivo de
  $\eta$. En particular, la estimaci\'on de verosimilitud~\footnote{El estimador
    del m\'aximo  de verosilitud \ $\eta_{\mathrm{mv}}$  \ es el \  $\eta$ \ que
    m\'aximiza \ $p_X$  \ o cualquier funci\'on creciente  como el logaritmo por
    ejemplo.    Para   la   familia   exponencial,  eso   da   sencillamente   \
    $\eta_{\mathrm{mv}}$  \  satisficiendo  \  $S(x)  =  \nabla  \varphi(\eta)$,
    dependiente solamente de  $S(x)$.\label{Foot:MP:MLE}}, o cualquier estimador
  Bayesiano~\footnote{Ver nota  de pie~\footref{Foot:SZ:BayesPrior}. Recuerdense
    que se modeliza  el par\'ametro como aleatorio, as\'i  que la distribuci\'on
    que se considera se ve como la distribuci\'on condicional \ $p_{X|N=\eta}(x)
    =   h(x)  \exp\left(   \eta^t  S(x)   -  \varphi(\eta)   \right)$,   con  la
    distribuci\'on  a  priori   \  $p_N(\eta)$.   De  la  regla   de  Bayes,  la
    distribuci\'on a  posteriori, \ie del par\'ametro dadas  las observaciones \
    $x$ \  se escribe \ $p_{N|X=x}(\eta) \propto  p_{X|N=\eta}(x) p_N(\eta)$. Si
    se da un costo de  estimaci\'on \ $C(\widehat{\eta},N) > 0$ \ caracterizando
    la  ``distancia'' entre  la estimaci\'on  y el  parametro  ``verdadero'', se
    busca la funci\'on  $\widehat{\eta}$ que minimiza el costo  promedio, lo que
    es equivalente para cada \ $x$ \  a buscar el valor \ $\widehat{\eta}$ \ que
    minimiza \ $\displaystyle \int_N C(\widehat{\eta},\eta) \, \exp\left( \eta^t
      S(x)  - \varphi(\eta)  \right) \,  p_N(\eta)  \, d\mu(\eta)$~\cite{Rob07}:
    claramente, el m\'inimo es  funci\'on, solamente de $S(x)$, cualquieras sean
    el costo $C$ y el a  priori $p_N$ (estos solamente determinan cual funci\'on
    de   $S(X)$   vamos  a   obtener).}    de   $\eta$   depende  solamente   de
  $S(X)$~\cite{Kay93,  LehCas98, Rob07,  Cen82, IbaPer12,  Muk00}.  Formalmente,
  una estadistica \  $T(X)$ \ es suficiente para un par\'ametro  \ $\theta$ \ si
  la distribuci\'on  de \ $X$  \ condicionalmente  a \ $S(X)  = s$ \  no depende
  m\'as de  \ $\theta$.  Por  ejemplo, para la  familia exponencial, en  el caso
  discreto, tenemos,\  $\displaystyle p_{X|S(X) =  s}(x) = \frac{P\big( (X  = x)
    \cap  (S(X) =  s) \big)}{P(S(X)  = s)}  = \frac{h(x)  \exp\left( \eta^t  s -
      \varphi(\eta)  \right) \uno_{\{  S(x) \}}(s)}{\displaystyle  \sum_{x\in \X}
    h(x)  \exp\left( \eta^t  s -  \varphi(\eta) \right)  \uno_{\{ S(x)  \}}(s)} =
  \frac{h(x)  \uno_{\{ S(x)  \}}(s)}{\displaystyle \sum_{x  \in \X}  h(x) \uno_{\{
      S(x)   \}}(s)}$   \   no   depende   de   \   $\eta$.    Veremos   en   el
  cap\'itulo~\ref{Cap:SZ:Informacion},   secci\'on~\ref{Ssec:SZ:Fisher}  que  la
  informaci\'on de  Fisher en $\eta$,  medida informacional y apareciendo  en la
  cota del error cuadratico m\'inimo posible  de un estimador de \ $\eta$, es la
  covarianza  de  $S$,  mostrando  de  nuevo  que  \  $S$  \  es  sufiente  para
  caracterizar  \ $\eta$.   En  este mismo  cap\'itulo~\ref{Cap:SZ:Informacion},
  secci\'on~\ref{Ssec:SZ:MaxEnt},  veremos  que,  sujeto  a \  $\Esp[S(X)]$,  la
  distribuci\'on  que  maximiza  la  entrop\'ia,  medida de  incerteza,  es  una
  distribuci\'on  exponencial, enfatizando el  rol de  esta familia  en f\'isica
  cuando se tiene la media de una estad\'istica fija (ej. energ\'ia fija).
%
\item $\theta$  es el par\'ametro, escalar  o multivariado, y  $\eta$ es llamado
  {\em par\'ametro natural}.
%
\item La  funci\'on $Z$ es llamada {\em  funci\'on de partici\'on}~\footnote{Hay
    no confundir la funci\'on de partici\'on y la funci\'on de repartici\'on que
    son  dos objectos  distintos.}; A  veces,  $\varphi$ es  as\'i llamada  {\em
    log-funci\'on de partici\'on};  no solo juegan un rol  en la normalizaci\'on
  de la  ley, pero tienen una  significaci\'on f\'isica como lo  vamos a evocar.
  Apareci\'o  $Z$   en  f\'isica  estad\'istica  por  ejemplo   en  trabajos  de
  Gibbs~\cite{Gib02}.
\end{itemize}

Se notar\'a, en particular, que \ $Z$  \ es relacionada a los momentos de $S$, o
$\varphi$ a los cumulantes:
%
\begin{teorema}[Funci\'on de partici\'on y generadoras]
%
  Sea \  $X$ \  de distribuci\'on exponencial  natural de par\'ametro  natural \
  $\eta$ \ y estad\'istica suficiente \ $S$  \ y denotamos \ $Z$ la funci\'on de
  partici\'on y  $\varphi$ su logaritmo.  Entonces,  las funci\'ones generadoras
  de los  momentos y de  los cumulantes \  $M_{S(X)}$ \ y  \ $C_{S(X)}$ \  de la
  estad\'istica  suficiente \  $S(X)$ \  son relacionadas  a  \ $Z$  \ y  \ a  \
  $\varphi = \log  Z$ \ por, \ $\forall \:  u \: \mbox{ tal que  } \: u+\eta \in
  N$,
  %
  \[
  M_{S(X)}(u) =  \frac{Z(u+\eta)}{Z(\eta)} \qquad \mbox{y}  \qquad C_{S(X)}(u) =
  \varphi(u+\eta) - \varphi(\eta)
  \]
  %
  En particular, si \  $\varphi$ \ es diferenciable tenemos
  %
  \[
  \nabla \varphi (\eta) = \Esp\left[ S(X) \right]
  \]
  %
  y si \ $\varphi$ \ es dos veces diferenciable tenemos
  %
  \[
  \Hess \varphi (\eta) = \Cov\left[ S(X) \right]
  \]
  %
  Pasando, de  \ $\Cov\left[ S(X)  \right] \ge 0$  \ tenemos que, hessiana  de \
  $\varphi$ \ siendo positiva, \ $\varphi$ \ es convexa~\cite{CamMar09}.
%
\end{teorema}
%
\begin{proof}
  De la definici\'on de la funci\'on generadora de \ $S(X)$, tenemos
  %
  \begin{eqnarray*}
  M_{S(X)}(u) & = &\Esp\left[ \exp\left( u^t S(X) \right) \right]\\[2mm]
  %
  & = & \int_\X \frac{1}{Z(\eta)} \, h(x) \, \exp\left( (u + \eta)^t S(x) \right) \,
  d\mu(x)\\[2mm]
  %
  & = & \frac{Z(u+\eta)}{Z(\eta)} \int_\X \frac{1}{Z(u+\eta)} \, h(x) \,
  \exp\left( (u + \eta)^t S(x) \right) \, d\mu(x)\\[2mm]
  %
  & = &  \frac{Z(u+\eta)}{Z(\eta)}
  \end{eqnarray*}
  %
  La secunda  relaci\'on es inmediata  de \ $C_X  = \log M_X$ \  conjuntamente a
  \ $\varphi = \log Z$.

  A continuaci\'on, los cumulantes y momentos coinciden hasta el orden $3$, dando inmediatamenta
  %
  \begin{eqnarray*}
  \Esp\left[ S(X) \right] & = & \left. \nabla_u C_{S(X)} \right|_{u=0}\\[2mm]
  %
  & = & \left. \nabla_u \left( \varphi(u+\eta) - \varphi(\eta) \right) \right|_{u=0}\\[2mm]
  %
  & = & \nabla \varphi(\eta)
  \end{eqnarray*}
  %
  y
  %
  \begin{eqnarray*}
  \Cov\left[ S(X) \right] & = & \left. \Hess_u C_{S(X)} \right|_{u=0}\\[2mm]
  %
  & = & \left. \Hess_u \left( \varphi(u+\eta) - \varphi(\eta)  \right) \right|_{u=0}\\[2mm]
  %
  & = & \Hess \varphi(\eta)
  \end{eqnarray*}
\end{proof}

En problemas  de estimaci\'on, frecuentemente, se tiene  $n$ vectores aleatorios
independiente  de misma  ley que  se  usan para  estimar un  p\'arametro. En  la
familia exponencial, resuelte que  la distribuci\'on conjunta en tal situaci\'on
queda en la familia exponencial:
%
\begin{lema}
  Sean \  $X_1 , \ldots ,  X_n$ \ vectores aleatorios,  independientes, de misma
  ley de la familia exponencial natural,  de estadistica suficiente \ $S$ \ y de
  log-funci\'on de  partici\'on \ $\varphi$. Entonces  la ley conjunta  de los \
  $X_i$ \ cae  el la familia exponencial  de mismo orden que el  de \ $p_{X_i}$,
  con  el   mismo  par\'ametro,  de  estadistica   suficiente  \  $\displaystyle
  (X_1,\ldots,X_n)  \mapsto  \sum_{i=1}^n  S(X_i)$   \  y  de  log-funci\'on  de
  partici\'on \ $n \varphi$.
\end{lema}
%
\begin{proof}
  El resulta  es inmediato siendos  los \ $X_i$  \ independientes, dando  la ley
  conjunta como el producto de las leyes de los $X_i$.
\end{proof}

Muchas  distribuciones caen  en la  familia  exponencial, que  sean discretas  o
continuas, como lo vamos a ver en dos ejemplos.
%
\begin{ejemplo}
  Sea  \ $p_X$  \ distribuci\'on  de Bernoulli  \ $\B(p)$.   Esta distribuci\'on
  partenece a la familia exponencial de  orden $1$.  De hecho, se puede escribir
  la ley \ $p_X(x) = p^x (1-p)^{1-x}$ \ bajo la forma
  %
  \[
  p_X(x) = \exp\left( x \log \left( \frac{p}{1-p} \right) + \log(1-p) \right)
  \]
  %
  Aparece  que el par\'ametro  natural es  \ $\eta  = \log  \left( \frac{p}{1-p}
  \right)$ \  y la estadistica  suficiente correspondiente es  \ $S(X) =  X$. En
  particular, para \  $X_i, \: i = 1,  \ldots , n$ \ independientes  tales que \
  $X_i \sim \B(p)$,  una estadistica sufficiente de la ley  conjunta es la media
  emp\'irica  $\displaystyle   \widebar{X}  =  \frac{1}{n}   \sum_{i=1}^n  X_i$.
  Aparece   que  el   estimador   de  verosimilitud   m\'axima   (Ver  nota   de
  pie~\ref{Foot:MP:MLE}) es precisamente la  media emp\'irica; es el estimador \
  $\widehat{p}$  \ de  error cuadratico  \  $\Esp\left[ \left(  \widehat{p} -  p
    \right)^2 \right]$ \ m\'inimo (ver ej.~\cite{Kay93}).
\end{ejemplo}

\begin{ejemplo}
  Sea \ $p_X$ \ distribuci\'on gamma \ $\G(a,b)$.  Esta distribuci\'on partenece
  a la familia exponencial  de orden $2$.  De hecho, se puede  escribir la ley \
  bajo la forma
  %
  \[
  p_X(x)     =    \frac{1}{x}    \,     \exp\left(    \begin{bmatrix}     a    &
      b \end{bmatrix} \begin{bmatrix}  \log x \\ - x  \end{bmatrix} - \log\left(
      \frac{\Gamma(a)}{b^a} \right)\right)
  \]
  %
  El   par\'ametro  natural   es   as\'i   \  $\eta   =   \begin{bmatrix}  a   &
    b \end{bmatrix}^t$ \ y la  estadistica suficiente correspondiente es \ $S(X)
  = \begin{bmatrix} \log x & - x \end{bmatrix}^t$.
\end{ejemplo}

Si muchas distribuciones  partenecen a la familia exponencial,  no todas caen en
esta familia:
%
\begin{ejemplo}
  Sea  \   $p_X$  \  distribuci\'on  Student-$t$   \  $\T_\nu(m,\Sigma)$.   Esta
  distribuci\'on no cae el la familia exponencial en general. De hecho, se puede
  todav\'ia escribir la ley bajo la forma
  %
  \[
  p_X(x)   =  C(\nu,\Sigma)   \exp\left(  -   \frac{d+\nu}{2}  \log\left(   1  +
      \frac{(x-m)^t \Sigma^{-1} (x-m)}{\nu} \right) \right)
  \]
  %
  A    pesar   de    que    la   ley    parece    tener   la    forma   de    la
  definici\'on~\ref{Def:MP:FamiliaExponencial}    con   \    $\eta(\nu)    =   -
  \frac{d+\nu}{2}$,  su  factor  \  $\log\left( 1  +  \frac{(x-m)^t  \Sigma^{-1}
      (x-m)}{\nu} \right)$  \ no es funci\'on  \'unicamente de \  $x$; se quedan
  todo los par\'ametros \ $\nu, \: m, \: \Sigma$. A\'un si un de esos es fijo, y
  no visto como par\'ametro, quedar\'a un par\'ametro en este t\'ermino.

  De hecho,  la excepci\'on notable aparece  cuando~\footnote{Es precisamente de
    esta forma que la  ley aparece bajo el nombre ``Pearson tipo  IV y VII'' (de
    hecho, el par\'ametro es $\frac{\nu+d}{2}$  entero en la ley de Pearson tipo
    VII).  La  diferencia entre  la Pearson tipo  IV o  VII y la  Student-$t$ es
    s\'util: entre otros, el comportamiento en $\nu \to \infty$ es drasticamente
    diferente en  cada caso  (no hay convergencia  el la  parametrizaci\'on tipo
    Pearson).}  $\Sigma = \nu^{-1} R$ \ con $R$ conocido, independiente de $\nu$
  y $m$ conocido.
\end{ejemplo}

Se puede tambi\'en que una ley  partenece o no a la familia exponencial, seg\'un
que unos par\'ametros sean fijos  (de hecho, no son par\'ametros m\'as entonces),
o no:
%
\begin{ejemplo}
  Sea \  $p_X$ \ distribuci\'on  binomial \  $\B(n,p)$. Si \  $n$ \ es  fijo, es
  decir  no  visto  como  par\'ametro,  la  distribuci\'on  cae  el  la  familia
  exponencial  de orden $1$.  Si se  consideran ambos  \ $n$  \ y  \ $p$  \ como
  par\'ametros,  no  cea m\'as  en  la familia  exponencial.  Para  ver eso,  se
  escribir la ley bajo la forma
  %
  \[
  %begin{eqnarray*}
  p_X(x) = \frac{n!}{x! (n-x)!}  \exp\left( x \log\left( \frac{p}{1-p}
    \right) + n \, \log(1-p) \right)
  %\\[2mm]
  %
  %& = & \frac{1}{x!}  \exp\left( - \log\left( (n-x)! \right) + x \log\left(
  %\frac{p}{1-p} \right) + n \, \log(1-p) + \log(n!) \right)
  %\end{eqnarray*}
  \]
  %
  Entonces, si  \ $n$  \ es fijo,  se concluye que  \ $p_X$  \ es en  la familia
  exponencial de  orden $1$, de  par\'ametro \ $\eta =  \log\left( \frac{p}{1-p}
  \right)$  \  y estad\'istica  suficiente  correspondiente  \  $S(x) =  x$.  Al
  rev\'es,  si \  $n$ \  es un  par\'ametro  (que \  $p$ \  sea fijo  o no),  en
  $(n-x)!$, no se puede  ``separar'' \ $x$ \ de \ $n$  \ y tampoco escribir este
  termino  de la  forma \  $\exp(  f(n) g(x)  )$: la  ley  no es  de la  familia
  exponencial mas.
\end{ejemplo}

De las distribuciones que hemos visto:
%
\begin{itemize}
\item Caen en  la familia exponencial la leyes: de  Bernoulli, binomial cuando \
  $n$ \ es fijo, binomial negativa cuando  \ $r$ \ es fijo, multinomial cuando \
  $n$ \  es fijo, geometrica,  de Poisson, gaussiana~\footnote{En este  caso, se
    puede ver  el par\'ametro  natural como \  $\left( \Sigma^{-1} m  , -\frac12
      \Sigma^{-1}  \right)$ \  en  lugar del  vector  formado de  los \  $\left(
      \Sigma^{-1} m \right)_i$ \ y \ $-\frac12 \left( \Sigma^{-1} \right)_{i,j},
    \: 1 \le i \le j \le d$ \ y la estad\'istica suficiente como \ $\left( x , x
      x^t \right)$  \ siendo  $x^t \Sigma^{-1} x  = \Tr\left( \Sigma^{-1}  x x^t
    \right)$, en lugar  del vector formado de los  \ $x_i$ \ y \ $x_i  x_j, \: 1
    \le  i  \le  j \le  d$  (de  la  simetr\'ia).   El  orden es  $d  +  \frac{d
      (d+1)}{2}$.}, gamma, Wishart~\footnote{De nuevo, en este caso se puede ver
    el  par\'ametro  natural  formalmente  como \  $\left(  \frac{\nu-d-1}{2}  ,
      -\frac12 V^{-1}  \right)$ \ y  la estad\'istica suficiente como  \ $\left(
      \log |x|  , x \right)$.  El orden  es $1 + \frac{d  (d+1)}{2}$.}, beta, de
  Dirichlet.
%
% resp. eta = log(p/(1-p)), log(p/(1-p)), log p, log p = [log p_1 ... \log p_k],
%       log(1-p), log lambda, (Sigma^{-1} m , -1/2 Sigma^{-1}), [a b],
%       ( (nu-d-1)/2 , -1/2 V^{-1}), [a b], a = [a_1 ... a_k]
% resp. S = x, x, x, x = [x_1 ... x_k], x,
%       x, (x,x x^t), [log x -x], (log |x| , x),
%       [log x  log(1-x)], [log x_1 ... log(x_k)]
%
\item No partenecen a la familia  exponencial las leyes: binomial cuando \ $n$ \
  es  un  par\'ametro, binomial  negativa  cuando \  $r$  \  es un  par\'ametro,
  multinomial cuando \ $n$ \ es un par\'ametro, hipergeometrica, hipergeometrica
  negativa,     hipergeometrica      multivaluada~\footnote{En     los     casos
    hipergeometricos, har\'ia falta que sean  fijos respectivamente \ $n, m, k$,
    \ $n,  r, k$  \ y  \ $n,  m, k_1, \ldots  , k_c$  \ y  la leyes  no ser\'ian
    parametricas  mas.},   Student-$t$  en   el  caso  general,   Student-$r$  y
  uniformas~\footnote{Eso  viene del  hecho de  que  el soporte  depende de  los
    par\'ametros. O si no, hay que verlas de orden 0.}.
\end{itemize}

Las   distribuciones   exponenciales   aparecen   frecuentemente   en   f\'isica
est\'adistica a trav\'es de  la teor\'ia de Boltzmann~\cite{Bol96, Bol98, Gib02,
  LanLif80, MezMon09, Mer10, Mer18}.   Adem\'as, cantidades f\'isicas se derivan
de la  log-funci\'on partici\'on~\cite{Max67, Gib02,  LanLif80, Ell06, MezMon09,
  Mer10, Mer18}:
%
\begin{ejemplo}
  En f\'isica estadistica, se enfrente al problema de descripci\'on macroscopico
  de un sistema de muchas part\'iculas (ej. hirviente de un liquido). Hay tantas
  part\'iculas que no se puede estudiar  tales sistemas con las leyes usuales de
  la  mec\'anica, as\'i que  se usa  un enfoque  probabil\'istico.  Por  eso, se
  considera  un  espacio  \  $\X$   \  $d$-dimensional  dicho  {\em  espacio  de
    configuraciones} (puede ser discreto o  continuo). En \ $x = \begin{bmatrix}
    x_1 & \cdots & x_d\end{bmatrix}$, cada  \ $x_i$ \ representa el {\em estado}
  de la  \ $i$-\'esima part\'icula (posici\'on,  velocidad, esp\'in,\ldots).  Lo
  importante es que a un tipo de sistema se asocia una {\em funci\'on energ\'ia}
  \  $\E(x)$,  llamado  tambi\'en   {\em  hamiltonian}  del  sistema,  debido  a
  W. R. Hamilton~\footnote{En 1833,  reformul\'o la mec\'anica dicha lagrangiana
    de   1788  en  t\'ermino   de  energ\'ia~\cite{Ham1834,   Ham1835,  Lag1788,
      MarRat10}.}. Por ejemplo, en  un sistema sin interacciones, $\displaystyle
  \E(x) = \sum_{i=1}^d \E_i(x_i)$.  El  el caso del gaz perfecto, $\displaystyle
  \E(x) =  \frac12 m  \sum_{i=1}^d x_i^2$ \  donde \  $m$ \ es  la masa  de cada
  particula y \  $x_i$ \ la velocidad de la \  $i$-\'esima particula (espacio de
  configuraciones continuo).   En el {\em modelo ferromagn\'etico  de Ising}, se
  consideran part\'iculas en una ret\'icula y \ $x_i = \pm 1$ \ es el esp\'in de
  la part\'icula  \ $i$  (espacio de configuraciones  discreto).  Sometido  a un
  campo magn\'etico \  $B$, la energ\'ia es dada por \  $\displaystyle \E(x) = -
  \sum_{(i,j)   \:   \mbox{\tiny   vecinos}}    x_i   x_j   -   B   \sum_{i=1}^d
  x_i$~\cite{Len20, Isi25,  Ons44, LanLif80, MezMon09, Mer10,  Mer18}.  Se puede
  poner  pesos \  $J_{i,j}$ \  en  cada vecinos,  positivos para  interracciones
  ferromagn\'eticas,   y  negativos  para   interacciones  antiferromagn\'eticas
  (modelos    {\em    vidrio   de    esp\'in},    o    m\'as   exactamente    de
  Edwards-Anderson~\cite{EdwAnd75,  LanLif80,   MezMon09,  Mer10,  Mer18}).   El
  modelo de Curie-Weiss~\footnote{Fue llamado as\'i en relaci\'on a los trabajos
    de  P.   Curie~\cite{Cur95}  y   P.   Weiss~\cite{Wei96,  Wei07}  sobre  los
    materiales  ferromagneticos.}   se  presenta  de  la misma  manera,  con  la
  energ\'ia $\displaystyle  \E(x) = -  \frac{1}{d} \sum_{i\ne j)  \: \mbox{\tiny
      pares}} x_i x_j + B \sum_{i=1}^d x_i$~\cite{MezMon09, Mer10, Mer18}.

  Seg\'un  la  teoria  de  Gibbs-Boltzmann,  la  dicha  {\em  distribuci\'on  de
    Gibbs-Boltzman}  asociada  a  un  espacio  de configuraci\'on  y  modelo  de
  energ\'ia es dada por
  %
  \[
  p_X(x) = \frac{1}{Z(\beta)} \exp\left( -  \beta \, \E(x) \right), \qquad \beta
  = \frac{1}{k_B T}
  \]
  %
  donde $k_B \approx  1.38 \times 10^{-23}$ julio por Kelvin  es la constante de
  Boltzmann,  y  \ $T$  \  es la  temperatura  en  Kelvin.  Esta  distribuci\'on
  partenece claramente a la familia exponencial natural de par\'ametro \ $\beta$
  \ y  de estad\'istica  suficiente \  $-\E(x)$ (ac\'a, $h  = 1$).   En f\'isica
  estad\'istica, la log-funci\'on de  partici\'on aparece en varias cantidades y
  potenciales f\'isicos:
  %
  \[
  F(\beta) = - \frac{1}{\beta} \log Z(\beta)
  \]
  %
  es  la  {\em  energ\'ia  libre}  o  {\em energ\'ia  libre  de  Helmholtz}  del
  sistema.  Es la  energ\'ia disponible  (o  que se  puede usar)  de un  sistema
  aislado.

  Luego, se define
  %
  \[
  U(\beta) =  \frac{\partial}{\partial \beta} \left( \beta F(\beta)  \right) = -
  \frac{\partial \log Z(\beta)}{\partial \beta} = \Esp\left[ \E(X) \right]
  \]
  %
  donde  \ $X$  \ ser\'ia  el  vector aleatorio  de distribuci\'on  \ $p_X$.   \
  $U(\beta)$ \ es la {\em energ\'ia interna} del sistema, promedio estad\'istico
  de la energ\'ia a trav\'es de todas las configuraciones posibles.

  Se define  tambi\'en una medida de  incerteza llamada {\em  entrop\'ia} o {\em
    entrop\'ia  de  Gibbs}~\cite{Bol77, Bol96,  Bol98,  Gib02, Jay65,  LanLif80,
    MezMon09, Mer10,  Mer18}.  Esta medida  caracteriza las fluctuaciones  de la
  energ\'ia libre~\footnote{La letra \ $S$ \ se us\'o historicamente. Obviamente
    no corresponde a la estad\'istica sufficiente que es ac\'a \ $\E$.},
  %
  \[
  S(\beta) = \beta^2 \frac{\partial}{\partial \beta} F(\beta)
  \]
  %
  Aparece   por   un  lado   que   \  $S(\beta)   =   \log   Z(\beta)  -   \beta
  \frac{\partial}{\partial \beta} \log Z(\beta)$  \ es decir, reconiciendo en el
  primer t\'ermino \ $- \beta F(\beta)$ \ y en el secundo \ $\beta U(\beta)$,
  %
  \[
  F = U - k_B T S
  \]
  %
  conocido como transformada de Legendre de la energ\'ia interna, y consecuencia
  de la  primera ley de  la termodynamica. Aparece tambi\'en  que $\displaystyle
  S(\beta) = \log Z(\beta) + \beta  \Esp[ \E(X) ] = \int_\X \left( \log Z(\beta)
    + \beta \E(x) \right) p_X(x) \, d\mu(x)$ \ es decir
  %
  \[
  S = - \int_\X p_X(x) \, \log p_X(x) \, d\mu(x)
  \]
  %
  Volveremos    en    esta    definici\'on    de    la    entrop\'ia    en    el
  capitulo~\ref{Cap:SZ:Informacion} en un marco m\'as general.
\end{ejemplo}

\SZ{Hablar de estadistica suficiente m\'inima?}
% Ver information geometry
% https://en.wikipedia.org/wiki/Partition_function_(mathematics)

%\SZ{Van den Bos 2007, p. 33, informaci\'on de Fisher}
