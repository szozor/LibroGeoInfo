\subsubseccion{Ley hipergeometrica multivariada}
\label{Sssec:MP:HipergeometricaMultivariada}

Esta ley aparece por ejemplo cuando  se generaliza la ley hipergeom\'etrica con $c
> 2$ clases \ con \ $k_i$ \ num\'ero de elementos de la clase \ $i$, $\sum_i k_i
= n$.   Se estudia esta ley,  entre otros, por la  primera vez, en  el ensayo de
Montmort  en  1708~\cite{Mon13},  o  m\'as   tarde,  en  1740,  en  trabajos  de
Simpson~\cite{Sim40, Hal90, DavEdw01}.

Se denota \ $X \, \sim \, \H\M(n,k,m)$ \ con \ $\displaystyle n \in \Nset$, \quad
$k = \begin{bmatrix} k_1 & \cdots & k_c\end{bmatrix}^t \in \Part{n}{c}$ (ver notaciones)
% \left\{ \{ 0 \; \ldots  \; n\}^c  \tq \sum_{i=1}^c k_i  = n \right\}$,
  \quad $m \in  \{ 0 \;  \ldots \;
n\}$.

Entonces, como en el caso de la ley multinomial, a pesar de que se escribe \ $X$
\  de manera  $c$-dimensional, el  vector partenece  a un  espacio  claramente \
$(c-1)$-dimensional.  Notar que  en el  caso \  $c  = 2$  \ se  recupera la  ley
hipergeom\'etrica.

Sus caracter\'isticas son las siguientes:

\begin{caracteristicas}
%
Dominio de definici\'on & $\displaystyle \X = \left\{ \left. x
\in \optimes_{i=1}^c \{ 0 \; \ldots \; k_i \} \, \right| \, \sum_{i=1}^c x_i = m
\right\}$\\[2mm]
\hline
%
Par\'ametros & $n \in \Nset_0$ \:
(poblaci\'on)\newline $c \in \Nset_0$ \: (n\'umero de clases)\newline
$\displaystyle k \in \Part{n,c}$ 
%\left\{ q \in \{ 0 \; \ldots \; n\}^c \tq \sum_{i=1}^c q_i
%= n \right\}$ 
\ (n\'umeros de elementos de cada clase)\newline $m \in \{ 0 \;
\ldots \; n\}$ \: (n\'umero de tiros)\\[2mm]
\hline
%
%Distribuci\'on de probabilidad
Funci\'on de masa &
\protect$\displaystyle p_X(x) = \frac{\prod_{i=1}^c
\smallbino{k_i}{x_i}}{\smallbino{n}{m}}$\protect\\[2mm]
\hline
%
Promedio & $\displaystyle m_X = \frac{m}{n} \, k$\\[2mm]
\hline
%
Covarianza
%~\footnote{Ver notas de pie~\ref{Foot:MP:HipergeometricaVarianza} y~\ref{Foot:MP::MultinomialCovarianza}.} 
& $\displaystyle \Sigma_X = \left\{
\protect\begin{array}{ccc} \frac{m (n-m)}{n^2 (n-1)} \left( n \Diag(k) - k k^t
\right) & \mbox{si} & n > 1\\ 0 & \mbox{si} & n = 1\end{array}\protect\right.$
%\\[2mm]
%\hline
%
%
%Generadora  de probabilidad~\footnote{Ver nota de pie~\ref{Foot:MP:MultinomialGeneProba}
%reemplazando $n$ por $m$.}  &  $\displaystyle  G_X(z) =  \left(  1 -  p  + p  z
%\right)^n$ \ sobre \ $\Cset$\\[2mm]
%\hline
%%
%Generadora  de momentos~\footnote{Ver nota de pie~\ref{Foot:MP:MultinomialGeneMomentos}
%reemplazando $n$ por $m$.}  &  $\displaystyle  M_X(u) =  \left(1  - p  +  p \,  e^u
%\right)^n$ \ sobre \ $\Cset$\\[2mm]
%\hline
%%
%Funci\'on caracter\'istica~\footnote{Ver nota de pie~\ref{Foot:MP:MultinomialCaracteristica}
%reemplazando $n$ por $m$.}  & $\displaystyle \Phi_X(\omega) =  \left( 1 -  p + p
%\, e^{\imath \omega} \right)^n$
\end{caracteristicas}

De hecho, se puede considerar que el vector aleatorio es \ $(c-1)$-dimensional \
$\widetilde{X}     =    \begin{bmatrix}     \widetilde{X}_1    &     \cdots    &
  \widetilde{X}_{c-1}   \end{bmatrix}^t$   \  definido   sobre   el  dominio   \
$\displaystyle    \widetilde{\X}   =    \left\{    \left.   \widetilde{x}    \in
    \optimes_{i=1}^{c-1} \{ 0 \; \ldots \; k_i\} \, \right| \, \max\left( 0 \, ,
    \,  \sum_{i=1}^{c-1}  k_i  +  m  -  n \right)  \:  \le  \:  \sum_{i=1}^{c-1}
  \widetilde{x}_i \: \le \: m \right\}$. Los par\'ametros de \ $\widetilde{X}$ \
son  entonces \  $n \in  \Nset_0$, \  $m \in  \{  0 \;  \ldots \;  n \}$  \ y  \
$\widetilde{k}     =     \protect\begin{bmatrix}      k_1     &     \cdots     &
  k_{c-1} \end{bmatrix}^t\protect \in \left\{ q \in \{ 0 \; \ldots \; n \}^{c-1}
  \tq \sum_{i=1}^{c-1} q_i  \, \le \, n \right\}$. A  continuaci\'on, la masa de
probabilidad   de  \   $\widetilde{X}$  \   es  naturalmente   \  $\displaystyle
p_{\widetilde{X}}(x)  =  \frac{\prod_{i=1}^{c-1} \bino{k_i\vspace{1mm}}{x_i}  \,
  \smallbino{n   -   \sum_{i=1}^{c-1}   k_i\vspace{2mm}}{m  -   \sum_{i=1}^{c-1}
    x_i}}{\bino{n}{m}}$.

Similarmente al  caso multinomial, se puede ver  que $\Sigma_X \un =  0$ \ as\'i
que \ $\Sigma_X \notin \Pos_k^+(\Rset)$.  De nuevo, es la consecuencia directa del
hecho   de  que   \  $X$   \  $c$-dimensional,   vive  sobre   una   variedad  \
$(c-1)$-dimensional. Aparentemente, siendo $\Sigma_X$ no invertible, no se puede
definir  ni  asimetr\'ia, ni  curtosis.  Sin  embargo,  habr\'ia para  esta  ley
tambi\'en  que  considerar  \   $\widetilde{X}$,  de  promedio  $\frac{m}{n}  \,
\widetilde{k}$ y de covarianza el bloque $(c-1) \times (c-1)$ de $\Sigma_X$, que
es ahora  invertible. $\gamma_{\widetilde{X}}$ \ y  \ $\kappa_{\widetilde{X}}$ \
son bien definidos.  Las expresiones, demasiado pesadas, no  son dadas ac\'a (se
deja al lector como exercicio).


\SZ{Ver si se calcula Phi}

La masa  de probabilidad es representada  en la
figura Fig.~\ref{Fig:MP:HipergeometricaMultivariada}.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/HipergeometricaMultivariada} \end{center}
%
\leyenda{Ilustraci\'on de  una distribuci\'on de  probabilidad hipergeom\'etrica
  multivariada  para   \  $c  =  3$   \  del  vector   \  $(c-1)$-dimensional  \
  $\widetilde{X} = \protect\begin{bmatrix}  X_1 & X_2 \protect\end{bmatrix}^t$ \
  ($X_3 = m-X_1-X_2$) \ con las marginales \ $p_{X_1}, \: p_{X_2}$.  Es dibujada
  solamente la distribuci\'on sobre $\widetilde{\X}$, siendo esta cero afuera de
  $\widetilde{\X}$.   Los par\'ametros  son  \ $n  =  18$, \  $m =  5$,  \ $k  =
  \protect\begin{bmatrix}  9  &  6   &  3  \protect\end{bmatrix}^t$  (a),  $k  =
  \protect\begin{bmatrix} 6 & 6 & 6 \protect\end{bmatrix}^t$ (b).}
\label{Fig:MP:HipergeometricaMultivariada}
\end{figure}


Notar: cuando $c = 2$ se  recupera la ley hipergeom\'etrica; adem\'as $X$ resuelta
cierta   en  los  casos   siguientes  (ver   subesecci\'on  anterior   para  las
explicaciones/ilustraciones):
%
\begin{itemize}
\item $m =  0 \: \Rightarrow \: X = 0$;
%
\item  $m = n \: \Rightarrow \: X = k$;
%
\item $k = n \un_i \: \Rightarrow \: X = m \un_i$.
\end{itemize}

% \SZ{casos particulares a ver; caso de propiedades de reflexividad a ver (igual
%   para la multinomial)}

Vectores  de  distribuci\'on  hipergeom\'etricas multivaluada  tienen  propiedades
notables similares a las de la  hipergeom\'etricas y de la multinomial, a saber de
tipo reflexividad, con respecto a una permutaci\'on de variable y con respecto a
una agregaci\'on.
%
\begin{lema}[Reflexividad]
\label{Lem:MP:ReflexividadHipergeomMulti}
%
  Sea \ $X \, \sim \, \H\M(n,k,m)$. Entonces
  %
  \[
  k-X \sim \H\M(n,k,n-m)
  \]
  %
\end{lema}
%
\begin{proof}
  %El  primer   resultado  es  inmediato  de  $P(m-X   =  x)  =  P(X   =  m-x)  =
  %\frac{\smallbino{k}{m-x} \smallbino{n-k}{x}}{\smallbino{n}{m}}$. El secundo de
  Sea $Y =  k-X$. De $P(Y = y) =  P(k-X = y) = P(X  = k-y) = \frac{\prod_{i=1}^c
    \smallbino{k_i}{k_i-y_i}}{\smallbino{n}{m}}       =      \frac{\prod_{i=1}^c
    \smallbino{k_i}{y_i}}{\smallbino{n}{n-m}}$   notando   que  $\bino{a}{b}   =
  \bino{a}{a-b}$. Se cierra la prueba  recordandose que \ $\sum_{i=1}^c k_i = n$
  \  y  \ $\sum_{i=1}^c  x_i  =  m$,  dando \  $\sum_{i=1}^c  k_i  =  n$ \  y  \
  $\sum_{i=1}^c y_i = n-m$.
\end{proof}
%
Como  el en  contexto  escalar, si  en una  urna  tenemos bolas  de $c$  colores
diferentes,  con  un n\'umero  $k_i$  para el  $i$-\'esimo  color,  $X_i$ es  el
num\'ero de este color que se sorte\'o y $k_i-X_i$ representan las de este color
que quedan en la urna, entre las  \ $n-m = \sum_{i=1}^c (k_i-X_i)$ \ que quedan,
es decir que en \ $k-X$ \ se  intercambia los roles de las bolas sorteadas y las
que quedan en la urna.

\begin{lema}[Efecto de una permutaci\'on]\label{Lem:MP:PermutacionHipergeomMulti}
%
  Sea  \ $X =  \begin{bmatrix} X_1  & \cdots  & X_c  \end{bmatrix}^t \,  \sim \,
  \H\M(n,k,m)$  \   y  \   $\Pi  \in  \Perm_c$   \  matriz   \  de
  permutaci\'on. Entonces
  %
  \[
  \Pi X \, \sim \, \H\M\left( n ,  \Pi k , m \right)
  \]
  %
\end{lema}
%
\begin{proof}
  La  prueba sigue  paso paso  la de  la multinomial.  Notando la  permutation \
  $\sigma$ \  tal que \ $\Pi  = \sum_{i=1}^c \un_i  \un_{\sigma(i)}^t$, se puede
  ver   que  \  $\displaystyle   P(\Pi  X   =  x)   =  P(X   =  \Pi^{-1}   x)  =
  \frac{\prod_{i=1}^c       \bino{k_i}{x_{\sigma^{-1}(i)}}}{\bino{n}{m}}       =
  \frac{\prod_{i=1}^c \bino{k_{\sigma(i)}}{x_i}}{\bino{n}{m}} $  \ por cambio de
  indices.
\end{proof}
%
\begin{lema}[Stabilidad por agregaci\'on]\label{Lem:MP:StabAgregacionHipergeomMulti}
%
  Sea  \ $X =  \begin{bmatrix} X_1  & \cdots  & X_c  \end{bmatrix}^t \,  \sim \,
  \H\M(n,k,m)$ \ y \ $G^{(i,j)}$ \ matriz de agrupaci\'on de las $(i,j)$-\'esima
  componentes (ver notaciones). Entonces,
  %
  \[
  G^{(i,j)} X \, \sim \, \H\M\left( n , G^{(i,j)} k , m \right)  
  \]
  %
\end{lema}
%
Este resultado es intuitivo del hecho que  vuelve a agrupar las clases \ $i$ \ e
\ $j$ \ en una clase, que tiene entonces \ $k_i + k_j$ \ elementos.
%

%
\begin{proof}
 Del  lema precediente,
  notando  que  existen  matrices  de permutaci\'on~\footnote{$\Pi_k$  pone  las
    componentes $i$ \ e \ $j$ el  las posiciones $c-1$ y $c$, sin cambiar el orden
    de  las  precedientes;  $\Pi_{k-1}$  trazlada  la  \'ultima  componente  en  la
    posici\'on $\min(i,j)$.}  \ $\Pi_k \in  \Perm_k$ \ y \ $\Pi_{k-1} \in
  \Perm_{k-1}$ \ tal que \ $G^{(i,j)} = \Pi_{k-1} \, G^{(c-1,c)} \, \Pi_k$,
  se puede concentrarse en el caso \ $(i,j) = (c-1,c)$. Ahora, claramente,
  %
  \begin{eqnarray*}
  P(G^{(c-1,c)} X = x) & = & \displaystyle P\left( \bigcap_{i=1}^{c-2} \big( X_i = x_i \big)
  \: \cap \: \big( X_{c-1} + X_c = x_{c-1} \big) \right)\\[2mm]
  %
  & = & \displaystyle \sum_{t=0}^{x_{c-1}} P\left( \bigcap_{i=1}^{c-2} \big( X_i = x_i \big)
  \: \cap \: \big( X_{c-1} = t\big) \: \cap \: \big( X_c = t-x_{c-1} \big) \right)\\[2mm]
  %
  & = & \frac{\prod_{i=1}^{c-2} \bino{k_i}{x_i}}{\bino{n}{m}}
  \: \sum_{t=0}^{x_{c-1}} \bino{k_{c-1}}{t}\bino{k_c}{x_c-t}
  \end{eqnarray*}
  %
  Se  cierra  la  prueba   de  la  identidad  de  Chu-Vandermonde~\footnote{Este
    identidad es  debido a A.-T. Vandermonde  en 1772, pero  esta conocida desde
    1303 por  el matem\'atico chino Chu Shi-Chieh,  explicando la denominaci\'on
    de  este   identidad~\cite{AndLar94}  o~\cite[p.~59-60]{Ask75}.   Se  prueba
    escribiendo $(1+x)^{r+s} = (1+x)^r  (1+x)^s$ y desallorando con la f\'ormula
    del     binomio    cada     potencia.}      $\displaystyle    \sum_{t=0}^{l}
  \bino{r}{t}\bino{s}{l-t}   =  \bino{r+s}{l}$~\cite[Ec.~(21),  p.~59]{Knu97_v1}
  o~\cite[Ec.~0.156]{GraRyz15}.
\end{proof}

De este lema, aplicado de manera recursiva, se obtiene el corolario siguiente:
%
\begin{corolario}\label{Cor:MP:MarginalMultinomial}
%
  Sea  \ $X  \,  \sim \,  \H\M(n,k,m)$, entonces  \  $\displaystyle X_i  \, \sim  \,
  \H(n,k_i,m)$.
\end{corolario}

% Cuando  $n  = 1$,  se  recupera  la lei  de  Bernoulli  $\B(p) \equiv  \B(1,p)$.
% Ad\'emas, se muestra  sencillamente usando la generadora de  probabilidad que
% %
% De este resultado,  se puede notar que, por  ejemplo, le distribuci\'on binomial
% aparece en el conteo de eventos independientes de misma probabilidad entre $n$.

% Tambi\'en,  la ley binomial  tiene una  propiedad de  reflexividad, consecuencia
% directa de la de Bernoulli:
% %
% \begin{lema}[Reflexividad]
% \label{Lem:MP:ReflexividadBinomial}
% %
%   Sea \ $X \, \sim \, \B(n,p)$. Entonces
%   %
%   \[
%   n-X \, \sim \, \B(n,1-p)
%   \]
%   %
% \end{lema}

% Nota que cuando $p = 0$ (resp. $p = 1$) la variable es cierta $X = 0$ (resp.  $X
% = n$).


\

Nota: esta  ley se  generaliza de  la misma manera  que para  la hipergeom\'etrica
negativa,  dando una  ley  hipergeom\'etrica negativa  multivariada  o, de  manera
equivalente, generalizando la hipergeom\'etrica negativa  a m\'as de dos clases se
obtiene la ley hipergeom\'etrica negativa. \SZ{Anadirlo en una seccion?}
