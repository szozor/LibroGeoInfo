\subsubseccion{Distribuci\'on uniforme sobre un intervalo}
\label{Sssec:MP:UniformeContinua}

Esta distribuci\'on es  la m\'as natural que se usa  cuando queremos modelar una
falta de  informaci\'on sobre una variable,  sabiendo que vive en  un espacio de
volumen  finito: sin  a  priori  m\'as, una  tendencia  natural/intuitiva es  de
asignar la  ``misma probabilidad''  a cada punto  del conjunto.   En particular,
aparece as\'i  naturalmente en  la inferencia bayesiana  que consiste  a modelar
como aleatorio un par\'ametro  que se quierre inferir~\footnote{En la inferencia
  bayesiana, nos interesamos al paremetro (posiblemente multivariado) \ $\theta$
  \ subyancente  a una distribuci\'on. Por ejemplo,  sabemos tener observaciones
  sorteados  de  una  distribuci\'on  de  Poisson, pero  con  el  par\'ametro  \
  $\lambda$ \  desconocido y  nos interesamos a  \ $\theta \equiv  \lambda$.  El
  enfoque bayesiano consiste a considerar el par\'ametro \ $\Theta$ \ aleatorio,
  tal que la  distribuci\'on de las observaciones sea  vista como distribuci\'on
  condicional \  $p_{X|\Theta = \theta}(x)$, llamada  distribuci\'on de sampleo.
  Dadas las  observaciones $X = x$,  la meta es de  determinar la distribuci\'on
  dicha a posteriori \  $p_{\Theta|X = x}$ \ a partir de  la cual se puede hacer
  estimaci\'on  de \ $\theta$  dadas las  observaciones, calcular  intervalos de
  confianza, etc. Se interpreta  como distribuci\'on explicando el par\'ametro a
  partir de  las observaciones.  Por eso, el  metodo se apoya sobre  la regla de
  Bayes $p_{\Theta|X=x}(\theta) \propto p_{X|\Theta=\theta}(x) p_\Theta(\theta)$
  \  as\'i que  se necesita  elegir una  distribuci\'on \  $p_\Theta$ \  dicha a
  priori.\label{Foot:MP:BayesPrior} }
%~\footnote{A  partir de una
%  distribuci\'on  parametrizada   por  un  par\'ametro   $\theta$.   El  enfoque
%  bayesiano consiste  a modelizar $\theta$ aleatorio, digamos  $\Theta$, tal que
%  la     distribuci\'on     de     observaciones     se     escribe     entonces
%  $p_{X|\Theta=\theta}(x)$.   Inferir  $\theta$ a  partir  de observaciones  $x$
%  consiste   a   determinar  la   distribuci\'on   dicha   {\it  a   posteriori}
%  $p_{\Theta|X=x}(\theta)$.  Por eso, hace  falta darse una distribuci\'on dicha
%  {\it a priori} $p_\Theta(\theta)$.\label{Foot:SZ:BayesPrior}}
~\cite{Rob07} (la
ley  es dicha  ley  {\em a  priori};  ver tambi\'en~\cite{Bay63}  o~\cite{Lap12,
  Lap14, Lap20}; tal a priori es conocido como a priori de Laplace).

Se denota $X \, \sim \, \U([a \; b])$. Las caracter\'isticas de \ $X$ \ son las
siguientes:

\begin{caracteristicas}
%
Dominio de definici\'on & $\X = [a \; b]$\\[2mm]
\hline
%
Par\'ametros & $(a,b) \in \Rset, \: b > a$\\[2mm]
\hline
%
Densidad de probabilidad & $p_X(x) = \frac{1}{b-a}$\\[2mm]
\hline
%
Promedio & $\displaystyle m_X = \frac{a+b}{2}$\\[2mm]
\hline
%
Varianza & $\displaystyle \sigma_X^2 = \frac{(b-a)^2}{12}$\\[2mm]
\hline
%
\modif{Asimetr\'ia} & $\gamma_X = 0$ \quad para \ $b \ne a$\\[2mm]
\hline
%
Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = -\frac65$ \quad para \ $b \ne a$\\[2mm]
\hline
%
Generadora de momentos & $\displaystyle M_X(u) = \frac{ e^{b u} - e^{a u}}{u}$ \quad
para~\footnote{En el caso l\'imite \ $u \to  0$, \ $\lim_{u \to 0} \frac{ e^{b u}
- e^{a u}}{u} = b-a$, y similarmente para la funci\'on caracter\'istica}  \ $u \in \Cset$\\[2mm]
\hline
%
Funci\'on caracter\'istica & $\displaystyle  \Phi_X(\omega) = \frac{ e^{\imath a
\omega} - e^{\imath b \omega}}{\imath \, \omega}$
\end{caracteristicas}

% Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
% Momento factorial & $\Esp\left[ (X)_k \right] = ?$\\[2mm]
% Generadora de probabilidad & $G_X(z) = e^{\lambda (z-1)}$ \ para \ $z \in \Cset$\\[2mm]
% modo 0
% Mediana \ln(2)/\lambda
% CDF 1-e^{-\lambda x}

Obviamente, se puede escribir \ $X \, \egald  \, a + (b-a) U$ \ donde \ $\egald$
\ significa que la equalidad es en distribuci\'on (las variables tienen la misma
distribuci\'on de probabilidad), con \ \ $U \, \sim \, \U \left( [ 0 \; 1 ]
\right)$ \ llamada {\em uniforme estandar}.

La densidad de probabilidad y funci\'on de repartici\'on de la variable estandar
son representadas en la figura Fig.~\ref{Fig:MP:Uniformecontinua}.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/UniformeContinua} \end{center}
% 
\leyenda{Ilustraci\'on  de  una densidad  de  probabilidad  uniforme  (a), y  la
funci\'on de repartici\'on asociada (b).}
\label{Fig:MP:Uniformecontinua}
\end{figure}

Una nota  importante es que cada ley  continua es v\'inculada a  la ley uniforme
sobre $(0 \; 1)$ de la manera siguiente:
%
\begin{lema}[Inversi\'on]\label{Lem:MP:InversionUniforme}
Sea $X$, continua sobre $\X \subset \Rset$, de funci\'on de repartici\'on $F_X$. Entonces
%
\[
U \equiv F_X(X) \sim \U(0 \; 1)
\]
%
Reciprocamente, definiendo la funci\'on de repartici\'on inversa (o quantile)
%
\[
F_X^{-1}(u) = \inf \{ x \tq F(x) \ge u \}
\]
%
si \ $V \sim \U( 0 \; 1 )$,
%
\[
Y = F_X^{-1}(V) \quad \Rightarrow \quad F_Y(y) = F_X(y)
\]
\end{lema}
%
Cuando  $F_X$ se  inversa  sencillamente eso  da  una manera  sencilla de  tirar
sampleos de funci\'on de repartici\'on $F_X$ a partir de sampleos tirados seg\'un
una ley uniforme.
%
\begin{proof}
Inmediatamente, $F_X$ siendo creciente,
%
\begin{eqnarray*}
P(U \le u) & = &  P( F_X(X) \le u)\\[2mm]
%
& = & P(X \le F_X^{-1}(u))\\[2mm]
%
& = & F_X\left( F_X^{-1} (u) \right)
\end{eqnarray*}
%
Similarmente
%
\begin{eqnarray*}
P(Y \le y) & = &  P( F_X^{-1}(V) \le y)\\[2mm]
%
& = & P(V \le F_X(y))\\[2mm]
%
& = & F_X(y)
\end{eqnarray*}
%
\end{proof}

De manera  general, para  cualquier ensemble $\D  \varsubsetneq \Rset^d$ de  volumen \
finito $|\D|$ \,  la variable uniforma sobre $\D$  tiene la densidad con  respecto a la
medida  ``natural'' sobre  $\D$  (Lebesque, discreta,\ldots)  constante sobre  \
$\D$,
%
\[
p_X(x) = \frac{1}{|\D|} \un_{\D}(x)
\]
%
La media va a ser el centro de gravedad de $\D$.

Vamos a ver en el cap\'itulo~\ref{Cap:SZ:Informacion} que esta distribuci\'on es
la distribuci\'on definida  sobre un conjunto de volumen  finito que maximiza la
entrop\'ia, \ie  que es la  ``menos informativa''. Por  ejemplo, si se  busca un
par\'ametro  modelizado como  aleatorio (enfoque  bayesiano), definido  sobre un
conjunto de volumen finito, sin  a priori m\'as, una tendencia natural/intuitiva
es de  asignar la ``misma probabilidad'' a  cada punto del conjunto.  Es por eso
que aparece as\'i naturalmente en la inferencia bayesiana~\cite{Rob07}.
 
Notar que  cuando $b \to a$,  la variable tiende a  una variable cierta  $X = a$
(ver principio de esta secci\'on).
