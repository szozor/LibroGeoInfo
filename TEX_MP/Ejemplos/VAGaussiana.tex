\subsubseccion{Distribuci\'on normal o Gaussiana multivariada real}
\label{Sssec:MP:Gaussiana}

En el caso escalar,  esta ley parece aparecer por unas de  las primeras veces en
trabajos de de  Moivre como approximaci\'on de la ley  binomial para $n$ grande,
usando la formula de Stirling~\cite{Moi30, Moi33, Moi56, Pea24, PeaMoi26, Dem33,
  Hal84,  Hal90,  JohKot95:v1, DavEdw01,  Hal06}.   Se  puede  ver tambi\'en  el
trabajo de F.  Galton, quien contruy\'o un experimento, la caja dicha de Galton,
que ilustra  por una parte como  se puede obtener  la ley binomial como  suma de
Bernoulli,  y  la  convergencia  a  la  Gausiana~\cite[Figs.~7-9,  p.~63]{Gal89}
o~\cite[p.~38]{Pea20}.  Aparte de Moivre,  la ley gausiana fue desarollado mucho
por los  matem\'aticos como Gauss en  el estudio del movimiento  de planetas con
perturbaciones (predicci\'on  de la trayectoria  de C\'eres)~\cite{Gau09, Pea24,
  DavEdw01,  Hal06},  basado  en   trabajos  de  A.   M.   Legendre~\cite{Leg05,
  DavEdw01,  Hal06},   o  Laplace   en  mismo  tipos   de  problema~\cite{Lap09,
  Lap09:Supp,  Lap12,   Lap14,  Lap20,  Pea24,  DavEdw01,   Hal06}.   De  hecho,
apoyandose en  trabajos de  de Moivre, la  formaliz\'o antes y  m\'as claramente
Laplace,    quien    revandic\'o     entonces    su    partenidad    (ver    por
ejemplo~\cite{Pea20}).   Por eso,  esta ley  es tambi\'en  conocida como  ley de
Laplace-Gauss.

En el contexto multivariado, la extensi\'on natural de la ley binomial siendo al
ley multinomial, es sin sorpresa  que se introdujo la gausiana multivaluada como
approximaci\'on de la multinomial.  Este trabajo  es debido entre otros a J.  L.
Lagrange en  los a\~nos 1770, con  correcciones debido unas  decadas despu\'es a
A. de  Morgan~\cite{Mor38}. Pero apareci\'o  antes en el caso  bidimensional, en
particular  a  trav\'es  del  estudio  del coeficiente  de  correlaci\'on  entre
variables   aleatorias  (ver   por  ejemplo   trabajos   de  Galton~\cite{Gal77,
  Gal77:Nature, Pea20}).

A pesar de que parece menos  natural en la modelisaci\'on de fenomenos aleatorio
que leyes uniformes, la ley gausiana es seguramente unas de las m\'as importante
en  probabilidad, sino  que  la m\'as  importante  y la  m\'as  expendida en  la
naturaleza.  Eso  viene sin  duda  del teorema  del  l\'imite  centrale. En  dos
palabras,  cuando  se  suman   un  numero  importante  de  variables  aleatorias
(independientes,   de  misma  ley,   admitiendo  una   varianza,  o   con  menos
restricciones~\cite[Cap.~11]{AthLah06}),  corectamente  normalizado,  esta  suma
tiende a una gausiana~\footnote{De hecho,  la approximaci\'on de la ley binomial
  por una  gausiana cuando  $n$ es  grande es una  caso particular  del teorema,
  siendo la binomial  una suma de Bernoulli independientes.}.  En la naturaleza,
se puede ver el ruido (se\~nales) como suma de un n\'umero importante de fuentes
de  ruido independientes,  justificando el  modelo  gausiano~\cite{Fel71, Cam86,
  AshDol99, JacPro03, AthLah06, Ren07, Bil12}.  Ad\'emas, como lo vamos a ver en
el  capitulo~\ref{Cap:SZ:Informacion},  esta ley  es  la  de incerteza  m\'axima
(maximizando la entrop\'ia) teniendo  una dada varianza. Aparece naturalmente en
termod\'inamica    (gaz    perfecto,   con    un    n\'umero    muy   alto    de
particulas)~\cite{Max67, Bol96,  Bol98, Gib02, Jay65}. En  estimaci\'on, bajo la
hipotesis  gausiana,  los  estimadores  de  par\'ametros  minimizando  el  error
cuadratico  promedio son generalmente  lineal~\cite{Kay93, Rob07}.   Todas estas
consideraciones  dan a la  ley gausiana  un rol  central en  la teor\'ia  de las
probabilidades.

Se denota \  $X \, \sim \, \N(m,\Sigma)$ \  con \ $m \in \Rset^d$  \ y \ $\Sigma
\in  P_d^+(\Rset)$  \  conjunto  de   las  matrices  de  \  $\M_{d,d}(\Rset)$  \
s\'imetricas definidas positivas. Las  caracter\'isticas de la Gaussiana son las
siguientes:

\begin{caracteristicas}
%
Dominio de definici\'on & $\X = \Rset^d$\\[2mm]
\hline
%
Par\'ametros & $m \in \Rset^d, \:\: \Sigma \in P_d^+(\Rset)$\\[2mm]
\hline
%
Densidad de probabilidad & $\displaystyle p_X(x) = \frac{1}{(2
\pi)^{\frac{d}{2}} \left| \Sigma \right|^{\frac12}} \, e^{-\frac12 (x-m)^t
\Sigma^{-1} (x-m)}$\\[2.5mm]
\hline
%
Promedio & $ m_X = m$\\[2mm]
\hline
%
Covarianza & $\Sigma_X = \Sigma$\\[2mm]
\hline
%
\modif{Asimetr\'ia} & $\gamma_X = 0$\\[2mm]
\hline
%
Curtosis por exceso & $\widebar{\kappa}_X = 0$\\[2mm]
\hline
%
Generadora de momentos & $\displaystyle M_X(u) = e^{u^t \Sigma u + u^t m}$ \
para \ $u \in \Cset^d$\\[2mm]
\hline
%
Funci\'on  caracter\'istica   &  $\displaystyle  \Phi_X(\omega)   =  e^{-\frac12
\omega^t \Sigma \omega + \imath \omega^t m}$
\end{caracteristicas}

Nota: trivialmente, se puede escribir $X  \, \egald \, \Sigma^{\frac12} N + m$ \
con \ $N \, \sim \, \N(0,I)$ \  donde \ $N$ \ es dicha {\em Gausiana estandar} o
{\em centrada-normalizada}. Las caracter\'isticas de  \ $X$ \ son v\'inculadas a
las  de  \  $N$ \  (y  vice-versa)  por  transformaci\'on afine  (ver  secciones
anteriores).


La densidad de probabilidad gausiana y  la funci\'on de repartici\'on en el caso
escalar son  representadas en la figura Fig.~\ref{Fig:MP:Gaussiana}-(a)  y (b) y
una      densidad      en       un      contexto      bi-dimensional      figura
Fig.~\ref{Fig:MP:Gaussiana}(c).
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Gaussiana} \end{center}
% 
\leyenda{Ilustraci\'on  de  una   densidad  de  probabilidad  gaussiana  escalar
  estandar  (a), y la  funci\'on de  repartici\'on asociada  (b), as\'i  que una
  densidad  de probabilidad  gaussiana bi-dimensional  centrada y  de  matriz de
  covarianza \ $\Sigma_X = R(\theta)  \Delta^2 R(\theta)^t$ \ con \ $R(\theta) =
  \protect\begin{bmatrix}   \cos\theta  &   -  \sin\theta\\[2mm]   \sin\theta  &
    \cos\theta  \protect\end{bmatrix}$ \  matriz  de rotaci\'on  y  \ $\Delta  =
  \diag\left(\protect\begin{bmatrix}  1   &  a\protect\end{bmatrix}  \right)$  \
  matriz  de   cambio  de  escala,   y  sus  marginales   \  $X_1  \,   \sim  \,
  \N\left(0,\cos^2\theta  + a^2  \sin^2\theta \right)$  \ y  \ $X_2  \,  \sim \,
  \N\left(0,\sin^2\theta + a^2 \cos^2\theta  \right)$ \ (ver m\'as adelante). En
  la figura, $a = \frac14$ \ y \ $\theta = \frac{\pi}{6}$.}
\label{Fig:MP:Gaussiana}
\end{figure}

La gaussiana tiene un par de propiedades particulares:
%
\begin{teorema}[Stabilidad]
\label{Teo:MP:StabilidadGaussiana}
%
  Sean \ $A_i , i = 1,\ldots,n$  \ matrices de \ $\M_{d',d}(\Rset), \: d' \le d$
  \ de rango lleno, $b_i \in \Rset^{d'}$ \ y \ $X_i \, \sim \, \N(m_i,\Sigma_i)$
  \ independientes, entonces
  %
  \[
  \sum_{i=1}^n \left(  A_i X_i  + b_i \right)  \, \sim \,  \N\left( \sum_{i=1}^n
    \left( m_i + b_i \right) \, , \, \sum_{i=1}^n A_i \Sigma_i A_i^t \right)
  \]
  % 
  En particular, cualquier combinaci\'on lineal  de los componentes de un vector
  Gaussiano da una gaussiana.  Reciprocamente, si cualquier combinaci\'on lineal
  de los componentes de un vector aleatorio sigue una ley gaussiana, entonces el
  vector es gaussiano.
\end{teorema}
%
\begin{proof}
  Este  resultato se proba  usando funci\'on  caracter\'istica de  la gaussiana,
  conjuntamente al teorema~\ref{Teo:MP:PropiedadesFuncionCaracteristica}.
\end{proof}
%
\begin{corolario}[Media empirica]\label{Cor:MP:MediaEmpiricaGauss}
%
  Sean \ $X_i \, \sim \, \N(m,\Sigma), \: i = 1, \ldots , n$ \ independientes. Entonces,
  %
  \[
  \overline{X} =  \frac{1}{n} \sum_{i=1}^n  X_i \,  \sim \, \N\left(  m \,  , \,
    \frac{1}{n} \Sigma \right)
  \]
   %
  $\overline{X}$  es llamada {\em  media empirica}~\footnote{Es  la estimaci\'on
    \'optima de  la media  $m$ a  partir de los  $X_i$ en  el sentido  del error
    cuadratico  promedio   m\'inimo,  o  en   el  sentido  de   la  verosimlitud
    m\'axima~\cite{Kay93, Rob07}.}, y es un estimador ``natural'' de la media de
  un vector aleatorio a partir de copias independientes de misma ley.
  %
\end{corolario}
%
\begin{teorema}[Independencia]
\label{Teo:MP:IndependenciaGaussiana}
%
  Sea   \   $X  \,   \sim   \,   \N(m,\Delta)$  \   con   \   $\Delta  =   \diag
  \left(  \begin{bmatrix}  \sigma_1^2  &  \cdots  &  \sigma_d^2  \end{bmatrix}^t
  \right)$   \  diagonal.   Entonces  las   componentes  \   $X_i  \,   \sim  \,
  \N(m_i,\sigma_i^2)$ \ son independientes.
\end{teorema}
%
\begin{proof}
  Este resultato se proba  trivialmente escribiendo la densidad de probabilidad,
  notando que se factorisa.
\end{proof}
%
Hemos visto que cuando un  vector tiene componentes independientes, la matriz de
covarianza  es   diagonal  (lema~\ref{Lem:MP:IndependenciaCov}),  pero   que  la
reciproca es falsa en general. El \'ultimo teorema muestra que la reciproca vale
en el caso gausiano.

Volvemos  ahora al rol  central de  la gausiana  como modelo  probabilistico muy
frecuente  de fenomeos  aleatorios. Este  rol particular  viene del  teorema del
l\'imite  central que  ya  introdujimos. A  veces  es conocido  como teorema  de
Lindenberg-Feller (por lo menos la forma con condiciones m\'as debiles que en la
formulaci\'on original).   Para unas de  las formulaciones originales,  se puede
referirse  al trabajo  de Laplace  de  1809 o  de 1912~\cite{Lap09,  Lap09:Supp,
  Lap12,  Lap14, Lap20}.   El nombre  ``central'' viene  de un  documento  de G.
P\'olya   de   1920,  titulado   ``\"Uber   den   zentralen  Grenzwertsatz   der
Wahrscheinlichkeitsrechnung  und das Momentenproblem''  (``Sobre el  teorema del
l\'imite central del  c\'alculo probabil\'istico y el problema  de los momentos;
el  teorema  es  central\ldots~\cite{Pol20,   Cam86}).   Se  enuncia  de  manera
siguiente~\cite{Spi76, BroDav87, LehCas98, AshDol99, JacPro03, AthLah06, Bil12}:
% ~\footnote{Aparte    en~\cite{AshDol99,   JacPro03},   el    teorema   aparece
%   frecuentemente en los libros en el contexto escalar, seguramente por razones
%   historicas.    Pero,  con   el  mismo   enfoque,  se   prueba  en   el  caso
%   multivariado.}:

\begin{teorema}[Teorema del l\'imite central]\label{Teo:MP:CLT}
%
  Sea  \  $\{  X_i \}_{i  \in  \Nset^*}$  \  una  serie de  vectores  aleatorios
  independientes, de misma ley,  y que admiten un promedio \ $m$  \ y una matriz
  de covarianza \ $\Sigma$. Entonces
  %
  \[
  \frac{1}{\sqrt{n}}  \sum_{i=1}^m  \left( X_i  -  m  \right)  \: \limitd{n  \to
    +\infty} \: Y \sim \N\left( 0 , \Sigma \right)
  \]
  %
  donde  \ $\limitd{}$ \  significa que  el l\'imite  es en  distribuci\'on (ver
  notaciones).
\end{teorema}
\begin{proof}
  Hay  varias  pruebas de  este  resultado.  Quiz\'as  la  m\'as  simple uas  la
  funci\'on c\'aracteristica.  Sin perdida de generalidad, supongamos que \ $m =
  0$. Sea \ $\displaystyle Y_n  = \frac{1}{n} \sum_{i=1}^m X_i$.  Sea \ $\omega$
  \       fijo.       Por       independencia       y       relaciones       del
  teorema~\ref{Teo:MP:PropiedadesFuncionCaracteristica}:
  %
  \begin{eqnarray*}
  \Phi_{Y_n}(\omega) & = & \left( \Phi_{X_i}\left( \frac{\omega}{\sqrt{n}}
  \right) \right)^n\\[2mm]
  %
  & = & \left( \Phi_{X_i}(0) + \frac{1}{\sqrt{n}} \omega^t \nabla_\omega
  \Phi_{X_i}(0) + \frac{1}{2 n} \omega^t \Hess_\omega\Phi_{X_i}(0) \omega +
  o\left( n^{-1} \right) \right)^n\\[2mm]
  %
  & = & \left( 1 - \frac{1}{2 n} \omega^t \Sigma \omega +
  o\left( n^{-1} \right) \right)^n\\[2mm]
  %
  & \xrightarrow[n \to +\infty]{} & \exp\left( -\frac12 \omega^t \Sigma \omega \right)
  \end{eqnarray*}
  %
  porque \ $\Phi_{X_i}(0) = 1$, $X_i$ \  siendo de media nula el gradiente de la
  funci\'on   caracter\'istica   se  cancela   en   \   $\omega   =  0$,   y   \
  $\Hess_\omega\Phi_{X_i}(0)  =  -  \Sigma$.   Se reconoce  ahora  la  funci\'on
  caracter\'istica   de  la   gausiana,   lo  que   prueba   que  la   funci\'on
  caracter\'istica  de  \  $Y_n$  \  converge  simplemente  hacia  la  funci\'on
  caracter\'istica  de la gausiana.   Se cierra  la pruba  usando el  teorema de
  convergencia de  L\'evy, diciendo que  la convergencia simple de  la funci\'on
  caracter\'istica  implica  la  convergencia en  distribuci\'on~\cite{AshDol99,
    Bil12, AthLah06}.
\end{proof}
%
En particular, la  media empirica hechas a partir  de vectores independientes de
media  $m$,  admitiendo  una  covarianza  \  $\Sigma$  \  y  de  misma  ley  (no
necesariamente gausiana), tiedne a ser gausiana  de media \ $m$ \ y covarianza \
$\frac{1}{n} \Sigma$.

Existen  varias   variantes  de  este   teorema  que  enunciamos,  sin   dar  la
prueba.  Dejamos el  lector a  libros m\'as  especializados como~\cite{AshDol99,
  Bil12, AthLah06, Lin22}.
% Lindeberg 1920

\begin{teorema}[Teorema de Lindenberg-Feller]\label{Teo:MP:LindenbergFeller}
%
  Sean  $\{  X_i \}_{i  \in  \Nset^*}$  vectores  aleatorios independientes,  no
  necesariamente de misma distribuci\'on de probabilidad, con \ $X_i$ \ de media
  \ $m_i = \Esp[X_i]$ \ y de matriz de covarianza \ $\Sigma_i \in P_d^+(\Kset)$.
  Sean \ $C_n = \sum_{i=1}^n \Sigma_i$, \ $c_n^2$ \ al autovalor m\'as peque\~na
  de $C_n$,  \ y  \ $Y_n  = C_n^{-\frac12} \sum_{i=1}^n  \left( X_i  - \Esp[X_i]
  \right)$.
  %
  \[
  \mbox{Si} \quad \lambda_n > 0 \quad \mbox{y} \quad \forall \: \varepsilon > 0,
  \quad  \lim_{n  \to  +\infty}  \sum_{i=1}^n  \Esp\left[  \left\|  \frac{X_i  -
        m_i}{c_n} \right\|^2  \un_{\left[ \varepsilon \;  +\infty \right)}\left(
      \left\| \frac{X_i - m_i}{c_n} \right\| \right) \right] = 0
  \]
  %
  entonces
  %
  \[
  Y_n \: \limitd{n \to +\infty} \: Y \sim \N\left( 0 , I \right)
  \]
\end{teorema}
%
En  numerosos libros,  este teorema  es  dado en  el caso  escalar. Se  extiende
sencillamente al caso multivariado gracia a lo que es conocido como {\it teorema
  de Cram\'er-Wold},  diciendo que una  secuencia de vectores aleatorios  \ $Y_n
\limitd{}  Y$ \  si y  solamente para  cualquier \  $u \in  \Rset^d$ \  $u^t Y_n
\limitd{} u^t Y$~\cite{AshDol99, AthLah06, Bil12}.

Sin dar  la prueba,  la condici\'on  de Lindenberg dice  que si  la suma  de las
``dispersi\'ones'' de  los vectores  normalizado por los  que es  basicamente la
varianza  m\'as  de  los componentes  de  la  suma  (una vez  diagonalizada)  se
concentra asintoticamente, la suma renormalizada de lo vectores centrados tiende
a la gausiana (en distribuci\'on).

Se  puede ver  que  se  satisface la  condici\'on  de Lindeberg  en  el caso  de
variables independientes de misma ley, del hecho  que \ $C_n = n \Sigma$, lo que
da \ $c_n^2 = n c^2$ \ con  \ $c^2$ \ autovalor m\'as peque\~na de \ $\Sigma$. A
continuaci\'on da la condici\'on \ $\displaystyle \lim_{n \to \infty} \Esp\left[
  \left\| X_i - m_i \right\|^2 \un_{\left[ \varepsilon \; +\infty \right)}\left(
    \left\|  \frac{X_i  -  m_i}{\sqrt{n}  c}  \right\|  \right)  \right]  =  0$,
satisfecha  porque el  argumento de  la funci\'on  indicadora tiende  a  0 (casi
siempre).

Un otra caso  ``trivial'' aparece cuando la secuencia  es uniformamente acotada,
\ie \  $\forall \: i, \quad  \| X_i \| \le  M$. Se puede  retomar los argumentos
anteriores, remplazando las variables por la cota.

Nota: si se satisface la condici\'on  dicha {\it de Lyapunov}, si existe $\delta
> 0$ tal que
%
\[
\lim_{n   \to  \infty}   \sum_{i=1}^n   \Esp\left[  \frac{\left\|   X_i  -   m_i
    \right\|^2}{c_n^{2+\delta}} \right] = 0,
\]
%
se satisface la de  Lindeberg~\cite{AshDol99}. Frecuentemente, es m\'as sencillo
verificar la condici\'on m\'as fuerte de Lyapunov para probar la convergencia de
una suma a la gausiana.

\

Al final, se puede a\'un debilitar la condici\'on de independencia sin perder la
convergencia a la gausiana~\cite[Sec.~6.4]{BroDav87}.
