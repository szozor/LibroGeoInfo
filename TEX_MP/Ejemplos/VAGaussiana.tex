\subsubseccion{Distribuci\'on normal o Gaussiana multivariada real}
\label{Sssec:MP:Gaussiana}

En el caso escalar,  esta ley parece aparecer por unas de  las primeras veces en
trabajos de de  Moivre como approximaci\'on de la ley  binomial para $n$ grande,
usando  la   formula  de  Stirling~\cite{Moi30,  Moi33,   Moi56,  Pea24,  Hal90,
  JohKot95:v1, Hal06}.  Se  puede ver tambi\'en el trabajo  de F.  Galton, quien
contruy\'o un  experimento, la caja dicha  de Galton, que ilustra  por una parte
como se puede obtener la ley  binomial como suma de Bernoulli, y la convergencia
a la  Gausiana~\cite[Figs.~7-9, p.~63]{Gal89} o~\cite[p.~38]{Pea20}.   Aparte de
Moivre, la ley  gausiana fue desarollado mucho por  los matem\'aticos como Gauss
en el estudio del movimiento  de planetas con perturbaciones (predicci\'on de la
trayectoria de C\'eres)~\cite{Gau09, Pea24, Hal06}, basado en trabajos de A.  M.
Legendre~\cite{Leg05, Hal06}, o Laplace  en mismo tipos de problema~\cite{Lap09,
  Lap09:Supp,  Lap20, Pea24,  Hal06}.  De  hecho, apoyandose  en trabajos  de de
Moivre,  la formaliz\'o  antes  y m\'as  claramente  Laplace, quien  revandic\'o
entonces  su partenidad  (ver por  ejemplo~\cite{Pea20}). Por  eso, esta  ley es
tambi\'en conocida como ley de Laplace-Gauss.

En el contexto multivariado, la extensi\'on natural de la ley binomial siendo al
ley multinomial, es sin sorpresa  que se introdujo la gausiana multivaluada como
approximaci\'on de la multinomial.  Este trabajo  es debido entre otros a J.  L.
Lagrange en  los a\~nos 1770, con  correcciones debido unas  decadas despu\'es a
A. de  Morgan~\cite{Mor38}. Pero apareci\'o  antes en el caso  bidimensional, en
particular  a  trav\'es  del  estudio  del coeficiente  de  correlaci\'on  entre
variables   aleatorias  (ver   por  ejemplo   trabajos   de  Galton~\cite{Gal77,
  Gal77:Nature, Pea20}).

A pesar de que parece menos natural en la modelisaci\'on de fenomenos aleatorio,
la ley  gausiana es  seguramente unas de  las m\'as importante  en probabilidad,
sino que  la m\'as importante y la  m\'as expendida en la  naturaleza. Eso viene
sin duda del teorema del l\'imite  centrale. En dos palabras, cuando se suman un
numero  importante  de  variables  aleatorias  (independientes,  de  misma  ley,
admitiendo  una varianza,  o con  menos restricciones~\cite[Cap.~11]{AthLah06}),
corectamente normalizado, esta suma tiende a una gausiana~\footnote{De hecho, la
  approximaci\'on de  la ley binomial por  una gausiana cuando $n$  es grande es
  una  caso particular del  teorema, siendo  la binomial  una suma  de Bernoulli
  independientes.}. En  la naturaleza,  se puede ver  el ruido  (se\~nales) como
suma de un n\'umero importante  de fuentes de ruido independientes, justificando
el  modelo  gausiano~\cite{Fel71, Cam86,  AshDol99,  JacPro03, AthLah06,  Ren07,
  Bil12}.      Ad\'emas,      como     lo     vamos     a      ver     en     el
capitulo~\ref{Cap:SZ:Informacion},  esta   ley  es  la   de  incerteza  m\'axima
(maximizando la entrop\'ia) teniendo  una dada varianza. Aparece naturalmente en
termod\'inamica    (gaz    perfecto,   con    un    n\'umero    muy   alto    de
particulas)~\cite{Max67, Bol96,  Bol98, Gib02, Jay65}. En  estimaci\'on, bajo la
hipotesis  gausiana,  los  estimadores  de  par\'ametros  minimizando  el  error
cuadratico  promedio son generalmente  lineal~\cite{Kay93, Rob07}.   Todas estas
consideraciones  dan a la  ley gausiana  un rol  central en  la teor\'ia  de las
probabilidades.

Se denota $X \, \sim \, \N(m,\Sigma)$ \  con \ $m \in \Rset^d$ \ y \ $\Sigma \in
P_d^+(\Rset)$ \  conjunto de las  matrices de \ $\M_{d,d}(Rset)$  \ s\'imetricas
definidas positivas. Las caracter\'isticas de la Gaussiana son las siguientes:

\begin{caracteristicas}
%
Dominio de definici\'on & $\X = \Rset^d$\\[2mm]
\hline
%
Par\'ametros & $m \in \Rset^d, \: \Sigma \in P_d^+(\Rset)$\\[2mm]
\hline
%
Densidad de probabilidad & $\displaystyle p_X(x) = \frac{1}{(2
\pi)^{\frac{d}{2}} \left| \Sigma \right|^{\frac12}} \, e^{-\frac12 (x-m)^t
\Sigma^{-1} (x-m)}$\\[2.5mm]
\hline
%
Promedio & $ m_X = m$\\[2mm]
\hline
%
Covarianza & $\Sigma_X = \Sigma$\\[2mm]
\hline
%
\modif{Sesgo} (caso escalar) & $\gamma_X = 0$\\[2mm]
\hline
%
Curtosis por exceso (caso escalar) & $\widebar{\kappa}_X = 0$\\[2mm]
\hline
%
Generadora de  momentos &  $\displaystyle M_X(u) =  e^{u^t \Sigma u + u^t m}$  \ para \  $u \in
\Cset^d$\\[2mm]
\hline
%
Funci\'on  caracter\'istica   &  $\displaystyle  \Phi_X(\omega)   =  e^{-\frac12
\omega^t \Sigma \omega + \imath \omega^t m}$
\end{caracteristicas}

% Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
% Momento factorial & $\Esp\left[ (X)_k \right] = ?$\\[2mm]
% Generadora de probabilidad & $G_X(z) = e^{\lambda (z-1)}$ \ para \ $z \in \Cset$\\[2mm]
% modo 0
% Mediana 0

Nota: trivialmente, se puede escribir $X  \, \egald \, \Sigma^{\frac12} N + m$ \
con \ $N \, \sim \, \N(0,I)$ \  donde \ $N$ \ es dicha {\em Gausiana estandar} o
{\em centrada-normalizada}. Las caracter\'isticas de  \ $X$ \ son v\'inculadas a
las  de  \  $N$ \  (y  vice-versa)  por  transformaci\'on afine  (ver  secciones
anteriores).


La densidad de probabilidad gausiana y  la funci\'on de repartici\'on en el caso
escalar son  representadas en la figura Fig.~\ref{Fig:MP:Gaussiana}-(a)  y (b) y
una      densidad      en       un      contexto      bi-dimensional      figura
Fig.~\ref{Fig:MP:Gaussiana}(c).
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Gaussiana} \end{center}
% 
\leyenda{Ilustraci\'on  de  una   densidad  de  probabilidad  gaussiana  escalar
  estandar  (a), y la  funci\'on de  repartici\'on asociada  (b), as\'i  que una
  densidad  de probabilidad  gaussiana bi-dimensional  centrada y  de  matriz de
  covarianza \ $\Sigma_X = R(\theta)  \Delta^2 R(\theta)^t$ \ con \ $R(\theta) =
  \protect\begin{bmatrix}   \cos\theta  &   -  \sin\theta\\[2mm]   \sin\theta  &
    \cos\theta  \protect\end{bmatrix}$ \  matriz  de rotaci\'on  y  \ $\Delta  =
  \diag\left(\protect\begin{bmatrix}  1   &  a\protect\end{bmatrix}  \right)$  \
  matriz  de   cambio  de  escala,   y  sus  marginales   \  $X_1  \,   \sim  \,
  \N\left(0,\cos^2\theta  + a^2  \sin^2\theta \right)$  \ y  \ $X_2  \,  \sim \,
  \N\left(0,\sin^2\theta + a^2 \cos^2\theta  \right)$ \ (ver m\'as adelante). En
  la figura, $a = \frac14$ \ y \ $\theta = \frac{\pi}{6}$.}
\label{Fig:MP:Gaussiana}
\end{figure}

La gaussiana tiene un par de propiedades particulares:
%
\begin{teorema}[Stabilidad]
\label{Teo:MP:StabilidadGaussiana}
%
  Sean \ $A_i , i = 1,\ldots,n$ \  matrices de \ $\Rset^{d' \times d}, d' \le d$
  \ de rango lleno, $b_i \in \Rset^{d'}$ \ y \ $X_i \, \sim \, \N(m_i,\Sigma_i)$
  \ independientes, entonces
  %
  \[
  \sum_{i=1}^n \left(  A_i X_i  + b_i \right)  \, \sim \,  \N\left( \sum_{i=1}^n
    \left( m_i + b_i \right) \, , \, \sum_{i=1}^n A_i \Sigma_i A_i^t \right)
  \]
  % 
  En particular, cualquier combinaci\'on lineal  de los componentes de un vector
  Gaussiano da una gaussiana.  Reciprocamente, si cualquier combinaci\'on lineal
  de los componentes de un vector aleatorio sigue una ley gaussiana, entonces el
  vector es gaussiano.
\end{teorema}
%
\begin{proof}
  Este  resultato se proba  usando funci\'on  caracter\'istica de  la gaussiana,
  conjuntalmente al teorema~\ref{Teo:MP:PropiedadesFuncionCaracteristica}.
\end{proof}

%
\begin{teorema}[Independencia]
\label{Teo:MP:IndependenciaGaussiana}
%
  Sea   \   $X  \,   \sim   \,   \N(m,\Delta)$  \   con   \   $\Delta  =   \diag
  \left(  \begin{bmatrix}  \sigma_1^2  &  \cdots  &  \sigma_d^2  \end{bmatrix}^t
  \right)$   \  diagonal.   Entonces  las   componentes  \   $X_i  \,   \sim  \,
  \N(m_i,\sigma_i^2)$ \ son independientes.
\end{teorema}
%
\begin{proof}
  Este resultato se proba  trivialmente escribiendo la densidad de probabilidad,
  notando que se factorisa.
\end{proof}
%
Hemos visto que cuando un  vector tiene componentes independientes, la matriz de
covarianza  es   diagonal  (lema~\ref{Lem:MP:IndependenciaCov}),  pero   que  la
reciproca es falsa en general. El \'ultimo teorema muestra que la reciproca vale
en el caso gausiano.

\SZ{y mas}