\subsubseccion{Distribuci\'on Gamma}
\label{Sssec:MP:Gamma}

Se  denota $X \,  \sim \,  \G(a,b)$ \  con \  $a \in  \Rset_+^*$ \  llamado {\em
parametro de  forma} \ y \  $b \in \Rset_+^*$  \ llamada {\em taza}  (inversa de
{\em escala}).  Las caracter\'isticas son:

\begin{caracteristicas}
%
Dominio de definici\'on & $\X = \Rset_+$\\[2mm]
\hline
%
Parametros & $a \in \Rset_+^*$ \ (forma), \: $b \in \Rset_+^*$ \ (taza)\\[2mm]
\hline
%
Densidad  de probabilidad  &  $\displaystyle p_X(x)  =  \frac{b^a \, x^{a-1} \,  e^{-b
x}}{\Gamma(a)}$\\[2mm]
\hline
%
Promedio & $\displaystyle m_X = \frac{a}{b}$\\[2mm]
\hline
%
Varianza & $\displaystyle \sigma_X^2 = \frac{a}{b^2}$\\[2mm]
\hline
%
Asimetr\'ia & $\displaystyle \gamma_X = \frac2{\sqrt{a}}$\\[2mm]
\hline
%
Curtosis por exceso & $\displaystyle \widebar{\kappa}_X = \frac6{a}$\\[2mm]
\hline
%
Generadora  de momentos  & $\displaystyle  M_X(u) =  \left( 1  - \frac{u}{b}
\right)^{-a}$ \ para \ $\real{u} < b$\\[2mm]
\hline
%
Funci\'on  caracter\'istica  &  $\displaystyle   \Phi_X(\omega)  =  \left(  1  -
\frac{ \imath \omega}{b} \right)^{-a}$
\end{caracteristicas}

% Momentos & $ \Esp\left[ X^k \right] = p^k$\\[2mm]
% Momento factorial & $\Esp\left[ (X)_k \right] = ?$\\[2mm]
% Generadora de probabilidad & $G_X(z) = e^{\lambda (z-1)}$ \ para \ $z \in \Cset$\\[2mm]
% modo max(a-1,0)
% Mediana no close ver inverse gamma

Nota: trivialmente, se puede escribir $X \,  \egald \, \frac{1}{b} G$ \ con \ $G
\, \sim \, \G(a,1)$  \ donde \ $G$ \ es estandardizada  o normalizada. De nuevo,
las  caracter\'isticas  de \  $X$  \  son  v\'inculadas a  las  de  \ $G$  \  (y
vice-versa) por transformaci\'on afine (ver secciones anteriores).

Una densidad de probabilidad gamma  y la funci\'on de repartici\'on asociada son
representadas en  la figura Fig.~\ref{Fig:MP:Gamma} para  varios $a$ \ y  \ $b =
1$.
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/Gamma} \end{center}
%
\leyenda{Ilustraci\'on de una densidad de probabilidad gamma (a), y la funci\'on
de  repartici\'on asociada  (b).   $b  = 1$  \  y \  $a  = 0.5$  (linea
punteada), $1$ (linea mixta), $2$ (linea guionada) y $3$ (linea llena).}
\label{Fig:MP:Gamma}
\end{figure}

Nota  que para  \  $X \,  \sim  \, \G(1,b)$  \ es  una  variable exponencial  de
parametro \  $b$, \ie \ $X \,  \sim \, \E(b)$. Cuando  \ $a < 1$,  la densidad \
$p_X$  \ diverge  para \  $x  \to 0$  \ (divergencia  integrable). Adem\'as,  se
muestra tambi\'en sencillamente con las funciones caracter\'isticas que:
%
\begin{lema}[Stabilidad]
\label{Lem:MP:StabilidadGamma}
%
  Sean $X_i  \, \sim  \, \G\left( a_i  , b  \right), \: i  = 1 ,  \ldots ,  n$ \
  independientes. Entonces
  % 
  \[
  \sum_{i=1}^n X_i \, \sim \, \G\left( \sum_{i=1}^n a_i \, , \, b \right)
  \]
\end{lema}

Adem\'as,  se muestra  sencillamente  por  cambio de  variables  y la  funci\'on
caracter\'istica un v\'inculo con variables gausianas:
%
\begin{lema}[V\'inculo con la gaussiana]
\label{Lem:MP:VinculoGammaGaussiana}
%
  Sean $X_i \, \sim \,  \N\left( 0 , \sigma^2 \right), \: i = 1  , \ldots , n$ \
  independientes. Entonces
  %
  \[
  \sum_{i=1}^n  X_i^2 \,  \sim \,  \G\left( \frac{n}{2}  \, ,  \,  \frac{1}{2 \,
      \sigma^2} \right)
  \]
\end{lema}

La distribuci\'on Gamma aparece entre  otros en problema de inferencia Bayesiana
como distribuci\'on a priori conjugado~\footnote{En la inferencia Bayesiana, nos
  interesamos al paremetro (posiblemente  multivariado) \ $\theta$ \ subyancente
  a una  distribuci\'on. Por ejemplo,  sabemos tener observaciones  sorteados de
  una distribuci\'on de Poisson, pero con el parametro \ $\lambda$ \ desconocido
  y nos interesamos a \ $\theta \equiv \lambda$. El enfoque Bayesiano consiste a
  considerar el paremetro  \ $\Theta$ \ aleatorio, tal  que la distribuci\'on de
  las observaciones  sea vista como distribuci\'on condicional  \ $p_{X|\Theta =
    \theta}(x)$, llamada distribuci\'on de sampleo. Dados las observaciones $X =
  x$,  la  meta  es  de  determinar  la  distribuci\'on  dicha  a  posteriori  \
  $p_{\Theta|X =  x}$ \ a  partir de  la cual se  puede hacer estimaci\'on  de \
  $\theta$ dados  las observaciones, calcular intervalos de  confianza, etc. Por
  eso,  el metodo  se  apoya  sobre la  regla  de Bayes  $p_{\Theta|X=x}(\theta)
  \propto  p_{X|\Theta=\theta}(x)  p_\Theta(\theta)$  \  as\'i que  se  necesita
  elegir  una distribuci\'on  \ $p_\Theta$  \  dicha a  priori.  Una  elecci\'on
  posible es  tomarla en una familia  parametrizada tal que  la distribuci\'on a
  posterior  partenece  tambi\'en a  esta  familia: es  lo  que  se llama  prior
  conjugado. La idea es que si  vienen observaciones, en lugar de re-calcular el
  posterior,   se   puede   actualizar   solamente  los   parametros   (llamados
  hiperparametros).    \SZ{Ver    nota    de    pie    en    el    cap    2    a
    modificar}.\label{Foot:MP:BayesPrior} } del parametro $\lambda$ de la ley de
Poisson~\cite{Rob07}.

\SZ{Esta distribuci\'on aparece...}
% en  el conteo  de conteo  de  une repetici\'on  de una  experiencia de  maneja
% independiente hasta que  occure un evento de probabilidad  $p$; por ejemplo el
% n\'umero de tiro de un dado  equilibriado hasta que occurre un ``6'' sigue una
% ley geometrica de parametro $p = \frac16$.