\subsubseccion{Distribuci\'on matriz-variada de Wishart}
\label{Sssec:MP:Wishart}

Este ejemplo es una  generalizaci\'on matriz-variada de la distribuci\'on gamma.
Se puede ver  una matriz como un vector, guardando por  ejemplo sus columnas una
bajo la  precediente.  Sin embargo, tal  distribuci\'on apareciendo naturalmente
en un contexto de estimaci\'on de  matriz de covarianza (ver m\'as adelante), es
m\'as natural verla matriz-variada.  Tal distribuci\'on fue introducida en $d=2$
dimensiones por R. Fisher en 1915~\cite{Fis15}, y el caso general es debido a J.
Wishart~\cite{Wis28,   Mui82,  BilBre99,   GupNag99,  And03,   Seb04}.   Aparece
tambi\'en naturalmente en problema de inferencia Bayesiana como distribuci\'on a
priori conjugado  de la matriz de  precisi\'on \ $\Sigma^{-1}$ \  (inversa de la
covarianza)  de  la  ley gaussiana  multivariada~\cite[Ec.~(4.4.5)]{Rob07}  (ver
notas de pie~\ref{Foot:MP:BayesPrior} y~\ref{Foot:MP:BayesPriorConjugado}).

Se denota  \ $X \, \sim  \, \W(V,\nu)$ \ donde  el dominio de  definici\'on es \
$\Pos_d^+(\Rset)$,  conjunto   matrices  simetricas  definida   positivas,  $V  \in
\Pos_d^+(\Rset)$ par\'ametro de escala  y \ $\nu > d-1$ \ es  el grado de libertad.
Las caracter\'isticas de la distribuci\'on son las siguientes:

\begin{caracteristicas}
%
  Dominio de definici\'on~\footnote{De hecho,  se puede considerar que la matriz
    aleatoria es equivalent a tener un vector \ $\frac{d (d+1)}{2}$-dimensional;
    por la simetria, claramente \ $X$  \ tiene solamente \ $\frac{d (d+1)}{2}$ \
    componentes diferentes; adem\'as, se puede  probar que cualquier matriz \ $A
    \in \Pos_d^+(\Rset)$ \ se  factoriza bajo la forma \ $A = L  L^t$ \ con \ $L
    \in \TriI_d(\Rset)$  \ triangular inferior con elementos  positivos sobre su
    diagonal, llamado factorizaci\'on  de Cholesky~\cite{Cho10, GupNag99, Bha07,
      Har08, HorJoh13}  y reciprocamente. Eso  muestra que \  $A$ \ se  define a
    partir      de     \      $\frac{d     (d+1)}{2}$      \      ``grado     de
    libertad''.\label{Foot:MP:WishartXtilde}} & $\X =
  \Pos_d^+(\Rset), \: d \in \Nset^*$\\[2mm]
  \hline
%
Par\'ametros & $V \in \Pos_d^+(\Rset)$ (escala) y \ $\nu > d-1$ \ (grado de
libertad)\\[2mm]
\hline
%
Densidad de probabilidad~\footnote{La densidad  de probabilidad corresponde a la
  densidad conjunta  de los \ $\frac{d  (d+1)}{2}$ \ elementos \  $X_{i,j}, \: 1
  \le i  \le j \le  d$~\cite{Wis28, PedRic91, SulTra96, Mui82, BilBre99,  GupNag99, And03,
    Seb04}.\label{Foot:MP:WishartDensidad}}    &    $\displaystyle   p_X(x)    =
\frac{|x|^{\frac{\nu-d-1}{2}}      \,      e^{-\frac12     \Tr\left(      V^{-1}
      x\right)}}{2^{\frac{d \nu}{2}} \, |V|^{\frac{\nu}{2}} \, \Gamma_d\left(
    \frac{\nu}{2} \right)}$\\[2mm]
\hline
%
Promedio & $\displaystyle m_X = \nu \, V$\\[2mm]
\hline
%
Covarianza & $\displaystyle \Sigma_X = \nu \sum_{i,j=1}^d \Big( \left( \un_i
\un_j^t V \right) \!\otimes\! \left( \un_j \un_i^t V \right) + \left( V \un_i
\un_j^t V \right) \!\otimes\! \left( \un_i \un_j^t \right) \Big)$\\[2mm]
%\big( J (V \otimes V) + (V \otimes I)
%K (V \otimes I) \big)$\\[2mm]
% $\Cov[X_{i,j},X_{k,l}] = nu \left( V_{i,k} V_{j,l} +V_{i,l} V_{j,k} \right)$}
\hline
%
Funci\'on caracter\'istica~\footnote{Se prueba que la funci\'on generadora de
momentos no existe en
general.\label{Foot:MP:CaracteristicaWishart}} &
$\displaystyle \Phi_X(\omega) = \left| I - 2 \, \imath \, \omega V
\right|^{-\frac{\nu}{2}}, \quad \omega \in \Sim_d(\Rset)$
\end{caracteristicas}
%
%(ver~\cite{PedRic91, SulTra96, And03}).

Fijense que $p_X$ no es la  distribuci\'on conjunta de los componentes de \ $X$:
el hecho  que \  $X$ \  sea una matriz  aleatoria de  \ $\Pos_d^+(\Rset)$  \ impone
v\'inculos sobre sus compnentes; entre otros, $X_{i,j} = X_{j,i}$.

Inmediatamente, si  $d = 1$, la  distribuci\'on de Whishart \  $\W(V,\nu)$ \ se
reduce  a la  distribuci\'on Gamma  $\G\left(\frac{\nu}{2} \,  , \,  \frac1{2 V}
\right)$. De este  hecho, se la podr\'ia ver  como extensi\'on matriz-variada de
la  distribuci\'on  gamma.  La  distribuci\'on  de  Wishart  tiene varias  otras
propiedades como las siguientes.
%
\begin{lema}[Stabilidad por transformaci\'on lineal]
\label{Lem:MP:StabilidadWishartLineal}
%
Sea $X \, \sim \, \W(V,\nu)$ \ y  \ $A \in \Mat_{d,d'}(\Rset)$ \ con \ $d' \le d$,
\ de rango lleno. Entonces
  \[
  A^t X A \, \sim \, \W\left( A^t V A , \nu \right)
  \]
  %
  En particular, si $d'  = 1$, \ $A^t X A \, \sim  \, G\left( \frac{\nu}{2} \, ,
    \, \frac1{2 \, A^t V A} \right)$. M\'as all\'a, tomando $A = \un_j$, aparece
  de que  las componentes diagonales de \  $X$ \ son de  distribuci\'on gamma, \
  $X_{j,j}  \, \sim  \,  \G\left( \frac{\nu}{2}  \,  , \,  \frac1{2 \,  V_{j,j}}
  \right)$.
\end{lema}
%
\begin{proof}
  El     resultado     es     inmediato     saliendo     de     la     funci\'on
  caracter\'istica~\footref{Foot:MP:CaracteristicaWishart} y notando de que
%
\begin{eqnarray*}
\Phi_{A^t X A}(\omega) & = & \Esp\left[ e^{\imath \Tr\left( \omega^t A^t X A
\right)}\right]\\[2mm]
%
& = & \Phi_X\left( A \omega^t A^t\right)\\[2mm]
%
& = &  \left| I - 2 \imath A \omega A^t V \right|^{-\frac{\nu}{2}}\\[2mm]
%
& = &  \left| I - 2 \imath \omega A^t V A \right|^{-\frac{\nu}{2}}
%
\end{eqnarray*}
%
de   \   $\Tr(AB)   =   \Tr(BA)$~\cite{Har08}   \   y   de   la   identidad   de
Sylvester~\cite{Syl51,  AkrAkr96}  o~\cite[\S~18.1]{Har08} \  $\left|  I  + A  B
\right| = \left| I + B A \right|$.  .
\end{proof}
%
De hecho, si los elementos diagonales son de distribuci\'on gamma, no es el caso
de         los        elementos         no-diagonales~\cite{Seb04,        And03}
o~\cite[Teo.~3.3.4]{GupNag99}.    De    eso   resuelte   delicado    llamar   la
distribuci\'on como gamma matriz-variada.

\begin{lema}[Stabilidad por suma]
\label{Lem:MP:StabilidadWishartSuma}
%
  Sea $X_i \,  \sim \, \W(V,\nu_i), \: i = 1,\ldots,n$ independientes. Entonces
  \[
  \sum_{i=1}^n X_i \, \sim \, \W\left( V \, , \, \sum_{i=1}^n \nu_i \right)
  \]
\end{lema}
%
\begin{proof}
  El     resultado     es     inmediato     saliendo     de     la     funci\'on
  caract\'eristica~\footref{Foot:MP:CaracteristicaWishart} y notando que como en
  el context vectorial $\Phi_{\sum_i X_i} = \prod_i \Phi_{X_i}$.
\end{proof}

La distribuci\'on  de Wishart aparece naturalmente en  problemas de estimaci\'on
de matriz de covarianza en el contexto gaussiano~\cite{Mui82, BilBre99, GupNag99,
  And03, Seb04, KotNad04}:
%
\begin{lema}[V\'inculo con vectores gaussianos]\label{Lem:MP:WishartGaussiana}
%
  Sean \ $X_i \, \sim \, \N(0,V), \: i = 1, \ldots , n > d-1$ \ independientes y
  la  matriz  \  $S  =  \sum_{i=1}^n   X_i  X_i^t$  \  llamada  {\em  matriz  de
    dispersi\'on} (scatter matrix en ingl\'es). Entonces, \ $S \in \Pos_d^+(\Rset)$
  (c.   s.)   \  ($S$ es  sim\'etrica  definida  positiva  casi siempre,  o  con
  probabilidad uno) y \
  %
  \[
  S = \sum_{i=1}^n X_i X_i^t \,\sim \, \W(V,n).
  \]
  %
  Eso de re-escribe de manera compacta bajo la forma
  %
  \[
  X X^t  \,\sim \, \W(V,n)  \qquad \mbox{con} \qquad  X = \begin{bmatrix}  X_1 &
    \cdots & X_n \end{bmatrix}
  \]
\end{lema}
%
\begin{proof}
  De  $\displaystyle  \Tr\left(  \omega^t  S \right)  =  \sum_{i=1}^n  \Tr\left(
    \omega^t  X_i  X_i^t \right)  =  \sum_{i=1}^n  X_i^t  \omega X_i^t$  (siendo
  $\omega$ sim\'etrica), y de la independencia, tenemos inmediatamente
%
\[
\Phi_S(\omega)  =  \prod_{i=1}^n \Esp\left[  e^{\imath  \,  X_i^t \omega  X_i^t}
\right] = \left( \Esp\left[ e^{\imath \, X_i^t \omega X_i^t} \right] \right)^n
\]
%
Luego,
%
\begin{eqnarray*}
\Esp\left[ e^{\imath \, X_i^t \omega X_i^t} \right] & = & \int_{\Rset^d} (2
\pi)^{-\frac{d}{2}} |V|^{-\frac12} e^{\imath \, x^t \omega x^t - \frac12 x^t
V^{-1} x} \, dx\\[2mm]
%
& = & |V|^{-\frac12} \left| V^{-1} - 2 \imath \, \omega \right|^{-\frac12}
\int_{\Rset^d} (2 \pi)^{-\frac{d}{2}} \left| \left(V^{-1} - 2 \imath \,
\omega\right)^{-1} \right|^{-\frac12} e^{- \frac12 x^t \left(V^{-1} - 2 \imath
\, \omega \right) x} \, dx
\end{eqnarray*}
%
Se puede probar  que la integral vale uno (moralmente,  integral de una densidad
de  probabilidad gaussiana)~\cite[Teo.~2.1.11]{Mui82}.  La  prueba se  cierra por
$\left| V^{-1} - 2 \imath  \, \omega \right|^{-\frac12} = |V|^{\frac12} \left| I
  -  2  \imath \,  \omega  V \right|^{-\frac12}$:  se  reconoce  en $\Phi_S$  la
funci\'on caracter\'istica de la ley de wishart (a condici\'on que $n > d-1$).
\end{proof}
%
Se    puede   referirse   tambi\'en    a~\cite{GupNag99,   And03,    Seb04},   o
a~\cite[Ej.~7.2]{BilBre99} para pruebas alternativas.

Este resultado  permite tambi\'en probar  el lema~\ref{Lem:MP:StabilidadWishart}
para \  $\nu = n$  \ entero  escribiendo \ $X  \egald \sum_{i=1}^n X_i  X_i^t$ \
as\'i que \ $A^t X A \egald  \sum_{i=1}^n A^t X_i X_i^t A = \frac1n \sum_{i=1}^n
\left( A^t X_i \right) \left( A^t X_i  \right)^t$ \ y notando que los \ $A^t X_i
\, \sim \, \N(0, A^t V A)$ \ son independientes~\cite{Seb04}.  Adem\'as, permite
re-obtener las  expreciones del promedio y de  las covarianzas~\footnote{Para la
  covarianza, su usa la formula \ $\Esp[Y_1 Y_2 Y_3 Y_4] = \Esp[Y_1 Y_2]\Esp[Y_3
  Y_4]  + \Esp[Y_1 Y_3]\Esp[Y_2  Y_4] +  \Esp[Y_1 Y_4]\Esp[Y_2  Y_3]$ \  para $Y
  = \begin{bmatrix}  Y_1 & Y_2 &  Y_3 & Y_4 \end{bmatrix}^t$  \ vector gaussiano,
  formula que se  obtiene por ejemplo a partir  de la funci\'on caracter\'istica
  de un vecor  gaussiano.}. Notar que cuando los \ $X_i$  \ tienen un promemedio,
el  lema conduce  a  lo que  es  conocido como  Wishart no  central~\cite{And03,
  Seb04}.

Este  lema  se  extiende  a  trav\'es  de  lo  que  aparece  como  combinaciones
ortonormales~~\cite{Mui82, GupNag99, BilBre99, And03, Seb04, KotNad04}:
%
\begin{lema}[V\'inculo con vectores gaussianos y matriz idempotenta]\label{Lem:MP:WishartGaussianaIdempotenta}
%
  Sean \ $X_i \, \sim \, \N(0,V), \: i = 1, \ldots , n > d-1$ \ independientes y
  denotamos \ $X = \begin{bmatrix} X_1 & \cdots & X_n \end{bmatrix}$ \ matriz de
  \ $\Mat_{d,n}(\Rset)$. Sea  \ $A \in \Sim_n(\Rset)$ \  idempotenta, es decir \
  $A^2 = A$ \ de rango $q > d-1$. Entonces
  %
  \[
  X A X^t \,\sim \, \W(V,q)
  \]
\end{lema}
%
\begin{proof}
  $A$ \  siendo idempotenta, existe una  matriz \ $B \in  \Mat_{n,q}(\Rset)$ \ tal
  que~\footnote{$A$ siendo idempotenta, si $x$  es un autovector con autovalor \
    $\lambda$ \ tenemos  \ $\lambda x = A x  = A^2 x = \lambda^2  x$, es decir \
    $\lambda (\lambda-1) = 0$: las autovalores  son $0$ o $1$.  Del rango $q$, y
    $A \in \Sim_n(\Rset)$  se diagonaliza bajo la forma \ $A  = \begin{bmatrix} B &
      \widetilde{B}   \end{bmatrix}    \begin{bmatrix}   I   &   0    \\   0   &
      0 \end{bmatrix} \begin{bmatrix} B^t\\ \widetilde{B}^t \end{bmatrix}$ \ con
    $B    \in     \Mat_{n,q}(\Rset)$    \    y    \     $\begin{bmatrix}    B    &
      \widetilde{B}             \end{bmatrix}$            de            columnas
    ortonormales~\cite[Teo.~21.5.7]{Har08} o~\cite{HorJoh13}.}  \  $A = B B^t$ \
  y \ $B^t B = I$~\cite{Har08}. Entonces,
  %
  \[
  X A  X^t =  Y Y^t \qquad  \mbox{con} \qquad Y  = X  B = \begin{bmatrix}  Y_1 &
    \cdots & Y_q \end{bmatrix}
  \]
  %
  Ahora,    del   teorema~\ref{Teo:MP:StabilidadGaussiana}    los    $Y_i$   son
  gaussianos de media nula. Adem\'as
  %
  \begin{eqnarray*}
  \Esp\left[ Y_i Y_j^t \right] & = & \sum_{k,l=1}^n B_{ki} B_{lj} \Esp\left[ X_k
  X_l^t \right]\\[2mm]
  %
  & = & \left( \sum_{k=1}^n B_{ki} B_{kj} \right) V
  \end{eqnarray*}
  %
  de la independencia de los $X_k$ que son de covarianza $V$. De $B^t B = I$ \ie
  \ $\sum_{k=1}^n  B_{ki} B_{kj} = \un_{\{i\}}(j)$  \ tenemos que los  \ $Y_i$ \
  son  independientes  (gaussianos  de  covarianza  nula  para  $i  \ne  j$),  de
  covarianza $V$.  La prueba se cierra del lema~\ref{Lem:MP:WishartGaussiana}.
\end{proof}
%
En particular,  la distribuci\'on, de Wishart  aparece en la  estimaci\'on de la
matriz de covarianza de un vector  gaussiano a partir de copias independientes de
vectores gaussianos independientes de mismos parametros~\cite{KotNad04, BilBre99,
  And03, Seb04, GupNag99}:
% Resp. ?? -- cap. 7 prop 7.1 -- corr 7.2.2, 7.2.3 -- ?? -- Cap. 9
%
\begin{corolario}\label{Cor:MP:WishartEstimacion}
  Sean  \  $X_i \,  \sim  \,  \N(m,\Sigma), \:  i  =  1, \ldots  ,  n  > d-1$  \
  independientes,      y       sea      la      media       emp\'irica      (ver
  corolario~\ref{Cor:MP:MediaEmpiricaGauss})
  %
  \[
  \overline{X} = \frac{1}{n} \sum_{i=1}^n X_i
  \]
  %
  Sean las  matrices aleatorias~\footnote{La primera matriz  aparece por ejemplo
    cuando querememos estimar  $\Sigma$, conociendo $m$, y la  secinda cuando no
    se conoce $m$. En cada caso, la renormalizaci\'on asegurida que la media del
    estimador es  precisamente $\Sigma$,  es decir que  los estimadores  son sin
    sesgo.}
  %
  \[
  \overline{\Sigma}_m =  \frac{1}{n} \sum_{i=1}^n (X_i  - m) (X_i -  m)^t \qquad
  \mbox{y} \qquad  \overline{\Sigma} =  \frac{1}{n-1} \sum_{i=1}^n \left(  X_i -
    \overline{X} \right) \left( X_i - \overline{X} \right)^t
  \]
  %
  Entonces
  %
  \[
  \overline{\Sigma}_m  \, \sim  \,  \W\left( \frac{1}{n}  \,  \Sigma \,  , \,  n
  \right)  \qquad   \mbox{y}  \qquad  \overline{\Sigma}  \,   \sim  \,  \W\left(
    \frac{1}{n-1} \, \Sigma \, , \, n-1 \right)
  \]
  %
\end{corolario}
%
\begin{proof}
  Denotamos  \  $\widetilde{X}_i   =  X_i  -  m  \,   \sim  \,  \N(0,\Sigma)$  \
  independientes, \ $\widetilde{X} =  \begin{bmatrix} \widetilde{X}_1 & \cdots &
    \widetilde{X}_n \end{bmatrix}$  \ matriz de  \ $\Mat_{d,n}(\Rset)$ \ y  \ $\un
  \in  \Rset^n$ \  vector de  componentes  iguales a  $1$.  Entonces,  denotando
  $\overline{\widetilde{X}}   =  \frac{1}{n}   \sum_{i=1}^n   \widetilde{X}_i  =
  \frac{1}{n}  \sum_{i=1}^n (X_i  - m)  = \frac{1}{n}  \sum_{i=1}^n X_i  -  m$ \
  tenemos \ $X_i -  \overline{X} = \widetilde{X}_i - \overline{\widetilde{X}}$ y
  se muestra sencillamente que
  %
  \[
  \overline{\Sigma}_m   =  \frac{1}{n}   \widetilde{X}   \widetilde{X}^t  \qquad
  \mbox{y}  \qquad \overline{\Sigma}  = \frac{1}{n-1}  \widetilde{X} \left(  I -
    \frac{\un \un^t}{n} \right) \widetilde{X}^t
  \]
  %
  Adem\'as,  se  proba  sencillamente  que   \  $I  -  \frac{\un  \un^t}{n}  \in
  \Sim_n(\Rset)$ es idempotenta de rango $n-1$. El resultado es consecuencia de los
  lemas~\ref{Lem:MP:StabilidadWishartLineal}   (por    la   normalizaci\'on)   y
  de~\ref{Lem:MP:WishartGaussiana}       y~\ref{Lem:MP:WishartGaussianaIdempotenta}
  respectivamente.
\end{proof}

Las distribuciones de Wishart tienen  varias propiedades m\'as que se encuentren
por ejemplo  en los  libros especialisados~\cite{Mui82, GupNag99,  And03, Seb04,
  NadKot04}. Entre estas, terminaremos  esta secci\'on con la factorizaci\'on de
una  matriz de  distribuci\'on de  Wishart  de la  manera siguiente:  sea \  $X$
definido sobre  $\Pos_d^+(\Rset)$; entonces, \ $\forall \:  \omega, \quad X(\omega)
\in \Pos_d^+(\Rset)$ \ puede escribirse por factorizaci\'on de Cholesky, es decir \
$X(\omega)  = T(\omega)  T(\omega)^t$  \ con  \  $T$ \  triangular inferior  con
elementos     positivos      sobre     su     diagonal      (ver     nota     de
pie~\footref{Foot:MP:WishartXtilde}).  En  el contexto de  Wishart (de parametro
$(I,\nu)$),  aparece  que  se   puede  caracterizar  la  distribuci\'on  de  los
coefficientes.     Eso    es   conocido    como    {\em   descomposici\'on    de
  Bartlett}~\cite{Bar34,  Mui82,  BilBre99,  GupNag99,  And03,  NadKot04}  y  se
formaliza en el caso Wishart general de la manera siguiente:
%
\begin{teorema}[Descomposici\'on de Bartlett]\label{Teo:MP:Bartlett}
%
  Sea \ $X \,  \sim \, \W(V,\nu)$ \ y \ $V =  L L^t$ factorizaci\'on de Cholesky
  de  \ $V$  \ con  \ $L  \in \TriI_d(\Rset)$  \ triangular  inferior con elementos positivos en su diagonal. Entonces,
  tenemos
  %
  \[
  X \, \egald \, L U U^t L^t
  \]
  %
  con \ $U$ de componentes independientes, $U_{ii} > 0$, tales que
  %
  \[
  \left\{\begin{array}{lll}
  %
  U_{ij} = 0 & \mbox{si} & j > i \:\: \mbox{($T$ \ triangular inferior)}\\[2mm]
  %
  U_{ij} \, \sim \, \N(0,1) & \mbox{si} & j < i\\[2mm]
  %
  U_{ii}^2 \, \sim \, \G\left( \frac{\nu-i+1}{2} \, , \, \frac12 \right) & &
  %
  \end{array}\right.
  \]
\end{teorema}
%
\begin{proof}
  Primero, se nota del lema~\ref{Lem:MP:StabilidadWishartLineal} que \ $X \egald
  L  Y  L^t$  \  con  \  $Y  \, \sim  \,  \W(I,\nu)$,  de  densidad  $p_Y(y)  =
  \frac{|y|^{\frac{\nu-d-1}{2}}    e^{-\frac12    \Tr(y)}}{2^{\frac{d   \nu}{2}}
    \Gamma_d\left(  \frac{\nu}{2} \right)}$.  Ahora, sea  la  factorizaci\'on de
  Cholesky de  \ $Y =  U U^t$ (\ie  para cada $\omega$ se  factoriza $Y(\omega)$
  dando  $U(\omega)$)   y  la  transformaci\'ion   $g:  Y  \mapsto   U$.   M\'as
  precisamente,  la  densidad  $p_X$   siendo  la  de  los  $\frac{d  (d+1)}{2}$
  componentes diferentes  de $X$ (parte  triangular inferior), $g: (X)_{1  \le j
    \le i \le d} \mapsto (U)_{1 \le  j \le i \le d}$.
  %
  % \SZ{Se calcula entonces la Jacobiana de la transformaci\'on inversa ZZZ}
  %
  Seg\'un el  teorema~2.1.9 de~\cite{Mui82}, el valor  absoluto del determinente
  de la jacobiana de \ $g^{-1}$ \ es dado por
  \[
  \left| \Jac_{g^{-1}}(u) \right| = 2^d \prod_{i=1}^d u_{ii}^{d-i+1}
  \]
  %
  (ver tambi\'en la prueba del teorema 7.2.1 de~\cite{And03} o~\cite[Prop.~2.22]{BilBre99}).
  
  Se nota ahora que \ siendo \ $u \in \TriI_d(\Rset)$,
  %
  \[
  \Tr(x) =  \sum_{i=1}^d x_{ii}  = \sum_{1 \le j \le i \le d} u_{ij}^2, \qquad
  |x| = |u|^2 = \prod_{i=1}^d u_{ii}^2.
  \]
  %
  Del  teorema~\ref{Teo:MP:TransformacionInyectivaDensidad}   obtenemos  para  \
  $u_{ii}  > 0$ \  (ver notaciones  para la  escritura de  $\Gamma_d$ funci\'on
  gamma multivariada, como producto de funciones Gamma)
  %
  \begin{eqnarray*}
  p_U(u) & = &\left| \Jac_{g^{-1}}(u) \right| \,  p_X(u u^t) \\[2mm]
  %
  & = & 2^d \prod_{i=1}^d u_{ii}^{d-i+1} \: \frac{\displaystyle \prod_{i=1}^d
  u_{ii}^{\nu-d-1} \, e^{ -\frac12 \sum_{i=1}^d \sum_{j=1}^i  u_{ij}^2}}{\displaystyle
  2^{\frac{d \nu}{2}} \pi^{\frac{d (d-1)}{4}} \prod_{i=1}^d \Gamma\left(
  \frac{\nu-i+1}{2} \right)}\\[2mm]
  %
  & = & \left( \prod_{i=1}^d \: \frac{2^{1 - \frac{\nu-i+1}{2}} \,
  u_{ii}^{\nu-i} \, e^{-\frac12 t_{ii}^2}}{\Gamma\left( \frac{\nu-i+1}{2} \right)}
  \right) \left( \prod_{1 \le j < i \le d} \frac{\displaystyle
  e^{-\frac{t_{ij}^2}{2}}}{\sqrt{2 \pi}} \right)
  \end{eqnarray*}
  %
  Por productos, se concluye que  las conmponentes son independientes. Luego, en
  el secundo  producto se  reconoce un producto  de leyes  gaussianas estandares,
  as\'i que los  $U_{ij}, \: 1 \le j  < i \le d$ \ son  gaussianas estandares. Al
  final,   la   densidad  de   \   $U_{ii}$   \   siendo  \   $p_{U_{ii}}(u)   =
  \frac{2^{1-\frac{\nu-i+1}{2}}  \, u^{\nu-i} \,  e^{-\frac12 u^2}}{\Gamma\left(
      \frac{\nu-i+1}{2}      \right)}$,      por      transformac\'ion      (ver
  corolario~\ref{Cor:MP:TransformacionInyectivaDensidadEscalar}), la densidad de
  \  $U_{ii}^2$  \  es  \  $p_{U_{ii}^2}(v)  =  \frac{2^{-\frac{\nu-i+1}{2}}  \,
    v^{\frac{\nu-i+1}{2}-1} \, \, e^{-\frac12 v}}{\Gamma\left( \frac{\nu-i+1}{2}
    \right)}$: es  la ley  Gamma \ $\G\left(  \frac{\nu-i+1}{2} \, ,  \, \frac12
  \right)$ \ (ver subsecci\'on~\ref{Sssec:MP:Gamma}).
\end{proof}
%
Nota: la distribuci\'on de \ $U_{ii}$ \ es a veces llamada {\em raiz-gamma}.

Esta   descomposici\'on   permite   de   sortear   sencillamente   matrices   de
distribuciones   de   Wishart,  con   grado   de   libertad  no   necesariamente
entero~\footnote{De hecho, en la  literatura, esta descomposici\'on aparece casi
  siempre con \ $\nu$ \ entero, pero la prueba no necesite este v\'inculo.}.
