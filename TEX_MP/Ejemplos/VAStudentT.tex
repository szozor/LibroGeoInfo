\subsubseccion{Distribuci\'on Student-$t$ multivariada}
\label{Sssec:MP:StudentT}

En   el  caso   escalar,  esta   ley   fue  introducida   inicialmente  por   F.
R. Helmert~\cite{Hel75, Hel76, She95}  y J.  L\"uroth~\cite{Lur76, Pfa96}.  Pero
es m\'as  conocida por su  introducci\'on por William  Sealy Gosset~\footnote{De
  hecho, W.  S.  Gosset fue un estudiante trabajando en  la f\'abrica de cerveza
  irlandesa  Guiness sobre  estad\'isticas  relacionadas a  la  qu\'imica de  la
  cerveza.  Hay  varias versiones sobre el  hecho que se  public\'o este trabajo
  bajo  el nombre  ``Student''.  Una  es que  fue  para que  no se  sabe que  la
  f\'abrica  estaba  trabajando  sobre  estas estad\'isticas  para  estudiar  la
  calidad   de   la   cerveza~\cite{Wen16}.\label{Foot:MP:Student}}   en   1908,
trabajando  sobre variables centradas  normalizadas por  el promedio  y varianza
emp\'iricos~\cite{Stu08}.   Fue  estudiada  entre  otros intensivamente  por  el
famoso  matem\'atico R.   Fisher~\cite{Fis25}.  En  la literatura,  esta  ley es
conocida bajo  los nombres {\em  Student}, {\em Student-$t$} o  simplemente {\em
  $t$-distribuci\'on} o  a\'un bajo el nombre  {\em Pearson tipo IV}  en el caso
escalar  y {\em  Pearson tipo  VII}  (para $\frac{\nu+d}{2}$  entero; ver  m\'as
abajo), debido  a la  familia de Pearson~\cite{Pea95,  JohKot95:v1, JohKot95:v1,
  KotBal00, FanKot90}.   Esta distribuci\'on aparece como a  priori conjugado de
la media de una gaussiana en inferencia bayesiana~\cite{Rob07, KotNad04}.

Se denota con \ $X \sim \T_\nu(m,\Sigma)$ \ con \ $m \in \Rset^d$, \ $\Sigma \in
\Pos_d^+(\Rset)$ \ conjunto  de las matrices de \  $\Mat_{d,d}(\Rset)$ \ sim\'etricas
definidas positivas. $m$ \ es llamado  {\em par\'ametro de posici\'on} (no es la
media   que  puede  no   existir),  \   $\Sigma$  \   es  llamada   {\em  matriz
  caracter\'istica} (no es [proporcional a]  la covarianza que puede no existir)
y \ $\nu > 0$ \ llamado  {\em grado de libertad}.  Las caracter\'isticas de una
Student-$t$ son las siguientes:
%
\begin{caracteristicas}
%
Dominio de definici\'on & $\X = \Rset^d$\\[2mm]
\hline
%
Par\'ametro & $\nu \in \Rset_{0,+}$ \ (grado de libertad), \ $m \in \Rset^d$ \
(posici\'on), \ $\Sigma \in \Pos_d^+(\Rset)$ \ (matriz caracter\'istica)\\[2mm]
\hline
%
Densidad de probabilidad & $\displaystyle p_X(x) = \frac{\Gamma\left(
\frac{\nu+d}{2} \right)}{\pi^{\frac{d}{2}} \nu^{\frac{d}{2}} \Gamma\left(
\frac{\nu}{2} \right) \, \left| \Sigma \right|^{\frac12}} \, \left( 1 +
\frac{(x-m)^t \Sigma^{-1} (x-m)}{\nu} \right)^{- \, \frac{\nu+d}{2}}$\\[2mm]
\hline
%
Promedio & $\displaystyle m_X = m$ \ si \ $\nu > 1$; \ no
existe si no~\footnote{De manera general, esta ley admite momentos de orden \ $k$ \
si y solamente si \ $\nu > k$.\label{Foot:MP:ExistenciaMomentosStudent}}.\\[2.5mm]
\hline
%
Covarianza~\footnote{Fijense de que $\Sigma$ no es la covarianza, pero es
proporcional a la covarianza\ldots cuando existe. Se podr\'ia imaginar
renormalizar la ley tal que \ $\Sigma_X$ \ y \ $\Sigma$ \ coinciden, pero no
ser\'ia posible en el caso \ $\nu \le 2$.} & $\displaystyle \Sigma_X =
\frac{\nu}{\nu-2} \, \Sigma$ \ si \ $\nu > 2$; \ no existe si
no~\footref{Foot:MP:ExistenciaMomentosStudent}.\\[2.5mm]
\hline
%
\modif{Asimetr\'ia} & $\displaystyle \gamma_X = 0$ \ si \ $\nu > 3$; \ no existe
si no~\footref{Foot:MP:ExistenciaMomentosStudent}.\\[2mm]
\hline
%
Curtosis por exceso & \modif{$\displaystyle \widebar{\kappa}_X = \frac{2}{\nu-4}
\Big( I \otimes I + \tau_{(2,3)}\big[ I \otimes I \big] + \tau_{(2,4)}\big[ I
\otimes I \big] \Big)$
%\sum_{i,j=1}^d \Big( \! \left( % \un_i \un_i^t \right) \otimes \left( \un_j
%\un_j^t \right) + \left( \un_i % \un_j^t \right) \otimes \left( \un_i \un_j^t
%\right) + \left( \un_i \un_j^t % \right) \otimes \left( \un_j \un_i^t \right) \!
%\Big)$
}\newline si \ $\nu >
4$; \ no existe si no~\footref{Foot:MP:ExistenciaMomentosStudent}.\\[2mm]
\hline
%
Funci\'on caracter\'istica~\footnote{Se  muestra sencillamente que  la funci\'on
  generatriz de  momentos puede existir si y  solamente si \ $\real{u}  = 0$. La
  funci\'on generadora de momentos restricta  al producto cartesiano de rectas \
  $\real{u} =  0$ \ es nada  m\'as que la  funci\'on caracter\'istica. Adem\'as,
  esta  funci\'on  fue  calculdada,   especialmente  en  el  caso  multivariado,
  relativamente   recientemente~\cite{Sut86,  Hur95,  KibJoa06,   SonPar14}.}  &
$\displaystyle  \Phi_X(\omega)  = \frac{\nu^{\frac{\nu}{4}}}{2^{\frac{\nu}{2}-1}
  \Gamma\left(  \frac{\nu}{2}  \right)}  \,  e^{\imath  \omega^t  m}  \,  \left(
  \omega^t   \Sigma   \omega   \right)^{\frac{\nu}{4}}   K_{\frac{\nu}{2}}\left(
  \sqrt{\nu \, \omega^t \Sigma \omega} \right)$
\end{caracteristicas}

Nota: nuevamente se puede escribir $X \, \egald \, \Sigma^{\frac12} S + m$ \ con
\ $S \, \sim \, \T_\nu(0,I)$ \  donde \ $S$ \ es dicha {\em Student-$t$ estandar}
y  las caracter\'isticas  de \  $X$  \ son  v\'inculadas a  las  de \  $S$ \  (y
vice-versa) por transformaci\'on lineal (ver secciones anteriores).

Densidades  de probabilidad  Student-$t$ estandar  y funciones  de repartici\'on
asociadas   en    el   caso   escalar    son   representadas   en    la   figura
Fig.~\ref{Fig:MP:StudentT}-(a) y  (b) para  varios $\nu$, y  una densidad  en un
contexto bi-dimensional figura Fig.~\ref{Fig:MP:StudentT}(c).
%
\begin{figure}[h!]
\begin{center} \input{TIKZ_MP/StudentT} \end{center}
% 
\leyenda{Ilustraci\'on  de  una  densidad  de probabilidad  Student-$t$  escalar
  estandar (a),  y la funci\'on  de repartici\'on asociada  (b) con \ $\nu  = 1$
  (linea llena), \ $\nu = 3$ (linea  guionada), \ $\nu = 7$ (linea punteada) \ y
  \ $\nu \to +\infty$ (linea llena fina; ver m\'as adelante) grado de libertad,
  as\'i que una densidad de probabilidad Student-$t$ bi-dimensional con \ $\nu =
  1$ \  grado de libertad,  centrada, y de  matriz caracter\'istica \  $\Sigma =
  R(\theta) \Delta^2  R(\theta)^t$ \ con \  $R(\theta) = \protect\begin{bmatrix}
    \cos\theta    &     -    \sin\theta\\[2mm]    \sin\theta     &    \cos\theta
    \protect\end{bmatrix}$   \   matriz   de    rotaci\'on   y   \   $\Delta   =
  \Diag\left(\protect\begin{bmatrix}  1   &  a\protect\end{bmatrix}^t  \right)$  \
  matriz  de   cambio  de  escala,   y  sus  marginales   \  $X_1  \,   \sim  \,
  \T_\nu\left(0,\cos^2\theta +  a^2 \sin^2\theta \right)$ \  y \ $X_2  \, \sim \,
  \T_\nu\left(0,\sin^2\theta   +   a^2  \cos^2\theta   \right)$   \  (ver   m\'as
  adelante). En la figura, $a = \frac13$ \ y \ $\theta = \frac{\pi}{6}$.}
\label{Fig:MP:StudentT}
\end{figure}

Nota: el caso  \ $\nu = 1$ \  es conocido como distribuci\'on de  {\em Cauchy} o
{\em  Cauchy-Lorentz} o  {\em Lorentzian}  o  {\em Breit-Wigner}~\cite{Cau53:07,
  Cau53,  Bie53, Bie53:07,  BreWig36, Sti74,  SamTaq94, Lorentz}.   Es  un caso
particular  tambi\'en de  distribuci\'on  $\alpha$-estables~\cite{SamTaq94}.  En
particular, una combinaci\'on lineal de variables de Cauchy independientes queda
de Cauchy. Pero, no  viola el teorema del l\'imite central del  hecho de que una
variable de Cauchy no admite covarianza.

Contrariamente al caso gaussiano, de la forma de la densidad de probabilidad, es
claro que si la matriz \ $\Sigma$ \ es diagonal, la densidad no factoriza, as\'i
que  las componentes  del vector  no son  independientes.  Este  ejemplo muestra
claramente que  la reciproca del lema~\ref{Lem:MP:IndependenciaCov}  es falsa en
general.

Sin embargo, las distribuciones Student-$t$ tienen varias propiedades notables.

\begin{lema}[Stabilidad por transformaci\'on lineal]
\label{Lem:MP:StabilidadLinealStudentT}
%
  Sea \ $X \, \sim \,  \T_\nu(m,\Sigma)$, \ $A$ \ matriz de \ $\Mat_{d',d}(\Rset)$
  \ con \ $d' \le d$, y de rango lleno y \ $b \in \Rset^{d'}$. Entonces
  %
  \[
  A X + b\, \sim \, \T_\nu( A m + b , A \Sigma A^t)
  \]
  %
  En particular los componentes de \ $X$ \ son student-$t$,
  %
  \[
  X_i \, \sim \, \T_\nu(m_i , \Sigma_{i,i} )
  \]
\end{lema}
\begin{proof}
  La prueba es inmediata usando  la funci\'on caracter\'istica y sus propiedades
  por  transformaci\'on lineal.  La condici\'on  sobre \  $A$ \  es  necesaria y
  suficiente para que \ $A \Sigma A^t \in \Pos_{d'}^+(\Rset)$.
\end{proof}

\begin{lema}[V\'inculo con las distribuciones gamma y gaussiana (mezcla de escala gaussiana)]
\label{Lem:MP:MezclaGaussianaEscalaStudentT}
%
  Sea \ $V \,  \sim \, \G\left( \frac{\nu}{2} , \frac{\nu}{2} \right)$  \ y \ $G
  \,  \sim  \,  \N(0,I)$  \ con  $\nu  >  0$  \  y  \ $G$  \  $d$-dimensional  e
  independiente  de \ $V$.  Entonces, para  $\Sigma \in  \Pos_d^+(\Rset)$ y  $m \in
  \Rset^d$,
  %
  \[
  \frac{\Sigma^{\frac12} G}{\sqrt{V}} + m  \, \sim \, \T_\nu(m,\Sigma)
  \]
  %
\end{lema}
\begin{proof}
  Sea  \  $X   =  \frac{G}{\sqrt{V}}$.   De  la  nota   siguiendo  la  tabla  de
  caracter\'isticas es  necesario y suficiente probar que  $X \sim \T_\nu(0,I)$.
  Lo  m\'as  simple  es  de  salir  de la  formula  de  probabilidad  total  del
  teorema~\ref{Teo:MP:ProbaTotalContinuo},  notando  que  de  la  independencia,
  condicionalmente  a  \  $V=v$  \   la  variable  es  gaussiana  de  covarianza
  $\frac{1}{v} I$,
  %
  \[
  p_{G|V=v}(x)  = (2  \pi)^{-\frac{d}{2}}  v^{\frac{d}{2}} e^{-  \frac{x^t x v}{2}}
  \]
  %
  Entonces, multiplicando \ $p_{G|V=v}$ \ por \ $p_V$ \ y por marginalizaci\'on,
  obtenemos
  %
  \begin{eqnarray*}
  p_X(x) & = & \frac{\nu^{\frac{\nu}{2}}}{2^{\frac{\nu+d}{2}} \pi^{\frac{d}{2}}
  \Gamma\left( \frac{\nu}{2} \right)} \, \int_{\Rset_+} v^{\frac{\nu+d}{2}-1} \,
  e^{- \frac{x^t x + \nu}{2} \, v} \, dv\\[2mm]
  %
  & = & \frac{\nu^{\frac{\nu}{2}} \left( \nu + x^t x \right)^{-
  \frac{\nu+d}{2}}}{\pi^{\frac{d}{2}} \Gamma\left( \frac{\nu}{2} \right)} \,
  \int_{\Rset_+} u^{\frac{\nu+d}{2}-1} \, e^{- u} \, du\\[2mm]
  %
  & = & \frac{\Gamma\left( \frac{\nu+d}{2} \right)}{(\pi \nu)^{\frac{d}{2}}
  \Gamma\left( \frac{\nu}{2} \right)} \, \left( 1 + \frac{x^t x}{\nu} \right)^{-
  \frac{\nu+d}{2}}
  \end{eqnarray*}
 %
  La secunda linea viene del cambio de variables \ $u = \frac{x^t x + \nu}{2} \,
  v$  \  y la  tercera  reconociendo  en la  integral  la  funci\'on gamma  (ver
  notaciones).
\end{proof}
%
Nota: este  lema permite tambi\'en  probar el lema~\ref{Lem:MP:StabilidadLineal}
escribiendo \ $A X + b \egald  \sqrt{\frac{\nu}{V}} A \Sigma^{\frac12} G + A m +
b$.

\begin{lema}[L\'imite gaussiana]
\label{Lem:MP:LimiteStudentTGaussiana}
%
  Sea \ $X_\nu \, \sim \, \T_\nu(m,\Sigma)$ \ vector Student-$t$ parametizado por
  \ $\nu$ \ su grado de libertad. Entonces
  %
  \[
  X_\nu \, \limitd{\nu \to \infty} \, = \, X \, \sim \, \N(m,\Sigma)
  \]
  %
  con \ $\displaystyle \limitd{}$ \ l\'imite es en distribuci\'on.
\end{lema}
\begin{proof}
  La prueba  es inmediata tomando el  logaritmo de la  densidad de probabilidad,
  usando la  formula de Stirling \  $\log\Gamma(z) = \left( z  - \frac12 \right)
  \log z - z  + \frac12 \log(2 \pi) + o(1)$ \  en \ $z \to +\infty$~\cite{Sti30,
    AbrSte70,  GraRyz15} \ y  \ $-\frac{d+\nu}{2}  \log\left( 1  + \frac{(x-m)^t
      \Sigma^{-1}  (x-m)}{\nu} \right)  = -\frac{d+\nu}{2}  \left( \frac{(x-m)^t
      \Sigma^{-1}   (x-m)}{\nu}  +   o\left(  \nu^{-1}   \right)  \right)   =  -
  \frac{(x-m)^t \Sigma^{-1} (x-m)}{2} + o(1)$.
\end{proof}

Las  variables   Student-$t$  tienen  varias   representaciones  estoc\'asticas,
relacionadas a la gaussiana~\cite{FanKot90, And03, KotNad04, AndKau65}:
% ej. KotNad p. 7 para la secunda
%
\begin{proof}

\end{proof}
%
Nota:       este       lema        permite       tambi\'en       probar       el
lema~\ref{Lem:MP:StabilidadLinealStudentT} escribiendo \ $A X + b \egald \frac{A
  \Sigma^{\frac12} G}{sqrt{V}} + A m + b$.

\begin{lema}[Relaci\'on con la distribuci\'on de Wishart]\label{Lem:MP:StudentTWishart}
%
  Sea \ $W \, \sim \, \W( \Sigma^{-1} \, , \, \nu+d-1)$ \ $d \times d$ \ Wishart
  con \ $\Sigma \in \Pos_d^+(\Rset)$, \ $Y  \, \sim \, \N(0,\nu I)$ \ con $\nu >
  0$ \ e \ $Y$ \ independiente de \ $W$. Entonces, para \ \ $m \in \Rset^d$,
  %
  \[
  W^{-\frac12} Y + m \, \sim \, \T_\nu\left( m , \Sigma \right)
  \]
  %
\end{lema}
\begin{proof}
  Sea \ $X = W^{-\frac12} Y$. De la nota siguiendo la tabla de caracter\'isticas
  es  necesario  y suficiente  probar  que $X  \sim  \T_\nu(0,\Sigma)$.  Ahora, de  la
  independencia tenemos
  %
  \[
  p_{X|W=w}(x)  = (2  \pi \nu)^{-\frac{d}{2}}  |w|^{\frac12} e^{-  \frac{x^t w  x}{2
      \nu}}
  \]
  %
  Denotamos  por \  $D =  \left\{ w_{ij},  \: 1  \le j  \le i  \le d  \tq  w \in
    \Pos_d^+(\Rset) \right\}$ \ y, por abuso de  escritura, \ $dw = \prod_{ 1 \le j
    \le i \le d} dw_{ij}$.  Entonces,  multiplicando \ $p_{X|W=w}$ \ por \ $p_W$
  \ y por marginalizaci\'on, obtenemos
  % ($\propto$ significa ``proporcional  a'', i.e., olvidando el coefficiente de
  % normalizaci\'on)
  %
  \begin{eqnarray*}
  p_X(x) & = & \int_D \frac{|w|^{\frac{\nu-1}{2}} e^{- \frac{x^t w x}{2 \nu} -
  \frac12 \Tr\left( \Sigma w \right)}}{2^{\frac{d (\nu+d)}{2}} (\pi
  \nu)^{\frac{d}{2}} \left| \Sigma^{-1} \right|^{\frac{\nu+d-1}{2}} \Gamma_d \left(
  \frac{\nu+d-1}{2} \right)} \, dw\\[2mm]
  %
  & = & \frac{\Gamma\left( \frac{\nu+d}{2} \right)}{(\pi \nu)^{\frac{d}{2}}
  \Gamma\left( \frac{\nu}{2} \right)} \left| \Sigma + \frac{x x^t}{\nu}
  \right|^{-\frac{\nu+d}{2}} \left| \Sigma
  \right|^{\frac{\nu+d-1}{2}} \: \int_D \frac{|w|^{\frac{\nu+d-d-1}{2}} e^{-
  \frac12 \Tr\left( \left[ \Sigma + \frac{x x^t}{\nu} \right] w \right)}}{2^{\frac{d
  (\nu+d)}{2}} \left| \left( \Sigma + \frac{x x^t}{\nu} \right)^{-1}
  \right|^{\frac{\nu+d}{2}} \Gamma_d \left( \frac{\nu+d}{2} \right)} \, dw\\[2mm]
  %
  & = & \frac{\Gamma\left( \frac{\nu+d}{2} \right)}{(\pi \nu)^{\frac{d}{2}}
  \Gamma\left( \frac{\nu}{2} \right) \left| \Sigma \right|^{\frac12}} \: \left( 1
  + \frac{x^t \Sigma^{-1} x}{\nu} \right)^{-\frac{\nu+d}{2}} \, \int_D
  \frac{|w|^{\frac{\nu+d-d-1}{2}} e^{- \frac12 \Tr\left( \left[ I + \frac{x
  x^t}{\nu} \right] w \right)}}{2^{\frac{d (\nu+d)}{2}} \left| \left( I + \frac{x
  x^t}{\nu} \right)^{-1} \right|^{\frac{\nu+d}{2}} \Gamma_d \left( \frac{\nu+d}{2}
  \right)} \, dw
  \end{eqnarray*}
  %
  Para  \ $a, b  \in \Rset^d,  \: M  \in \Mat_{d,d}(\Rset)$,  en la  secunda linea
  usamos la  identidad \  $a^t M b  = \Tr(b a^t  M)$ \  y \ $\Gamma_d\left(  x -
    \frac12 \right) = \frac{\Gamma\left(  x - \frac{d}{2} \right)}{\Gamma(x)} \,
  \Gamma_d(x)$ \ (ver notaciones) y en  la tercera linea usamos $\left| \Sigma +
    \frac{x   x^t}{\nu}   \right|  =   \left|   \Sigma   \right|   \left|  I   +
    \frac{\Sigma^{-1}   x    x^t}{\nu}   \right|$   \   y    la   identidad   de
  Sylvester~\cite{Syl51} o~\cite[\S~18.1]{Har08} \ $\left| I + a b^t \right| = 1
  + b^t  a$. Se concluye  que \ $X  \sim \T_\nu(0,\Sigma)$ \ reconociendo  en el
  factor de  la integral como la  distribuci\'on \ $\T_\nu(0,\Sigma)$ \  y en el
  integrande  la  distribuci\'on de  Wishart  \  $\W\left(  \left( I  +  \frac{x
        x^t}{\nu}  \right) \,  , \,  \nu+d  \right)$ \  que suma  entonces a  la
  unidad.
  %  la  formula de  Sherman-Morrison-Woodbury  $\left(  I  + \frac{x  x^t}{\nu}
  % \right)^{-1} = I$~\cite{HorJoh13, Har08}
\end{proof}

Como lo hemos introducido, la distribuci\'on Student-$t$ aparece naturalmente en
el  marco  de la  estimaci\'on,  especialmente  a  trav\'es de  la  estimaci\'on
emp\'irica  de la  media  y covarianza~\cite{Mui82,  GupNag99, BilBre99,  And03,
  Seb04}:
% resp. p 80 teo 3.2.1 -- p. 92 teo 3.3.6 -- p. 87 prop. 7.1 -- p. 77 teo. 3.3.2 -- p. 63 teo. 3.1
% Nota : ver corolarios 2 y 3, p. 25 de Seber
% VER GupNag Th. 4.2.1
%
\begin{teorema}%[]
%
  Sean  \  $X_i \,  \sim  \,  \N(m,\Sigma), \:  i  =  1, \ldots  ,  n  > d-1$  \
  independientes,       y      sea       la       media      emp\'irica       (ver
  corolario~\ref{Cor:MP:MediaEmpiricaGauss})
  %
  \[
  \overline{X} = \frac{1}{n} \sum_{i=1}^n X_i
  \]
  %
  y  la  covarianza emp\'irica  construida  a partir  de  la  media emp\'irica  (ver
  corolario~\ref{Cor:MP:WishartEstimacion})
  %
  \[
  \overline{\Sigma}  =  \frac{1}{n-1}  \sum_{i=1}^n  \left( X_i  -  \overline{X}
  \right) \left( X_i - \overline{X} \right)^t
  \]
  %
  Entonces:
  %
  \begin{itemize}
  \item $\overline{X} -  m \, \sim \,  \N\left( 0 \, , \,  \frac{1}{n} \, \Sigma
    \right)$ \ y  \ $\overline{\Sigma} \, \sim \, \W(  \frac{1}{n-1} \Sigma \, ,
    \, n-1 ) $ \ son independientes;
  %
  \item $\sqrt{\frac{n (n-d)}{n-1}} \: \overline{\Sigma}^{\, -\frac12} \, \left(
      \overline{X} - m \right) \, \sim \, \T_{n-d}\left( 0 \, , \, I \right)$
  \end{itemize}
\end{teorema}
%
\begin{proof}
  Se      refiera      a     los      corolarios~\ref{Cor:MP:MediaEmpiricaGauss}
  y~\ref{Cor:MP:WishartEstimacion}   por   lo  de   las   distribuciones  de   \
  $\overline{X}-m$ \ y de \ $\overline{\Sigma}$ \ respectivamente.

  A continuaci\'on,  sean \  $\widetilde{X}_i =  X_i - m$  \ y  \ $\widetilde{X}
  =        \begin{bmatrix}        \widetilde{X}_1        &       \cdots        &
    \widetilde{X}_n \end{bmatrix}$. Obviamente
  %
  \[
  \overline{\widetilde{X}} \equiv \overline{X} - m = \frac{1}{n} \, \widetilde{X} \un
  \]
  %
  con \ $\un \in  \Rset^n$ \ vector de componentes iguales a \  $1$ \ y vimos en
  la prueba del corolario~\ref{Cor:MP:WishartEstimacion} que
  %
  \[
  \overline{\Sigma} = \frac{1}{n-1} \widetilde{X} \left( I - \frac{\un \un^t}{n}
  \right) \widetilde{X}^t
  \]
  %
  $A = I - \frac{\un \un^t}{n} \in  \Pos_n(\Rset)$ \ es idemponenta de rango 1, con
  $A  \un = 0$,  as\'i que  por diagonalizaci\'on~\cite{HorJoh13,  Bha97, Bha07}
  tenemos
  %
  \[
  A = P \begin{bmatrix} I_{n-1} & 0\\ 0 & 0 \end{bmatrix} P^t \qquad \mbox{con} \qquad
  P = \begin{bmatrix} B & \frac{1}{\sqrt{n}} \un \end{bmatrix}
  \]
  %
  $P P^t = P P^t = I$ \ y
  %
  \[
  B \in \Mat_{n,n-1}(\Rset) \quad \mbox{tal que} \quad  B^t B = I \: \mbox{ y } \:
  \un^t B = 0
  \]
  %
  Ahora,   poniendo   la  descomposici\'on   diagonal   de   \   $A$  \   en   \
  $\overline{\Sigma}$ \ obtenemos (ver~corolario~\ref{Cor:MP:WishartEstimacion})
  %
  \[
  \overline{\Sigma} = \frac{1}{n-1} \, Y Y^t \qquad \mbox{con} \qquad Y = \widetilde{X} B
  \]
  %
  Luego, de la gaussianidad y independencia de los \ $\widetilde{X}_i$ \ tenemos,
  para \   $\widetilde{x}   =    \begin{bmatrix}   \widetilde{x}_1   &   \cdots   &
    \widetilde{x}_n \end{bmatrix} \in \Mat_{d,n}(\Rset)$
  %
  \begin{eqnarray*}
  p_{\widetilde{X}}(\widetilde{x}) & = & (2 \pi)^{-\frac{n d}{2}}
  |\Sigma|^{-\frac{n}{2}} \exp\left(- \frac12 \sum_{i=1}^n \widetilde{x}_i^t
  \Sigma^{-1} \widetilde{x}_i \right)\\[2mm]
  %
  & = & (2 \pi)^{-\frac{n d}{2}} |\Sigma|^{-\frac{n}{2}} \exp\left(- \frac12
  \sum_{i=1}^n \Tr\left( \Sigma^{-1} \widetilde{x}_i \widetilde{x}_i^t
  \right) \right)\\[2mm]
  %
  & = & (2 \pi)^{-\frac{n d}{2}} |\Sigma|^{-\frac{n}{2}} \exp\left(- \frac12
  \Tr\left( \Sigma^{-1} \widetilde{x} \, \widetilde{x}^t \right) \right)
  \end{eqnarray*}
  %
  Sea    la     transformaci\'on    \    $\begin{bmatrix}     Y    &    \sqrt{n}
    \overline{\widetilde{X}}   \end{bmatrix}   =   \widetilde{X}   P$,   \ie   \
  $\widetilde{X}        =       \begin{bmatrix}        Y        &       \sqrt{n}
    \overline{\widetilde{X}} \end{bmatrix}  P^t$.  Se nota que  \ $|P| =  1$ \ y
  por                            transformaci\'on                           (ver
  teorema~\ref{Teo:MP:TransformacionInyectivaDensidad}),    para   \    $y   \in
  \Mat_{d,n-1}(\Rset)$ \ y \ $x \in \Rset^d$
  %
  \begin{eqnarray*}
  p_{Y,\sqrt{n} \overline{\widetilde{X}}}(y,x) & = & (2 \pi)^{-\frac{n d}{2}}
  |\Sigma|^{-\frac{n}{2}} \exp\left(- \frac12 \Tr\left(
  \Sigma^{-1} \begin{bmatrix} y & x \end{bmatrix} P^t P \begin{bmatrix} y^t\\
  x \end{bmatrix}\right) \right)\\[2mm]
  %
  & = & (2 \pi)^{-\frac{n d}{2}} |\Sigma|^{-\frac{n}{2}} \exp\left(- \frac12
  \Tr\left( \Sigma^{-1} \left( y y^t + x x^t \right) \right) \right)\\[2mm]
  %
  & = & (2 \pi)^{-\frac{(n-1) d}{2}} |\Sigma|^{-\frac{n-1}{2}} \exp\left(-
  \frac12 \Tr\left( \Sigma^{-1} y y^t \right) \right) \times (2
  \pi)^{-\frac{d}{2}} |\Sigma|^{-\frac12} \exp\left(- \frac12 x^t \Sigma^{-1} x
  \right)
  \end{eqnarray*}
  %
  Claramente,  de la factorizaci\'on  de las  distribuciones, $Y  = X  B$ \  y \
  $\sqrt{n}  \overline{\widetilde{X}}$  \ son  independientes,  es  decir que  \
  $\frac{1}{n-1} \, Y Y^t = \overline{\Sigma}$ \ y \ $\overline{\widetilde{X}} =
  \overline{X} -  m$ \ son  independientes, lo que  cierra la prueba  del primer
  item.  Pasando, la forma  de $p_{Y,\sqrt{n}  \overline{\widetilde{X}}}(y,x)$ \
  confirma  que  \ $\overline{X}-m$  \  es  gaussiana  centrada de  covarianza  \
  $\frac{1}{n} \,  \Sigma$, y  que los \  $Y_i$ \ son  independientes gaussianos,
  dando la distribuci\'on  de Wishart del lema~\ref{Lem:MP:WishartGausiana} para
  la covarianza emp\'irica.

  A continuaci\'on,
  %
  \[
  \sqrt{\frac{n   (n-d)}{n-1}}   \,   \overline{\Sigma}^{\,   -\frac12}   \left(
    \overline{X}-m \right)  = \frac{1}{\sqrt{n-1}} \:  \Sigma^{- \frac12} \left(
    \Sigma^{-1}  \, \overline{\Sigma}  \, \Sigma^{-1}  \right)^{-\frac12} \left(
    \sqrt{n (n-d)} \: \Sigma^{-\frac12} \left( \overline{X}-m \right) \right)
  \]
  %
  Del           teorema~\ref{Teo:MP:StabilidadGaussiana}          y          del
  lema~\ref{Lem:MP:StabilidadWishartLineal} tenemos
  %
  \[
  \sqrt{n (n-d)}  \: \Sigma^{-\frac12} \left( \overline{X}-m \right)  \, \sim \,
  \N(0 ,  (n-d) I)  \qquad \mbox{y} \qquad  \Sigma^{-1} \,  \overline{\Sigma} \,
  \Sigma^{-1}  \, \sim  \, \W\left(  \left( (n-1)  \Sigma \right)^{-1}  \,  , \,
    n-d+d-1 \right)
  \]
  %
  Se   cierra   la  prueba   usando   los  lemas~\ref{Lem:MP:StudentTWishart}   \
  y~\ref{Lem:MP:StabilidadLineal}.
\end{proof}

%\SZ{
% VER LO QUE PASA SI overline{Sigma} si X_i y X_i-overline{X} para estandardizar los datos.

%Sampling distribution, Gosset, Fisher 25. Applications
%}

M\'as propiedades de esta distribuci\'on se encuentran en libros especializados,
por ejemplo~\cite{KotNad04} completamente dedicado a esta distribuci\'on.

\

La distribuci\'on  Student-$t$ se generaliza al  caso complejo \  $Z$ \ definido
sobre $\Cset^d$; se denota  \ $Z \, \sim \, \CT_\nu(m,\Sigma)$ \  donde \ $m \in
\Cset^d$, \ $\Sigma \in \Pos_d^+(\Cset)$ \ y la densidad es dada por \
%
\[
p_Z(z) = \frac{\Gamma\left( d  + \frac{\nu}{2} \right)}{\pi^d \nu^d \Gamma\left(
    \frac{\nu}{2}   \right)  \,   \left|   \Sigma  \right|}   \:   \left(  1   +
  \frac{(z-m)^\dag \, \Sigma^{-1} \, (z-m)}{\nu} \right)^{- \frac{\nu}{2}-d}
\]
%
(ver  por  ejemplo~\cite[\S~5.12  y   ref.]{KotNad04}  para  una  versi\'on  muy
parecida).
% Gupta 64, Tan 73, 69b
%Se puede referirse  a~\cite[Cap.~4]{GupNag99} para tener m\'as
%detalles.

\

% \left|  \Sigma' \right|^{\frac{d}{2}}}  \:  \left| I  +  \Sigma^{-1} (x-m)  \,
% \Sigma'^{-1} \, (x-m)^t \right|^{- \frac{\nu+d+d'-1}{2}}
Tambi\'en, la  distribuci\'on Student-$t$  se generaliza al  caso matriz-variada
$X$  definido   sobre  $\Mat_{d,d'}(\Rset)$;   se  denota  \   $X  \,   \sim  \,
\T_\nu(m,\Sigma,\Sigma')$  \ donde \  $m \in  \Mat_{d,d'}(\Rset), \:  \Sigma \in
\Pos_d^+(\Rset), \:  \Sigma' \in \Pos_{d'}^+(\Rset)$  y la densidad es  dada por
$\displaystyle             p_X(x)             =             \frac{\Gamma_d\left(
    \frac{\nu+d+d'-1}{2}\right)}{\pi^{\frac{d       d'}{2}}       \Gamma_d\left(
    \frac{\nu+d-1}{2}\right)  \,  \left|  \Sigma  \right|^{\frac{d'}{2}}  \left|
    \Sigma'  \right|^{\frac{d}{2}}} \:  \left| I  + \frac{1}{\nu}  \,  (x-m)^t \
  \Sigma^{-1}  \, (x-m) \,  \Sigma'^{-1} \right|^{-  \frac{\nu+d+d'-1}{2}}$.  Se
refiera a~\cite{Dic67}, \cite[Cap.~4]{GupNag99} o~\cite[\S5.11 y ref.]{KotNad04}
para tener m\'as detalles (las formas son un poco diferentes, pero completamente
equivalente  a   la  de  este   libro).   En  particular,  se   generalizan  los
lema~\ref{Lem:MP:StabilidadLinealStudentT}~\cite[Teo.~4.3.5]{GupNag99}       (ver
tambi\'en           secci\'on~\ref{Ssec:MP:FamiliaElipticaMatriz}),           el
lema~\ref{Lem:MP:LimiteStudentTGaussiana}~\cite[Teo.~4.3.4]{GupNag99}    y    el
lema~\ref{Lem:MP:StudentTWishart}~\cite[Teo.~4.2.1]{GupNag99}.  Notar  que  esta
distribuci\'on cae en en una clase  dicha el\'iptica, que vamos a ver brevemente
en  la secci\'on~\ref{Ssec:MP:FamiliaElipticaMatriz},  as\'i que  propiedades en
este marco general.
% Dickey 66, Cornis 54