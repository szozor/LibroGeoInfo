\seccion{Variables aleatorias y distribuciones de probabilidad}
\label{s:variablealeatoria}

\emph{En  un  experimento  o  un  dado  proceso,  los  posibles  resultados  son
  t\'ipicamente  n\'umeros reales, siendo  cada n\'umero  un evento.   Luego los
  resultados  son mutuamente  excluyentes. Se  considera a  esos  n\'umeros como
  valores de una  \emph{variable aleatoria} $X$ a valores  reales, que puede ser
  discreta o continua.}

%  (cuando  el espacio  muestral  es  finito  o infinito  numerable)  o
%  continua.  La ley  de la variable aleatoria $X$ es  una medida de probabilidad
%  definida por  \ $P_X(x) =  \Pr(X=x)$ o, en  general, por $P_X(A)  = \Pr(X=x\in
%  A)$.  Para indicar  que la variable~$X$ sigue la ley  de distribuci\'on $p$ se
%  escribe  \  $X\sim p  $.   Puede  ser  \'util tambi\'en  considerar  variables
%  aleatorias  complejas  $Z=X+iY$, donde  $X$  e  $Y$  son variables  aleatorias
%  reales. }


\modif{  Formalmente,  la noci\'on  de  variable  aleatoria  se apoya  sobre  la
  noci\'on de funci\'on medible~\cite{AthLah06, Coh13}:
%
\begin{definicion}[Funci\'on medible]
  Sean  \ $(\Omega,\A)$  \ y  \ $(\Upsilon,\B)$  \ dos  espacios  medibles.  Una
  funci\'on $f: \Omega \mapsto \Upsilon$ es dicha {\it $(\A,\B)$-medible} si
  %
  \[
  \forall \,  B \in  \B, \quad  A \equiv f^{-1}(B)  = \left\{  \omega \in  A: \,
    f(\omega) \in B \right\} \: \in \: \A
  \]
  %
  Dicho de  otra manera,  la pre-imagen  de un elemento  dada de  $\B$ (elemento
  medible)  partenece  a  $\A$  (elemento  medible).  A  veces,  se  dice  m\'as
  simplemente de que $f: (\Omega,\A) \mapsto (\Upsilon,\B)$ es medible por abuso
  de escritura.
\end{definicion}

Adem\'as, saliendo de un espacio de medida y una funci\'on $f$ medible, se puede
definir una medida imagen sobre el espacio de llegada~\cite{AthLah06, Coh13}:
%
\begin{teorema}[Teorema de la medida imagen]
  Sean  \ $(\Omega,\A,\mu)$  \  un espacio  de  medida, \  $(\Upsilon,\B)$ \  un
  espacios  medibles  y una  funci\'on  $f:  (\Omega,\A) \mapsto  (\Upsilon,\B)$
  medible. Sea $\mu_f$ tal que
  %
  \[
  \forall \,  B \in  \B, \quad  \mu_f(B) = \mu\left( f^{-1}(B) \right)
  \]
  %
  Entonces, $\mu_f$ es una medida sobre el espacio medible $(\Upsilon,\B)$ \ \ie
  \ $(\Upsilon,\B,\mu_f)$ \ define un espacio de medida.  Adem\'as, $\mu(\Omega)
  = \mu_f(\Upsilon)$ (posiblemente infinitas).
\end{teorema}
%
\begin{proof}
  Por  definici\'on,  claramente $\mu_f  \ge  0$ \  y  por  definici\'on de  una
  funci\'on,  $f^{-1}(\emptyset)  =   \emptyset$  \  dando  $\mu_f(\emptyset)  =
  \mu(\emptyset) =  0$.  Luego,  si para  un conjunto numerable  $\{ B_i  \}$ de
  elementos  de $\B$  disjuntos entre  s\'i, las  preimagenes de  los  $B_i$ son
  disjuntos tambi\'en entre  s\'i (para $i \ne j$ no se  puede tener $\omega \in
  f^{-1}(B_i) \cap f^{-1}(B_j)$ si no $\omega$ tendr\'ia dos imagenes distinctas
  por  $f$).   Entonces  $\displaystyle f^{-1}\left( \bigcup_i  B_i  \right)  =
  \bigcup_i  f^{-1}(B_i)$.   Eso  implica  de   que  $\displaystyle  \mu_f\left(
    \bigcup_i B_i \right) = \mu\left( f^{-1}\left( \bigcup_i B_i \right) \right)
  =  \bigcup_i  \mu\left( f^{-1}(B_i)  \right)  =  \sum_i \mu\left(  f^{-1}(B_i)
  \right) = \sum_i  \mu_f(B_i)$.  Finalmente, necesariamente $f^{-1}(\Upsilon) =
  \Omega$  (es incluida y  $f(\Omega)$ siendo  en $\Upsilon$  son necesariamente
  iguales) lo  que cierra la  prueba~\footnote{De hecho, se  puede sencillamente
    probar que la  preimagen de una uni\'on numerable (que  sean disjuntos o no)
    es la uni\'on  de las preimagenes; lo mismo occure  para la intersecci\'on y
    adem\'as la preimagen del complemento es el complemento de la preimagen. Eso
    es conocido como {\it leyes de de Morgan}~\cite{AthLah06, Coh13, HogMck13}.}.
\end{proof}

Un espacio  jugando un  rol particular es  $\Rset$, a  lo cual se  puede asociar
$\B(\Rset)$ la  $\sigma$-\'algebra m\'as  peque\~na generada por  los intervalos
$(-\infty  \,  ;  \, b]$  (equivalentemente,  por  los  abiertos de  $\Rset$,  o
tambi\'en  por  los  intervalos  $(a  \,  ; \,  b]$),  \ie  uniones  numerables,
intersecciones  numerables,  complementos  de  estos  intervalos~\cite{AthLah06,
  Coh13}.   $\B(\Rset)$  es  llamada   {\it  Borelianos   de  $\Rset$}   o  {\it
  $\sigma$-\'algebra de Borel de $\Rset$}.

Con  estas   definiciones,  tenemos  todo   lo  necesario  para   introducir  la
definici\'on de una variable aleatoria real~\cite{AthLah06, Coh13, Bre88}:
%
\begin{definicion}[Variable aleatoria real]
  Una variable aleatoria real es una funci\'on medible
  %
  \[
  X: (\Omega,\A,P) \mapsto (\Rset,\B(\Rset),P_X)
  \]
  %
  donde la medida  $P_X$ sobre $\B(\Rset)$ es la medida imagen  de $P$.  \ $P_X$
  es frecuentemente llamada {\it distribuci\'on  de probabilidad} o {\it ley} de
  la variable aleatoria $X$. En lo que sigue, escribiremos los eventos
  %
  \[
  (X \in B) \equiv X^{-1}(B) = \{ \omega \in \Omega: \: X(\omega) \in B \}
  \]
  %
  as\'i que, por definici\'on,
  %
  \[
  P_X(B) = P(X \in B)
  \]
\end{definicion}
%
Para  ilustrar esta definici\'on,  tomando el  ejemplo de  un dado,  $\Omega$ es
discreto y representa  las caras, mientras de que los  numeros ser\'an la imagen
de $\Omega$ por $X$ (ej. $X(\omega_i) = i, \quad i = 1, \ldots , 6$).

Fijense de que,  por la propiedades de una  medida sobre una $\sigma$-\'algebra,
para caracterizar completamente la  distribuci\'on $P_X$ es suficiente conocerla
sobre  los intervalos de  la forma  $(-\infty \,  ; \,  b]$. Eso  da lugar  a la
definici\'on  de  la funci\'on  de  repartici\'on~\cite{AthLah06, Coh13,  Bre88,
  HogMck13}:
%
\begin{definicion}[Funci\'on de repartici\'on]
  Por  definici\'on,  la  funci\'on  de  repartici\'on  $F_X$  de  una  variable
  aleatoria es definida por
  %
  \[
  F_X(x) = P_X((-\infty \, ; \, x]) = P(X \le x)
  \]
  %
  A veces, por abuso de terminologia,  se denomina $F_X$ como ley de la variable
  aleatoria.
\end{definicion}
%
Naturalmente, de las propiedades de una medida de probabilidad,
%
\begin{itemize}
\item $0 \le F_X(x) \le 1$;
%
\item $\displaystyle \, \lim_{x \to -\infty} F_X(x) = 0$ \ y \ $\displaystyle \,
  \lim_{x \to +\infty} F_X(x) = 1$  (viene de $P_X(\emptyset) = 0$ y $P_X(\Rset)
  = 1$);
%
\item $F_X$ es creciente (viene de que $x_1 \le x_2 \Rightarrow (-\infty \, ; \,
  x_1] \subseteq (-\infty \, ; \, x_2]$);
%
\item $F_X$ no es necesariamente continua  (lo vamos a ver m\'as adelante), pero
  en cada punto $x$ es continua a su derecha (ver punto anterior).
\end{itemize}
}

Cuando se trabaja  con $d\geq 2$ variables aleatorias  es conveniente definir un
{\it vector aleatorio}  de dimensi\'on $d$, y apelar para  su estudio a nociones
del \'algebra  lineal y  a notaci\'on matricial.   Se tiene el  vector aleatorio
$d$-dimensional  \ $X  = \begin{bmatrix}  X_1  & \cdots  & X_d  \end{bmatrix}^t$
\modif{donde $\cdot^t$  denota la  transpuesta,} caracterizado por  $d$-uplas de
variables aleatorias reales.  Como en  el caso univariado, se define este vector
de la manera siguiente\modif{:~\cite{AthLah06, Coh13, Bre88}
%
\begin{definicion}[Vector aleatorio real]
  Un variable aleatorio real es una funci\'on medible
  %
  \[
  X: (\Omega,\A,P) \mapsto (\Rset^d,\B(\Rset^d),P_X)
  \]
  %
  donde  $\B(\Rset^d)$  son  los  borelianos  de  $\Rset^d$,  $\sigma$-\'algebra
  generada por  los productos cartesianos $(-\infty  \, ; \,  b_1] \times \cdots
  \times (-\infty \,  ; \, b_d]$ y donde la medida  $P_X$ sobre $\B(\Rset^d)$ es
  la  medida imagen  de $P$  llamada  {\it distribuci\'on  de probabilidad}  del
  vector aleatorio $X$. como el el caso escalar,
  %
 \[
 (X \in B) \equiv X^{-1}(B) = \{ \omega \in \Omega: \: X(\omega) \in B \} \qquad
 \mbox{y} \qquad P_X(B) = P(X \in B)
 \]
\end{definicion}
%

De la propiedades de una  medida sobre una $\sigma$-\'algebra, para caracterizar
completamente la distribuci\'on $P_X$ de nuevo es suficiente conocerla sobre los
elementos de la forma $(-\infty \, ;  \, b_1] \times \cdots \times (-\infty \, ;
\, b_d]$, \ie la  funci\'on de repartici\'on multivariada~\cite{AthLah06, Coh13,
  Bre88, HogMck13}:
%
\begin{definicion}[Funci\'on de repartici\'on multivariada]
  Por  definici\'on,  la  funci\'on  de  repartici\'on  $F_X$  de  un vector
  aleatorio es definida en $x = (x_1 , \ldots , x_d)$ por
  %
  \[
  F_X(x) = P_X((-\infty \, ; \, x_1] \times \cdots \times (-\infty \, ; \, x_d])
  = P\left( \bigcap_{i=1}^d (X_i \le x_i) \right)
  \]
\end{definicion}
%
De nuevo, de las propiedades de una medida de probabilidad,
%
\begin{itemize}
\item $0 \le F_X(x) \le 1$;
%
\item $\displaystyle  \, \lim_{\forall  i, x_i \to  -\infty} F_X(x)  = 0$ \  y \
  $\displaystyle \, \lim_{\forall i, x_ \to +\infty} F_X(x) = 1$;
%
\item $F_X$ es creciente con respecto a cada variable $x_i$.
\end{itemize}
%
Al  final, para  un subconjunto  $I_k =  (i_1,\ldots,i_k)$ de  $1 \le  k  \le d$
elementos de  $\{ 1  , \ldots ,  d \}^k$,  $X_{I_k} = \begin{bmatrix}  X_{i_1} &
  \cdots   &   X_{i_k}\end{bmatrix}^t$  es   obviamente   un  vector   aleatorio
$k$-dimensional. Es entonces sencillo ver de que
%
\[
F_{X_{I_k}}(x_{I_k}) = \lim_{\forall i \not\in I_k, x_i \to +\infty} F_X(x)
\]
%
(viene de  que $\bigcap_{j=1}^k (X_{i_j}  \le x_{i_j}) =  \left( \bigcap_{j=1}^k
  (X_{i_j} \le x_{i_j}) \right) \bigcap  \left( \bigcap_{i \not\in I_k} (X_i \in
  \Rset)  \right)$). Esta  funci\'on es  dicha {\it  funci\'on  de repartici\'on
  marginale} de $F_X$. 

Cerramos estas generalidades con el caso de variables independientes:
%
\begin{definicion}[Independencia]
  Sean       $d$        variables       aleatorias       $X_i$        y       $X
  = \begin{bmatrix}  \end{bmatrix}^t$. Los $X_i$  son mutualmente independientes
  si y  solamente si, para cualiquier  ensemble de conjuntos  $B_i$, los eventos
  $(X_i \in \B_i)$ son mutualmente inependientes, \ie
  %
  \[
  P_X(\times_{i=1}^d B_i) = \prod_{i=1}^d P_{X_i}(B_i)
  \]
  %
  donde $\times$ denota el producto cartesiano entre elementos de $\B(\Rset)$.
  %
  Es equivalente a
  %
  \[
  F_X(x) = \prod_{i=1}^d F_{X_i}(x_i)
  \]
  %
  La ley del vector aleatorio se factoriza.
\end{definicion}
%
Es importante notar de que no es equivalente a tener la independencia por pares,
como ilustrado en el fin de la seci\'on precediente.

\

M\'as  all\'a de  este  enfoque  general, dos  casos  particulares de  variables
aleatorias son  de inter\'es:  las variables discretas  y las continuas.   En el
primer  caso $\Omega$  es discreto,  finito  o no.  La meta  de las  susecciones
siguientes es estudiar las particularidades de cada caso.

Par fijar unas notaciones, en todo lo que sigue, escribiremos
%
\[
\X = X(\Omega)
\]
%
conjuntos de llegada  de $X$, o conjunto de valores que  puede tomar la variable
aleatoria.  A veces,  por razones  de simplificaciones,  se considera  $\X$ como
siendo el  espacio muestral  y se olvida  de que  $X$ sea una  funci\'on medibel
entre espacios de probabilidades,  \ie se trabaja en $(\Rset^d,\B(\Rset^d),P_X)$
como es el espacio preimagen.

}



% ================================= MaxEnt

\subseccion{Variable aleatoria discreta}
\label{sec:MP:VADiscreta}

Los  posibles valores de  una variable  aleatoria discreta  $X$ consisten  en un
conjunto  contable  (finito  o   infinito  numerable)  de  n\'umeros  reales:  \
$x\in\Omega=\{x_1,   x_2,  \ldots\}$.   A   cada  uno   de  los   valores  $x_n$
($n=1,2,\ldots$) se puede asociar una  probabilidad $p_n=p(x_n)$, de modo que se
satisface la condici\'on de normalizaci\'on:
%
$$\sum_n p_n = 1 . $$ 
%
La \emph{funci\'on (de masa) de probabilidad} es  de la forma: 
%
$$
p(x) = \left\{
\begin{array}{cl}
\Pr(X=x) & \mbox{si} \ x=x_1, x_2, \ldots \\ 0 &
\mbox{en~todo~otro~punto}
\end{array} \right.
$$
%
En  la   Fig.~\ref{fig:distribprobdiscreta}  se  muestra   una  representaci\'on
gr\'afica de una distribuci\'on de probabilidad discreta.
%
\begin{figure}[h!] %%ojo numeracion de figs !
%\centerline{\includegraphics[width=3cm]{distribprobdiscreta.jpg}} %%rehacer
%
\leyenda{Una distribuci\'on de probabilidad discreta.}
\label{fig:distribprobdiscreta}
\end{figure}

Tambi\'en, se puede caracterizar la ley de la variable discreta $X$ por medio de
su \emph{funci\'on de repartici\'on}:
%
$$
F_X(x)= \Pr(X\in(-\infty,x]) = \Pr (X\leq x) = \sum_{\forall
  n \,:\, x_n\leq x} p(x_n)
$$
%
que es una funci\'on discontinua, con saltos finitos, y no decreciente. 

Sin  p\'erdida de  generalidad, el  conjunto de  valores que  toma  una variable
aleatoria discreta $X$ puede considerarse como $\{0,1,2,\ldots,N\}$ para alg\'un
$N$ natural, o todo $\Nset$. Entonces la ley de una variable aleatoria a valores
naturales est\'a dada por  \ $\{p_n = \Pr(X=n), \ n\in \Nset  \}$.  Luego \ $\Pr
(X\in  A)=\sum_{n\in A\cap  \Nset}  p_n$,  y la  funci\'on  de repartici\'on  se
calcula como  \ $\Pr(X\leq x) = \sum_{n  \leq x} \Pr(X=n)$ que  es una funci\'on
que presenta un  salto finito en cada n\'umero natural.  En  general un salto de
la funci\'on  de repartici\'on corresponde a  la presencia de  una \emph{masa de
  Dirac} en el entorno del salto. %%

Un caso especial se tiene cuando un  valor $x_j$ es cierto o seguro, y no ocurre
ninguno de  los otros valores $x_i \  (i\neq j)$. La forma  de la distribuci\'on
es: \ $p_n=\delta_{nj}$, donde
%
$$
\delta_{ij} = \left\{
\begin{array}{cl}
1 & \mbox{si} \ i=j \\
0 & \mbox{si} \ i\neq j 
\end{array} \right.
$$
%
es el s\'imbolo \emph{delta de  Kronecker}. Cuando el espacio muestral es finito
de dimensi\'on $N$, la ley de  distribuci\'on se puede representar por medio del
siguiente vector columna:
%
$$
p = \begin{pmatrix}
% \left(  
%\begin{array}{c}
0 \\ \vdots \\ 0 \\ 1  \\ 0 \\ \vdots \\ 0
\end{pmatrix}
%\end{array} \right)
$$
%
con  un  1  en  el  lugar  $j$-\'esimo,  que tambi\'en  se  escribe  como  \  $p
= \begin{pmatrix} 0  & \cdots & 0 &  1 & 0 & \cdots &  0 \end{pmatrix}^t$, donde
$t$ indica transposici\'on.  La funci\'on de repartici\'on resulta una funci\'on
escal\'on o de Heaviside: \ $F(x)=\Theta(x-x_j)$.

Otra    situaci\'on   particular    es   la    de    \emph{equiprobabilidad}   o
\emph{distribuci\'on uniforme}.  La forma  de la distribuci\'on es: \ $p_n=\frac
1N \ \ \forall\ n=1,\ldots,N$, donde  $N$ señala el tamaño del espacio muestral.
La ley  de distribuci\'on  se puede representar  por medio del  siguiente vector
columna:
%
$$
p = \begin{pmatrix}
% \left(  
%\begin{array}{c}
1/N \\ 1/N  \\ \vdots \\ 1/N
\end{pmatrix}
%array} \right) , 
$$
%
que tambi\'en se escribe como \  $p = \begin{pmatrix} \frac1N & \frac1N & \cdots
  &  \frac1N  \end{pmatrix}^t$.   La  funci\'on  de  repartici\'on  resulta  una
funci\'on escalonada,  con saltos de  altura $\frac1N$ para  cada $n$ entre  1 y
$N$.

\hfill 

\emph{Reordenamiento y relaci\'on de mayorizaci\'on} 

Para comparar dos  distribuciones es \'util reordenar el  vector de probabilidad
permutando  sus  elementos  hasta  listarlos  de forma  descendente.   Se  anota
$p^\downarrow$, de  modo que \  $p^\downarrow_1 \geq p^\downarrow_2  \geq \ldots
\geq  p^\downarrow_N$.   En  el  ejemplo   del  caso  con  certeza  se  tiene  \
$p^\downarrow =  \begin{pmatrix} 1 & 0  & \cdots &  0 \end{pmatrix}^t$, mientras
que la distribuci\'on uniforme no var\'ia.

Se  define  \emph{mayorizaci\'on} del  siguiente  modo,  para distribuciones  de
dimensi\'on  $N$ (con  sus elementos  acomodados  en forma  decreciente): \  una
distribuci\'on $p$  es mayorizada por otra $q$,  y se denota $p\prec  q$, si las
primeras $N-1$  sumas parciales de $p^\downarrow$ y  $q^\downarrow$ satisfacen \
$\sum_{i=1}^n  p^\downarrow_i   \leq  \sum_{i=1}^n  q^\downarrow_i$   para  todo
$n=1,\ldots,N-1$, con \ $\sum_{i=1}^N p_i = 1 = \sum_{i=1}^N q_i$.

Por    ejemplo,   $\begin{pmatrix}    \frac12    &   \frac14    &   \frac18    &
  \frac18 \end{pmatrix}^t  \prec \begin{pmatrix} \frac12  & \frac14 &  \frac14 &
  0 \end{pmatrix}^t$.  Es posible  comparar por mayorizaci\'on distribuciones de
distinta  dimensionalidad, completando con  ceros el  vector de  probabilidad de
menor  dimensi\'on.  Es  importante  resaltar que  la  mayorizaci\'on provee  un
\emph{orden  parcial}  (no  total)  entre distribuciones,  existiendo  pares  de
distribuciones   tales  que   ninguna  mayoriza   a  la   otra.    Por  ejemplo,
$\begin{pmatrix} 0.50 &  0.40 & 0.10 \end{pmatrix}^t$ y  $\begin{pmatrix} 0.70 &
  0.15 & 0.15 \end{pmatrix}^t$ no se comparan por mayorizaci\'on.

Es  interesante  notar  que  la   siguiente  propiedad  es  v\'alida  para  toda
distribuci\'on $p$ de tamaño~$N$:
%
$$
\begin{pmatrix} \frac1N & \frac1N & \cdots & \frac1N \end{pmatrix}^t \ \prec \ p
\ \prec \ \begin{pmatrix} 1 & 0 & \cdots & 0 \end{pmatrix}^t.
$$
%
En este  sentido, los  casos particulares de  equiprobabilidad y de  certeza, se
dice  que  son  distribuciones  extremas.  Notamos que  uno  implica  ignorancia
m\'axima  en el  resultado de  la variable  mientras que  el otro  corresponde a
conocimiento completo.
%% graficamente 
%
\begin{figure}[h!] %%ojo numeracion de figs !
%\centerline{\includegraphics[width=3cm]{majorizationplot.pdf}} %%rehacer
%
\leyenda{Orden parcial por mayorizaci\'on}
\label{fig:majorizationplot}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subseccion{Variable aleatoria continua}

\SZ{\cite[\& Ref.]{BarNov78}}

Los posibles valores de una  variable aleatoria continua $X$ son cualesquiera de
los n\'umeros en un dado  intervalo de la recta real: \ $x\in\Omega\subset\Rset$
que  puede ser  un intervalo  $[x_m,x_M]$  o un  subconjunto (semi)infinito.  Es
conveniente asociar una  \emph{funci\'on densidad de probabilidad} (com\'unmente
anotada por su  sigla en ingl\'es: pdf por  \emph{probability density function})
$p(x)$ que tiene el  sentido de que la probabilidad de que  $X$ tome valor entre
$a$ y $b$ est\'a dada por:
%
$$
\Pr(a\leq X\leq b) = \int_a^b p(x) \, dx ,
$$
%
siendo $p(x)  \, dx$  la densidad de  probabilidad de  hallar a la  variable con
valores  en el intervalo  infinitesimal entre  $x$ y  $x+dx$. La  condici\'on de
normalizaci\'on se escribe
%
$$
\int_{x_m}^{x_M} p(x) \, dx=1 . 
$$ 

En  la   Fig.~\ref{fig:distribprobcontinua}  se  muestra   una  representaci\'on
gr\'afica de una funci\'on densidad de probabilidad para una variable continua.
%
\begin{figure}[h!] %%ojo numeracion de figs !
%\centerline{\includegraphics[width=3cm]{distribprobcontinua.png}} %%rehacer
%
\leyenda{Una distribuci\'on de probabilidad continua.}
\label{fig:distribprobcontinua}
\end{figure}

Tambi\'en, se puede caracterizar la ley de la variable continua $X$ por medio de
su  \emph{funci\'on  de   repartici\'on}  o  \emph{funci\'on  de  distribuci\'on
  cumulativa} (CDF por \emph{cumulative distribution function}):
%
$$
F_X(x) = \Pr(X\leq x) = \int_{x_m}^x p(t) \, dt
$$
%
que da la  probabilidad de que $X$ sea  menor o igual que cierto  valor $x$ dado
(dentro del conjunto~$\Omega$ de todos  los valores posibles de la variable). En
forma an\'aloga,  \ $\Pr(X\in  A) = \int_A  p(x) \,  dx$ acumula la  densidad de
probabilidad en un subconjunto $A$ del espacio muestral.  Por la propiedad de la
inclusi\'on,  se  tiene \  $\Pr(X\leq  x_1)  \leq  \Pr(X\leq x_2)$  siempre  que
$x_1\leq x_2$; luego $F_X(x)$ es una funci\'on creciente
%%no decreciente ?? SI, LO DIRIA ASI
de   $x$,  acotada   por  la   unidad,  con   valores  extremos   dados   por  \
$\lim_{x\rightarrow -\infty} F_X(x)=0$  y $\lim_{x\rightarrow \infty} F_X(x)=1$,
tomando $\Omega=\Rset$.  Adem\'as la derivada respecto de $x$ es la pdf:
%
$$
\frac{dF_X(x)}{dx}=p(x) . 
$$ 
%
De aqu\'i  se observa que  la densidad de  probabilidad $p(x)$ puede no  ser una
funci\'on ``ordinaria''  cuando $\Pr(X\leq x)$  es discontinua, pero  como mucho
tiene  la  singularidad  de   una  distribuci\'on  \emph{delta  de  Dirac}  cuya
representaci\'on integral es:
%
$$
\delta (x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{itx} dt . 
$$
% SZ: LA  VERDAD ES  QUE NO ME  GUSTA MUCHO  escribir una distribucion  como una
% fuccion porque no viven en el  mismo espacio. Pero no importa. Para mi, cuando
% hay  una  discontinuidad  X no  admite  une  pdf  por  definicion del  'f'  de
% pdf\sum_{}

Un caso especial  se tiene cuando la variable aleatoria $X$  toma el valor $x_0$
con certeza.  La forma  de la pdf  es: \ $p(x)=\delta(x-x_0)$.  Otra situaci\'on
particular es la distribuci\'on uniforme en  un intervalo; la pdf es de la forma
\  $p(x)=\frac{1}{b-a} \  \forall  \  x\in[a,b]$, donde  $[a,b]$  es el  espacio
muestral.

Usando las  funciones delta de  Dirac, se puede  unificar el tratamiento  de las
variables  aleatorias discretas  con las  continuas: si  una  variable aleatoria
discreta  toma los  valores $x_1,  x_2,  \ldots$ con  probabilidades $p_1,  p_2,
\ldots$ respectivamente,  entonces formalmente  se puede describir  mediante una
variable  aleatoria  continua  $X$  con  funci\'on densidad  de  probabilidad  \
$p(x)=\sum_j p_j \,\delta(x-x_j)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subseccion{Vector aleatorio}


Cuando se trabaja  con $d\geq 2$ variables aleatorias  es conveniente definir un
\emph{vector aleatorio} de dimensi\'on $d$,  y apelar para su estudio a nociones
del \'algebra  lineal y  a notaci\'on matricial.   Se tiene el  vector aleatorio
$d$-dimensional  \  $\mathbf{X} =  \{X^1,  \ldots,  X^d  \}$, o  simplemente  $X
=  \begin{pmatrix}  X^1  &  \cdots  & X^d  \end{pmatrix}^t$,  caracterizado  por
$d$-uplas de variables aleatorias reales, con funci\'on densidad de probabilidad
conjunta~$p(x^1, \ldots, x^d)$. La ley  del vector $\mathbf{X}$ es una medida de
probabilidad sobre $\Rset^d$, con
%
$$
P_{\mathbf{X}}(\mathbf{A})  =   \Pr(\mathbf{X}\in\mathbf{A})  =  \int_{\mathbf{A}}
p(x^1, \ldots, x^d)\ dx^1\ldots dx^d
$$
%
para  $\mathbf{A}  \subset \mathbf{\Omega}$,  siendo  la  pdf  conjunta $p$  una
funci\'on positiva, definida sobre $\mathbf{\Omega}\subset\Rset^d$, y tal que se
satisface la condici\'on de normalizaci\'on:
%
$$
\int_{\mathbf{\Omega}} p(x^1, \ldots, x^d)\ dx^1\ldots dx^d  = 1 .
$$

La  \emph{funci\'on densidad  de  probabilidad marginal}  que  caracteriza a  la
variable aleatoria  $X^i$ es la  ley que se  obtiene integrando la  pdf conjunta
sobre todas las variables excepto la $i$-\'esima:
%
$$
p_{X^i}(x^i)  =  \int_{\mathbf{\Omega}^{(i)}}  p(x^1, \ldots,  x^d)\  dx^1\ldots
dx^{i-1} dx^{i+1} \ldots dx^d
$$
%
donde $\mathbf{\Omega}^{(i)}\subset\Rset^{d-1}$  barre el espacio  muestral para
$X^1, \ldots, X^{i-1}, X^{i+1}, \ldots, X^d$.

Las  $d$  variables  aleatorias  $X^1,  \ldots,  X^d$  de  un  vector  aleatorio
$\mathbf{X}$ se dicen \emph{independientes} si corresponden a eventos mutuamente
independientes. Esto se  da si y s\'olo  si la pdf conjunta se  factoriza en las
$d$ pdf marginales:
%
$$
p(x^1, \ldots, x^d) = p_{X^1}(x^1) \cdots p_{X^d}(x^d) . 
$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subseccion{Transformaci\'on de variables aleatorias}


Sea $X$ una  variable aleatoria (continua, en general)  definida en el intervalo
$[x_m, x_M]$ con funci\'on densidad  de probabilidad $p(x)$. Sea $Y=\Psi(X)$ una
funci\'on real  de $X$, luego $Y$  toma los valores $y=\Psi(x)$  en el intervalo
$[y_m,y_M]$.  La funci\'on  densidad  de probabilidad  $q(y)$  para la  variable
aleatoria transformada $Y$ se obtiene  de la siguiente manera, dependiendo de la
forma de la transformaci\'on:

\begin{itemize}
\item Si  $\Psi$ es inversible, con  inversa (\'unica), se  tiene \ $x=\Phi(y)$,
  con  $\Phi=\Psi^{-1}$.  A partir  de  la  propiedad  de conservaci\'on  de  la
  probabilidad
  %
  $$
  |q(y)\, dy| = |p(x) \, dx|
  $$ 
  %
  para  una correspondencia  biun\'ivoca  entre $x$  e  $y$, se  obtiene la  pdf
  transformada
  %
  $$
  q(y)  = p(x)  \left| \frac{dx}{dy}  \right| =  p\left(\Phi(y)\right)  \ \left|
    \Phi'(y)   \right|   =  \frac{p\left(\Phi(y)\right)}{\left|   \Psi'(\Phi(y))
    \right|} .
  $$
  %
  Una forma alternativa  de derivar este resultado es partir  de la funci\'on de
  repartici\'on:
  %
  $$
  F_Y(y)  =  P(Y\leq  y)  =  P(\Psi(X)  \leq  y)  =  P(X\leq  \Psi^{-1}  (y))  =
  F_X(\Phi(y))
  $$
  %
  y  calcular las  derivadas del  primer y  \'ultimo t\'erminos  respecto  de la
  variable transformada~$y$.
%
\item Si la inversa de $\Psi$  es multivaluada, cada valor de $y$ se corresponde
  con  un conjunto de  valores de  $x$, digamos  $\{x_k =  \Phi_k(y), \  k=1, 2,
  \ldots\}$.  Debido a  que  estas soluciones  son  mutuamente excluyentes,  las
  probabilidades se suman, de modo que
  %
  $$
  q(y)   =    \sum_k   p(x_k)   \left|   \frac{dx_k}{dy}    \right|   =   \sum_k
  \frac{p\left(\Phi_k(y)\right)}{\left| \Psi'(\Phi_k(y)) \right|} ,
  $$
  %
  que   formalmente  se   puede  expresar   como  \   $q(y)  =   \int   p(x)  \,
  \delta(y-\Psi(x)\,) \ dx$ , donde se  usa la expansi\'on de la funci\'on delta
  en  t\'erminos de sus  ceros: \  $\delta(y-\Psi(x)\,)= \sum_k  \delta(x-x_k) /
  |\Psi'(x_k)| $.\newline  Por ejemplo, para la transformaci\'on  de variables \
  $Y=X^2$ se tiene \  $Y=\Psi(X)=X^2$ cuyas inversas son \ $X_1=\Phi_1(Y)=+\sqrt
  Y$  y   $X_2=\Phi_2(Y)=-\sqrt  Y$;  luego   \  $q(y)=\frac{p(\sqrt  y)}{2\sqrt
    y}+\frac{p(-\sqrt y)}{|-2\sqrt y|}$ , para $y>0$.
\end{itemize}

\hfill

Consideramos ahora el  caso de un vector aleatorio  $\mathbf{X} = \{X^1, \ldots,
X^d\}$  con  funci\'on  densidad  de  probabilidad conjunta  \  $p(x^1,  \ldots,
x^d)$. Se  define otro vector aleatorio  \ $\mathbf{Y} =  \{Y^1, \ldots, Y^d\}$,
por   medio   de   las   transformaciones  \   $Y^j=\Psi^j(X^1,\ldots,X^d)$,   \
$j=1,\ldots,d$. Suponiendo que las  funciones $\Psi^j$ tienen inversa (\'unica),
se  puede escribir \  $X^j=\Phi^j(Y^1,\ldots,Y^d)$ para  cada $j$.  La funci\'on
densidad de probabilidad conjunta $q(y^1,\ldots,y^d)$ para $\mathbf{Y}$ se puede
obtener a partir de la propiedad de conservaci\'on de la probabilidad
%
$$
|q(y^1,\ldots,y^d)\ dy^1\cdots dy^d| = |p(x^1,\ldots,x^d) \ dx^1\cdots dx^d| .
$$ 
%
Para  una  correspondencia biun\'ivoca  entre  $\mathbf{x}$  e $\mathbf{y}$,  se
obtiene la pdf transformada
%
$$
q(y^1,\ldots,y^d) = \left| \Jac_\Phi \right| \, p(x^1,\ldots, x^d) 
%%= \int  p(x^1,\ldots, x^d) \delta(y^1-\Psi^1) .......  dx^1 .....
$$
%
donde  $\Jac_\Phi =  \frac{\partial(\Phi^1  , \ldots  , \Phi^d)}{\partial(y^1  ,
  \ldots , y^d)}$ es el Jacobiano de la transformaci\'on.

%% Ejercicio: Estudiar el caso multivaluado / Resolver un ej. 

\hfill

Una  \emph{variable  aleatoria  compleja}   $Z=X+i  Y$  puede  interpretarse  en
t\'erminos de las  dos variables aleatorias reales $X$ e $Y$.  La pdf asociada \
$P(z)=p(x,y)$ est\'a dada por la  funci\'on densidad de probabilidad conjunta de
las variables reales. La condici\'on de normalizaci\'on se escribe
%
$$
\int P(z) \, d^2 z = 1
$$
%
donde $d^2 z=dx\,dy$.